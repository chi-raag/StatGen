[
  {
    "objectID": "lectures/lecture-01.html#about-the-instructors",
    "href": "lectures/lecture-01.html#about-the-instructors",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "About the Instructors",
    "text": "About the Instructors\n\n\n\n\n\n\nDr. Keith Crandall\n\n\n\n\n\n\n\nDr. Ali Rahnavard\n\n\n\n\n\n\n\nChiraag Gohel"
  },
  {
    "objectID": "lectures/lecture-01.html#introductions",
    "href": "lectures/lecture-01.html#introductions",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Introductions",
    "text": "Introductions\n\nName, Degree, any other ways you like to define yourself (favorite hot dog brand, etc.)\nCurrent research focus or research interests.\nWhat you hope to get out of this course."
  },
  {
    "objectID": "lectures/lecture-01.html#course-site",
    "href": "lectures/lecture-01.html#course-site",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Course Site",
    "text": "Course Site\nhttps://gwcbi.github.io/StatGen/"
  },
  {
    "objectID": "lectures/lecture-01.html#zotero",
    "href": "lectures/lecture-01.html#zotero",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Zotero",
    "text": "Zotero"
  },
  {
    "objectID": "lectures/lecture-01.html#course-logistics-and-expectations",
    "href": "lectures/lecture-01.html#course-logistics-and-expectations",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Course Logistics and Expectations",
    "text": "Course Logistics and Expectations\nGrading Breakdown\n\n\n\nAssignment Type\n% of Grade\n\n\n\n\nProblem Sets\n60%\n\n\nClass Participation\n20%\n\n\nResearch Project\n20%\n\n\n\nTotal Workload: 112.5 hours (5+ hrs/week independent + 6 hrs/week class/async)"
  },
  {
    "objectID": "lectures/lecture-01.html#problem-sets-60-of-grade",
    "href": "lectures/lecture-01.html#problem-sets-60-of-grade",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Problem Sets (60% of grade)",
    "text": "Problem Sets (60% of grade)\n\nWeekly assignments blending mathematical derivations with coding\nCollaborative time provided in class\nIndividual work required – substantial time outside class\nLate penalty: 1% per hour (first 5 hours), then 5% per day\nFormat: Mix of theory problems and R/Bioconductor analysis"
  },
  {
    "objectID": "lectures/lecture-01.html#class-participation-20-of-grade",
    "href": "lectures/lecture-01.html#class-participation-20-of-grade",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Class Participation (20% of grade)",
    "text": "Class Participation (20% of grade)\nMore than just showing up!\n\nCome prepared with questions from async materials\nEngage thoughtfully in discussions and problem-solving\nDemonstrate mastery through insightful contributions\nCollaborate respectfully - help others understand concepts\n\nQuantitative + Qualitative Assessment: Async responses + live session engagement"
  },
  {
    "objectID": "lectures/lecture-01.html#research-project-20-of-grade",
    "href": "lectures/lecture-01.html#research-project-20-of-grade",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Research Project (20% of grade)",
    "text": "Research Project (20% of grade)\nFinal third of semester - Choose your own adventure!\n\n\n\nProject Types\n\n\n\nComputational methods\nApplied analysis\nMethodological development\nAll based on primary literature\n\n\n\n\n\nDeliverables\n\n\n\nScientific paper (Genetics journal format)\nConference-style presentation\nGroups of up to 3 encouraged\nIndividual contributions must be clear"
  },
  {
    "objectID": "lectures/lecture-01.html#course-structure-overview",
    "href": "lectures/lecture-01.html#course-structure-overview",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Course Structure Overview",
    "text": "Course Structure Overview\nFoundation → Scale → Specialization (10 weeks)\n\nWeeks 1-4: Foundations (Mendelian genetics, heritability, likelihood algorithms, population structure & Bayesian methods)\nWeek 5: GWAS at scale (design, linear mixed models, meta-analysis)\nWeeks 6-7: Prediction models & multiple testing control\nWeeks 8-9: Binary traits & causal inference (Mendelian randomization)\nWeek 10: Advanced AI topics in statistical genetics\n\nLab Component: Hands-on R/Bioconductor workflows parallel to lectures"
  },
  {
    "objectID": "lectures/lecture-01.html#technology-setup-requirements",
    "href": "lectures/lecture-01.html#technology-setup-requirements",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Technology Setup Requirements",
    "text": "Technology Setup Requirements\n\nR (4.5+) and IDE installation\nGit setup and basic commands\nZotero for reference management\nZoom\nOptional: HPC access"
  },
  {
    "objectID": "lectures/lecture-01.html#course-textbook",
    "href": "lectures/lecture-01.html#course-textbook",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Course Textbook",
    "text": "Course Textbook\n\nSörensen (2025)"
  },
  {
    "objectID": "lectures/lecture-01.html#what-is-statistical-genetics",
    "href": "lectures/lecture-01.html#what-is-statistical-genetics",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "What is statistical genetics?",
    "text": "What is statistical genetics?\n\n\n\n\n\n\nPopulation Genetics\n\n\n\n\n\n\n\nGenetic Epidemiology\n\n\n\n\n\n\n\nQuantitative Genetics\n\n\n\n\n\n\n\nPopulation genetics is the study of evolutionary processes affecting genetic variation between organisms. A classic example is Darwin’s finches, where beak shape and size vary between islands in the Galapagos, driven by natural selection.\nGenetic Epidemiology is the study of how genetic factors contribute to health and disease in populations. A classic example is the study of genetic variants associated with diseases like diabetes or heart disease.\nQuantitative genetics is the study of continuous traits influenced by multiple genes and environmental factors. A classic example is human height, which is influenced by many genetic variants and environmental factors like nutrition. Shawn Bradley was a basketball player who as 7’6 tall, The study found Bradley had 198 more height-associated genetic variants than the average person in the sample"
  },
  {
    "objectID": "lectures/lecture-01.html#a-statistical-geneticist-may-want-to-know",
    "href": "lectures/lecture-01.html#a-statistical-geneticist-may-want-to-know",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "A statistical geneticist may want to know",
    "text": "A statistical geneticist may want to know\n\nIs there a genetic component contributing to the total variance of these traits?\nIs the genetic component of the traits driven by a few genes located on a particular chromosome, or are there many genes scattered across many chromosomes? How many genes are involved and is this a scientifically sensible question?\nAre the genes detected protein-coding genes, or are there also noncoding genes involved in gene regulation?\nHow is the strength of the signals captured in a statistical analysis related to the two types of genes? What fraction of the total genetic variation is allocated to both types of genes?\nWhat are the frequencies of the genes in the sample? Are the frequencies associated with the magnitude of their effects on the traits?\nWhat is the mode of action of the genes?"
  },
  {
    "objectID": "lectures/lecture-01.html#a-statistical-geneticist-may-want-to-know-1",
    "href": "lectures/lecture-01.html#a-statistical-geneticist-may-want-to-know-1",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "A statistical geneticist may want to know",
    "text": "A statistical geneticist may want to know\n\nWhat proportion of the genetic variance estimated in 1 can be explained by the discovered genes?\nGiven the information on the set of genes carried by an individual, will a genetic score constructed before observing the trait help with early diagnosis and prevention?\nHow should the predictive ability of the score be measured?\nAre there other non-genetic factors that affect the traits, such as smoking behavior, alcohol consumption, blood pressure measurements, body mass index and level of physical exercise?\nCould the predictive ability of the genetic score be improved by incorporation of these non-genetic sources of information, either additively or considering interactions? What is the relative contribution from the different sources of information?"
  },
  {
    "objectID": "lectures/lecture-01.html#what-does-the-data-look-like",
    "href": "lectures/lecture-01.html#what-does-the-data-look-like",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "What does the data look like?",
    "text": "What does the data look like?\nFamily/Pedigree Studies\n\nFamily/Pedigree Studies\n\nFamily studies are used to understand the inheritance patterns of traits and diseases within families. They can help identify genetic factors contributing to diseases by analyzing the occurrence of traits in related individuals.\nPedigree charts visually represent family relationships and the transmission of genetic traits across generations. They are useful for tracking the inheritance of specific traits or diseases within a family."
  },
  {
    "objectID": "lectures/lecture-01.html#what-does-the-data-look-like-1",
    "href": "lectures/lecture-01.html#what-does-the-data-look-like-1",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "What does the data look like?",
    "text": "What does the data look like?\nGenome-Wide Association Studies (GWAS)\n\nGenome-Wide Association Studies\n\nGWAS are studies that look for associations between genetic variants across the genome and specific traits or diseases in a large population. They can help identify genetic factors contributing to complex diseases by analyzing the frequency of genetic variants in affected and unaffected individuals.\nManhattan plots are visual representations of GWAS results, where each point represents a genetic variant and its association with the trait of interest. The x-axis represents the genomic position, while the y-axis represents the significance of the association (usually -log10(p-value)). Peaks in the plot indicate regions of the genome that may be associated with the trait."
  },
  {
    "objectID": "lectures/lecture-01.html#core-abstractions-for-quantitative-biology",
    "href": "lectures/lecture-01.html#core-abstractions-for-quantitative-biology",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Core abstractions for quantitative biology",
    "text": "Core abstractions for quantitative biology\n\nModel-Building\n\nFormulate generative models linking genotype, environment, and phenotype (e.g., linear mixed, Bayesian hierarchical, non-parametric kernels).\nEncode biological structure: linkage & LD, population stratification, dominance/epistasis, multi-omics priors.\nBalance realism and tractability to enable scalable computation on genome-scale data."
  },
  {
    "objectID": "lectures/lecture-01.html#core-abstractions-for-quantitative-biology-1",
    "href": "lectures/lecture-01.html#core-abstractions-for-quantitative-biology-1",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Core abstractions for quantitative biology",
    "text": "Core abstractions for quantitative biology\n\nInference\n\nEstimate unknown parameters and latent effects via likelihood maximisation, EM, MCMC, or SGD\nQuantify uncertainty with standard errors, posterior intervals, and credible sets\nControl false discoveries across millions of tests with FDR/Q-value, permutation, and empirical-Bayes shrinkage."
  },
  {
    "objectID": "lectures/lecture-01.html#core-abstractions-for-quantitative-biology-2",
    "href": "lectures/lecture-01.html#core-abstractions-for-quantitative-biology-2",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Core abstractions for quantitative biology",
    "text": "Core abstractions for quantitative biology\n\nPrediction\n\nUse fitted models for out-of-sample trait prediction: BLUP/GBLUP, ridge/lasso/elastic-net, Bayesian whole-genome regressions, random forests, neural nets.\nEvaluate accuracy (MSE, AUC), bias–variance trade-off, calibration, and portability across ancestries or cell types.\nTranslate genomic predictions into actionable scores for breeding, risk stratification, and drug-target prioritisation."
  },
  {
    "objectID": "lectures/lecture-01.html#core-abstractions-for-quantitative-biology-3",
    "href": "lectures/lecture-01.html#core-abstractions-for-quantitative-biology-3",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Core abstractions for quantitative biology",
    "text": "Core abstractions for quantitative biology\n\nInterpretation & Validation\n\nIntegrate functional annotations, eQTL, and single-cell data to refine biological mechanisms.\nPerform replication, cross-cohort meta-analysis, and sensitivity analyses to population assumptions.\nCommunicate findings with clear visualisations and reproducible workflows (R/Bioconductor, Git, notebooks)."
  },
  {
    "objectID": "lectures/lecture-01.html#early-work-in-the-field",
    "href": "lectures/lecture-01.html#early-work-in-the-field",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Early work in the field",
    "text": "Early work in the field\n\n\n\n\n\n\n\n\n\n\n\n\nGregor Mendel (1822-1884) was an Austrian monk and scientist who is considered the father of modern genetics. He conducted experiments on pea plants and discovered the fundamental laws of inheritance, including the concepts of dominant and recessive traits, segregation, and independent assortment.\nMendel’s work laid the foundation for the field of genetics, but it was largely ignored during his lifetime. It wasn’t until the early 20th century that his findings were rediscovered and integrated into the broader understanding of biology.\nIt is hypothesized that Charles Darwin would have begun his work on evolution much earlier if he had access to Mendel’s work on inheritance."
  },
  {
    "objectID": "lectures/lecture-01.html#genetics-statistics-and-eugenics-have-always-been-closely-linked",
    "href": "lectures/lecture-01.html#genetics-statistics-and-eugenics-have-always-been-closely-linked",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Genetics, statistics, and eugenics, have always been closely linked",
    "text": "Genetics, statistics, and eugenics, have always been closely linked\n\n\n\n\n\n\nStandard eugenics scheme of descent\n\n\n\n\n\n\n\nFrancis Galton and his work, “Inquires into Human Faculty and its Development”\n\n\n\n\n\n\n\nFrancis Galton (1822-1911) was an English polymath who made significant contributions to various fields, including statistics, psychology, and anthropology. He is best known for his work on eugenics, a now-discredited movement that aimed to improve the genetic quality of the human population through selective breeding.\nGalton was a cousin of Charles Darwin and was influenced by Darwin’s theory of evolution. He believed that intelligence and other traits were hereditary and could be measured and quantified. He developed statistical methods, such as correlation and regression, to study the inheritance of traits.\nGalton’s work on eugenics has been widely criticized for its ethical implications and its association with discriminatory practices, such as forced sterilizations and racial segregation"
  },
  {
    "objectID": "lectures/lecture-01.html#the-ethics-of-genetics-and-genomics-iswas-of-great-importance",
    "href": "lectures/lecture-01.html#the-ethics-of-genetics-and-genomics-iswas-of-great-importance",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "The ethics of genetics and genomics is/(was) of great importance!",
    "text": "The ethics of genetics and genomics is/(was) of great importance!"
  },
  {
    "objectID": "lectures/lecture-01.html#some-vocabulary",
    "href": "lectures/lecture-01.html#some-vocabulary",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Some vocabulary",
    "text": "Some vocabulary\n\nTrait/Phenotype: A measurable characteristic of an organism, such as height, weight, or disease status.\nGene: Unit of inheritance\nGenotype: The genetic constitution of an individual, often represented by specific alleles at particular loci.\nAllele: A variant form of a gene that can exist at a specific locus on a chromosome.\nLocus: A specific, fixed position on a chromosome where a particular gene or genetic marker is located.\nDiploid: Having two sets of chromosomes (one from each parent)\nPenetrance: Likelihood of expressing a phenotype given a genotype\nPolymorphism: The occurrence of two or more genetically determined forms in a population, such as single nucleotide polymorphisms (SNPs) or copy number variations (CNVs).\nGenetic Marker: A specific DNA sequence with a known location on a chromosome that can be used to identify individuals or species, often used in genetic mapping or association studies."
  },
  {
    "objectID": "lectures/lecture-01.html#some-vocabulary-1",
    "href": "lectures/lecture-01.html#some-vocabulary-1",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Some vocabulary",
    "text": "Some vocabulary\n\nMendelian Disease: A disease caused by a mutation in a single gene, following Mendelian inheritance patterns (dominant, recessive, X-linked).\nLinkage Disequilibrium (LD): The non-random association of alleles at different loci, indicating that certain allele combinations occur together more frequently than expected by chance.\nHeritability: The proportion of phenotypic variance in a trait that can be attributed to genetic variance, often estimated through twin or family studies.\nGenome-Wide Association Study (GWAS): A study that looks for associations between genetic variants across the genome and specific traits or diseases in a large population.\nPolygenic Score (PGS): A score that aggregates the effects of multiple genetic variants to predict an individual’s genetic predisposition to a trait or disease.\nQuantitative Trait Locus (QTL): A region of the genome that is associated with a quantitative trait, often identified through linkage or association mapping.\nEpistasis: The interaction between genes where the effect of one gene is modified by one or more other genes, influencing the expression of a trait."
  },
  {
    "objectID": "lectures/lecture-01.html#genetic-variant-example",
    "href": "lectures/lecture-01.html#genetic-variant-example",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Genetic Variant Example",
    "text": "Genetic Variant Example\n\nLaird and Lange (2011)"
  },
  {
    "objectID": "lectures/lecture-01.html#probability-refresher",
    "href": "lectures/lecture-01.html#probability-refresher",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Probability Refresher",
    "text": "Probability Refresher\nRandom Variables and Distributions\n\nA random variable is a numerical summary of randomness (e.g., the count of (A) alleles in a sample). We will denote random variables with uppercase letters (e.g., X, Y).\nWe use a model (e.g., binomial/multinomial) to describe how data vary from sample to sample.\n\nThis is otherwise known as a probability mass function (pmf) for discrete variables, or a probability density function (pdf) for continuous variables\n\nKey ideas:\n\nExpectation (mean) and variance: long‑run average and spread.\n\nE[X] = \\sum x \\cdot P(X = x) (discrete) or E[X] = \\int x f(x) dx (continuous)\n\\textsf{Var}(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2\n\nStandard error (SE): typical sampling variability of an estimator.\n\n\\textsf{SE} = \\frac{\\sigma}{\\sqrt{n}} (uncertainty in sample estimates)\n\nCentral Limit Theorem (CLT): averages and proportions are roughly normal for large (n).\n\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) as n \\to \\infty"
  },
  {
    "objectID": "lectures/lecture-01.html#probability-distributions-vs-sampling-distributions",
    "href": "lectures/lecture-01.html#probability-distributions-vs-sampling-distributions",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Probability Distributions vs Sampling Distributions",
    "text": "Probability Distributions vs Sampling Distributions\n\nA probability distribution describes how a random variable behaves in the population.\n\nExample: X \\sim \\textsf{Binomial}(n, p)\n\nA sampling distribution describes how a statistic (e.g., sample mean, sample proportion) behaves across repeated samples from the population.\n\nExample: \\hat{p} \\sim N\\left(p, \\frac{(p(1-p))}{n}\\right) as n \\to \\infty"
  },
  {
    "objectID": "lectures/lecture-01.html#probability-refresher-1",
    "href": "lectures/lecture-01.html#probability-refresher-1",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Probability Refresher",
    "text": "Probability Refresher\nKey Distributions in Statistical Genetics\n\n\n\nBinomial Distribution X \\sim \\textsf{Binomial}(n, p) P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\n\nUsed for: Allele counts\n\n\n\n\nNormal Distribution X \\sim N(\\mu, \\sigma^2) f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\n\nUsed for: Quantitative traits, effect sizes"
  },
  {
    "objectID": "lectures/lecture-01.html#conditional-probability-and-independence",
    "href": "lectures/lecture-01.html#conditional-probability-and-independence",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Conditional Probability and Independence",
    "text": "Conditional Probability and Independence\n\nConditional Probability: P(A|B) = \\frac{P(A \\cap B)}{P(B)}\nBayes’ Theorem: P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\nIndependence: P(A \\cap B) = P(A) \\cdot P(B) \\iff A and B are independent\n\nConditional probability and Bayes’ Theorem are crucial for understanding rare disease genetics! For example, consider a rare disease with a prevalence of 1 in 10,000. If a genetic test has a sensitivity of 99% and a specificity of 99%, we can use Bayes’ Theorem to calculate the probability that an individual has the disease given a positive test result.\n\n\\begin{align*}\nP(Disease|Positive) &= \\frac{P(Positive|Disease)P(Disease)}{P(Positive)} \\\\\n&= \\frac{(0.99)(0.0001)}{(0.99)(0.0001) + (0.01)(0.9999)} \\\\\n&\\approx 0.0098\n\\end{align*}"
  },
  {
    "objectID": "lectures/lecture-01.html#parameter-estimation",
    "href": "lectures/lecture-01.html#parameter-estimation",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Parameter Estimation",
    "text": "Parameter Estimation\n\nImagine a geneticist is studying allele ages (i.e., how many generations ago an allele arose via mutation) under a simplified model where new mutations arise randomly and uniformly over a fixed window — say, the last \\theta generations.\nWe could model X \\sim \\textsf{Uniform}(0, \\theta).\nWe have our observed sample \\{x_1,x_2, \\ldots, x_n\\}, ages in generations of n observed alleles.\n\nFor concreteness, let our sample be \\{0, 1, 8, 4, 3, 7, 7, 6\\}.\n\nHow should we estimate \\theta ?"
  },
  {
    "objectID": "lectures/lecture-01.html#genetic-models",
    "href": "lectures/lecture-01.html#genetic-models",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Genetic Models",
    "text": "Genetic Models\n\nUnderstanding random variables, sampling distributions, and bias/variance of estimators helps motivate biological questions, and understand how they can be answered\nLet y be the expression of a trait, G be the additive contribution of genetic variables, and an environmental value E;\nWhat assumptions do we make via the following models?\n\n\n\n\n\\begin{align}\n\ny &= G + E \\\\\ny &= \\beta_0 + \\beta_1 G + \\beta_2 E + \\epsilon \\\\\ny &= \\beta_0 + \\beta_1 G + \\beta_2 E + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2) \\\\\ny &= \\beta_0 + \\beta_1 G + \\beta_2 E + \\beta_3 \\left(G \\times E\\right) + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2)\n\n\\end{align}"
  },
  {
    "objectID": "lectures/lecture-01.html#modeling",
    "href": "lectures/lecture-01.html#modeling",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Modeling",
    "text": "Modeling\n\nImagine that we have a sample size of n unrelated haploid individuals from some population\nWe want to estimate allele frequencies for a biallelic SNP, say A/a\nIn our sample, we observe x individuals with allele A and n - x individuals with allele a\nLet p be the frequency of allele A and q = 1 - p be the frequency of allele a\n\nPr(X = x| n, p) = \\binom{n}{x} p^x q^{n - x}\n\nLets say we observe n = 27 and x = 11. How do we estimate p?"
  },
  {
    "objectID": "lectures/lecture-01.html#likelihood-functions",
    "href": "lectures/lecture-01.html#likelihood-functions",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Likelihood Functions",
    "text": "Likelihood Functions\n\nThe likelihood function is a function of the parameters of a statistical model, given specific observed data.\nIt represents the plausibility of different parameter values based on the observed data.\nFor a given set of data, the likelihood function is defined as: L(\\theta | data) = P(data | \\theta)\nIn our case, the likelihood function for p is: L(p | x = 11, n = 27) = P(X = 11 | n = 27, p) = \\binom{27}{11} p^{11} (1 - p)^{16}\nWe are asking, “how likely is it that we would observe this data for different values of p?”"
  },
  {
    "objectID": "lectures/lecture-01.html#maximum-likelihood-estimator",
    "href": "lectures/lecture-01.html#maximum-likelihood-estimator",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Maximum Likelihood Estimator",
    "text": "Maximum Likelihood Estimator\nOur likelihood function for p is\nL(P|x = 11, n = 27) = {27 \\choose 11} p^{11} q^{27-11}\n\n\n\nn &lt;- 27\nx &lt;- 11\np_seq &lt;- seq(0, 1, length.out = 200)\nlikelihood &lt;- dbinom(x, n, p_seq)\nplot &lt;- ggplot(\n    data.frame(p = p_seq, likelihood = likelihood),\n    aes(x = p, y = likelihood)\n) +\n    geom_line(linewidth = 1) +\n    labs(\n        x = \"Probability of Success (p)\",\n        y = \"Likelihood\",\n        title = \"Binomial Likelihood Function\"\n    ) +\n    geom_vline(\n        xintercept = x / n,\n        color = \"red\",\n        linetype = \"dashed\"\n    ) +\n    theme_light(base_size = 18) +\n    theme(panel.grid = element_blank())"
  },
  {
    "objectID": "lectures/lecture-01.html#modeling-but-make-it-bayesian",
    "href": "lectures/lecture-01.html#modeling-but-make-it-bayesian",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Modeling, but make it bayesian",
    "text": "Modeling, but make it bayesian\n\nIn Bayesian statistics, we treat parameters as random variables and use probability distributions to represent our uncertainty about them.\nWe start with a prior distribution that reflects our beliefs about the parameter before seeing the data.\nWe then use the observed data to update our beliefs and obtain a posterior distribution.\nThe posterior distribution combines the information from the prior and the likelihood of the observed data using Bayes’ theorem: P(p | data) = \\frac{P(data | p) P(p)}{P(data)}\nLet’s say we have run a previous experiment and observed n = 20 individuals and x = 3 individuals with allele A (again, bilelic SNP).\nWe can use this information to inform our prior distribution for p"
  },
  {
    "objectID": "lectures/lecture-01.html#modeling-but-make-it-bayesian-1",
    "href": "lectures/lecture-01.html#modeling-but-make-it-bayesian-1",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Modeling, but make it bayesian",
    "text": "Modeling, but make it bayesian\n\n\n\np &lt;- seq(0, 1, length.out = 1000)\n# Prior (Beta distribution from previous experiment)\nprior &lt;- dbeta(p, 3 + 1, 17 + 1)\n# Likelihood (Beta distribution from current data)\nlikelihood &lt;- dbeta(p, 11 + 1, 16 + 1)\n# Posterior (Beta distribution combining both)\nposterior &lt;- dbeta(p, 14 + 1, 33 + 1)\n# Combine into a data frame\nplot_data &lt;- data.frame(\n    p = rep(p, 3),\n    density = c(prior, likelihood, posterior),\n    type = rep(c(\"Prior\", \"Likelihood\", \"Posterior\"), each = length(p))\n)\n# Add flat prior distribution\nflat_prior &lt;- dbeta(p, 1, 1)\nflat_posterior &lt;- dbeta(p, 12, 17)\nplot_data &lt;- rbind(\n    plot_data,\n    data.frame(\n        p = rep(p, 3),\n        density = c(flat_prior, likelihood, flat_posterior),\n        type = rep(c(\"Prior\", \"Likelihood\", \"Posterior\"), each = length(p))\n    )\n)\n# Add a column to distinguish between informative and flat scenarios\nplot_data$scenario &lt;- c(rep(\"Informative\", 3000), rep(\"Flat\", 3000))\n\n\n\n\n\n\np1 &lt;- ggplot(\n    plot_data %&gt;% filter(scenario == \"Informative\"),\n    aes(x = p, y = density, color = type)\n) +\n    geom_line(linewidth = 2) +\n    geom_vline(xintercept = 11 / 27, linetype = \"dashed\", color = \"red\") +\n    labs(\n        x = \"Allele Frequency (p)\",\n        y = \"Density\",\n        title = \"Bayesian Update with Informative Prior\"\n    ) +\n    scale_color_viridis_d() +\n    theme_minimal(base_size = 15)\n\n\n\np2 &lt;- ggplot(\n  plot_data %&gt;% filter(scenario == \"Flat\"),\n  aes(x = p, y = density, color = type, linetype = type)\n) +\n  geom_line(linewidth = 2) +\n  scale_linetype_manual(values = c(\"Prior\" = \"dotted\", \"Likelihood\" = \"solid\", \"Posterior\" = \"dashed\")) +\n  geom_vline(xintercept = 11 / 27, linetype = \"dotdash\", color = \"red\") +\n  labs(\n    x = \"Allele Frequency (p)\",\n    y = \"Density\",\n    title = \"Bayesian Update with Flat Prior\"\n  ) +\n  theme_minimal(base_size = 15) +\n  scale_color_viridis_d() +\n  annotate(\"text\",\n    x = 0.7, y = max(flat_posterior) * 0.8,\n    label = \"Likelihood ≈ Posterior\", color = \"gray30\"\n  )"
  },
  {
    "objectID": "lectures/lecture-01.html#modeling-but-make-it-bayesian-2",
    "href": "lectures/lecture-01.html#modeling-but-make-it-bayesian-2",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Modeling, but make it Bayesian",
    "text": "Modeling, but make it Bayesian"
  },
  {
    "objectID": "lectures/lecture-01.html#mendels-laws",
    "href": "lectures/lecture-01.html#mendels-laws",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Mendel’s Laws",
    "text": "Mendel’s Laws\n\n\nMendel’s First Law (Segregation)\nOne allele of each parent is randomly transmitted, with probability 1/2, to the offspring; the alleles unite randomly to form the offspring’s genotype.\nMendel’s Second Law (Independent Assortment)\nAt different loci, alleles are transmitted independently (when loci are unlinked).\n\n\n\n\n\nMendel’s First Law, the Law of Segregation, states that during the formation of gametes (sperm and egg cells), the two alleles for a given gene separate, so that each gamete receives only one allele. This means that each parent contributes one allele to their offspring, and the combination of these alleles determines the offspring’s genotype.\nMendel’s Second Law, the Law of Independent Assortment, states that alleles for different genes are transmitted independently of one another, provided the genes are located on different chromosomes or are far apart on the same chromosome. This means that the inheritance of one trait does not influence the inheritance of another trait, allowing for a variety of genetic combinations in the offspring."
  },
  {
    "objectID": "lectures/lecture-01.html#worked-example-segregation-31-ratio",
    "href": "lectures/lecture-01.html#worked-example-segregation-31-ratio",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Worked Example: Segregation (3:1 Ratio)",
    "text": "Worked Example: Segregation (3:1 Ratio)\nCross: Aa × Aa\n\n\n\nPunnett Square\n\n\n\n\n\n\nA\na\n\n\n\n\nA\nAA\nAa\n\n\na\nAa\naa\n\n\n\n\n\n\n\nExpected Ratios\n\n\n\nGenotypic: 1 AA : 2 Aa : 1 aa\nPhenotypic: 3 dominant : 1 recessive\nProbabilities: P(AA)=\\frac14, P(Aa)=\\frac12, P(aa)=\\frac14"
  },
  {
    "objectID": "lectures/lecture-01.html#test-cross-example",
    "href": "lectures/lecture-01.html#test-cross-example",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Test Cross Example",
    "text": "Test Cross Example\nProving law of segregation with a test cross\nExample data:\n\nCross: Aa × aa\nObserved: 21 dominant, 28 recessive\nNull hypothesis: Probability of transmission is 1:1.\nAlternative hypothesis: Probability of transmission is not 1:1.\nHow to test this?\n\nOur question is related to observation of proportions –&gt; use a chi-squared goodness-of-fit test or a binomial test."
  },
  {
    "objectID": "lectures/lecture-01.html#test-cross-example-1",
    "href": "lectures/lecture-01.html#test-cross-example-1",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Test Cross Example",
    "text": "Test Cross Example\n\nobserved_testcross &lt;- c(21, 28)     # Dominant, Recessive\nchisq_test &lt;- chisq.test(observed_testcross, p = c(0.5, 0.5))\nchisq_test\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed_testcross\nX-squared = 1, df = 1, p-value = 0.3173"
  },
  {
    "objectID": "lectures/lecture-01.html#testing-via-simulation",
    "href": "lectures/lecture-01.html#testing-via-simulation",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Testing via Simulation",
    "text": "Testing via Simulation\nWe could also simulate our idea of the null hypothesis to get a sense of how extreme our observed data is.\n\nset.seed(123)\nn_sim &lt;- 10000\nN &lt;- 49\nsimulated &lt;- rbinom(n_sim, N, 0.5)\np_value_sim &lt;- mean(simulated &lt;= 21 | simulated &gt;= 28)\np_value_sim\n\n[1] 0.3868\n\nhist(simulated, breaks = 30, main = \"Simulated Test Cross Outcomes\", xlab = \"Number of Dominant Offspring\")\nabline(v = c(21, 28), col = \"red\", lty = 2, lwd = 2)"
  },
  {
    "objectID": "lectures/lecture-01.html#on-simulations",
    "href": "lectures/lecture-01.html#on-simulations",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "On Simulations",
    "text": "On Simulations\n\nDowney (2016)"
  },
  {
    "objectID": "lectures/lecture-01.html#estimation-of-allele-frequencies",
    "href": "lectures/lecture-01.html#estimation-of-allele-frequencies",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Estimation of Allele Frequencies",
    "text": "Estimation of Allele Frequencies\nn_{AA} = Number of individuals with genotype AA n_{Aa} = Number of individuals with genotype Aa n_{aa} = Number of individuals with genotype aa\nwhere, n = n_{AA} + n_{Aa} + n_{aa} = N.\nThe sample proportions of allele A is:\n\n\\hat{p} = \\frac{2n_{AA} + n_{Aa}}{2n}\n\nThe usual standard error for a proportion is \\sqrt{\\hat{p}(1 - \\hat{p})/2n}, but this assumes independence of the 2n sampled chromosomes."
  },
  {
    "objectID": "lectures/lecture-01.html#hardyweinberg-equilibrium-hwe-assumptions",
    "href": "lectures/lecture-01.html#hardyweinberg-equilibrium-hwe-assumptions",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Hardy–Weinberg Equilibrium (HWE): Assumptions",
    "text": "Hardy–Weinberg Equilibrium (HWE): Assumptions\nLet’s reframe the problem in terms of genotype frequencies. First, some assumptions:\n\nRandom mating\nLarge population (no strong drift in one generation)\nNo selection, no migration, no mutation (in the generation under study)\nAccurate genotypes (no systematic genotyping error)\n\nIf these hold, and the allele frequency of A is p (so q = 1-p), we say that the population is in Hardy-Weinberg Equilibrium (HWE). The population genotype probabilities are:\n\nP(AA)=p^2 \\quad P(Aa)=2pq \\quad P(aa)=q^2\n\nAs a consequence, the genotype frequencies in a random sample of n individuals are approximately multinomially distributed with cell probabilities (p^2, 2pq, q^2). Thus, the sampling error of \\hat{p} is approximately \\sqrt{p(1-p)/2n}.\nThis confidence interval is known as a Wald confidence interval."
  },
  {
    "objectID": "lectures/lecture-01.html#testing-for-hwe",
    "href": "lectures/lecture-01.html#testing-for-hwe",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Testing for HWE",
    "text": "Testing for HWE\n\nWith large samples, we can test for HWE using a chi-squared goodness-of-fit test.\n\n\nLaird and Lange (2011)"
  },
  {
    "objectID": "lectures/lecture-01.html#hwe-worked-example",
    "href": "lectures/lecture-01.html#hwe-worked-example",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "HWE: Worked Example",
    "text": "HWE: Worked Example\n\nn &lt;- 212\nn_AA &lt;- 175\nn_Aa &lt;- 33\nn_aa &lt;- 4\n\n# Estimate allele frequency\np_hat &lt;- (2*n_AA + n_Aa) / (2*n)\nq_hat &lt;- 1 - p_hat\np_hat\n\n[1] 0.9033019\n\n# Expected counts\nexpected &lt;- c(n * p_hat^2, 2*n * p_hat*q_hat, n * q_hat^2)\nobserved &lt;- c(n_AA, n_Aa, n_aa)\nexpected\n\n[1] 172.982311  37.035377   1.982311\n\n# Pearson chi-squared\nchisq_val &lt;- sum((observed - expected)^2 / expected)\ndf &lt;- 3 - 1 - 1\np_val &lt;- pchisq(chisq_val, df = df, lower.tail = FALSE)\np_val\n\n[1] 0.1126299"
  },
  {
    "objectID": "lectures/lecture-01.html#hwe-in-practice",
    "href": "lectures/lecture-01.html#hwe-in-practice",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "HWE in Practice",
    "text": "HWE in Practice\n\nHWE is a useful null model for quality control of genotype data.\nDeviations from HWE can indicate genotyping errors, population stratification, or selection"
  },
  {
    "objectID": "lectures/lecture-01.html#sampling-distributions-intuition-via-simulation",
    "href": "lectures/lecture-01.html#sampling-distributions-intuition-via-simulation",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Sampling distributions: intuition via simulation",
    "text": "Sampling distributions: intuition via simulation\n\nset.seed(123)\ntrue_p &lt;- 0.3\nsample_sizes &lt;- c(5, 10, 100, 500)\nn_sims &lt;- 1000\n\nsampling_results &lt;- map_dfr(sample_sizes, function(n) {\n  p_hats &lt;- replicate(n_sims, {\n    # Sample n individuals (2n alleles total)\n    alleles &lt;- rbinom(n, 2, true_p)  # each individual contributes 0,1,2 A alleles\n    sum(alleles) / (2 * n)           # sample allele frequency\n  })\n  tibble(sample_size = n, p_hat = p_hats,\n         theoretical_se = sqrt(true_p * (1 - true_p) / (2 * n)))\n})"
  },
  {
    "objectID": "lectures/lecture-01.html#sampling-distributions-intuition-via-simulation-1",
    "href": "lectures/lecture-01.html#sampling-distributions-intuition-via-simulation-1",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Sampling distributions: intuition via simulation",
    "text": "Sampling distributions: intuition via simulation\n\n# Plot sampling distributions\nggplot(sampling_results, aes(x = p_hat)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30, alpha = 0.7) +\n  geom_vline(xintercept = true_p, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  facet_wrap(~sample_size_name, scales = \"free_y\") +\n  labs(title = \"Sampling Distribution of p̂\", x = \"Sample Allele Frequency (p̂)\", y = \"Density\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/lecture-01.html#references",
    "href": "lectures/lecture-01.html#references",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "References",
    "text": "References\n\n\n\n\nDowney,A. (2016) Probably overthinking it: There is still only one test.\n\n\nLaird,N.M. and Lange,C. (2011) The fundamentals of modern statistical genetics Springer Science, New York.\n\n\nSörensen,D. (2025) Statistical Learning in Genetics: An Introduction Using R 2nd ed. 2025. Springer Nature Switzerland, Cham."
  },
  {
    "objectID": "lectures/lecture-03.html#review-likelihood-function-basics",
    "href": "lectures/lecture-03.html#review-likelihood-function-basics",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Review: Likelihood Function Basics",
    "text": "Review: Likelihood Function Basics\n\nLikelihood function: \\(L(\\boldsymbol{\\theta} \\mid \\text{data}) = P(\\text{data} \\mid \\boldsymbol{\\theta})\\)\nMaximum Likelihood Estimation (MLE)\nSimple one-parameter examples\n\nHowever often direct MLE is not possible because\n\nClosed-form solution does not exist\nMissing or latent data\nHigh-dimensional or constrained parameter space leads to non-convex surface with multiple local maxima\n\n\nScript:\n“Let’s start by aligning on what likelihood means. We take the sampling model \\(p(\\text{data}\\mid\\theta)\\) and, after we’ve observed the data, we view it as a function of the parameter \\(\\theta\\). That function is the likelihood \\(L(\\theta)\\). The MLE is simply the value of \\(\\theta\\) that maximizes \\(L\\).\nTwo reminders that will be useful all semester: first, the invariance property—if \\(\\hat\\theta\\) maximizes \\(L(\\theta)\\), then \\(g(\\hat\\theta)\\) maximizes the likelihood of \\(g(\\theta)\\). Second, the curvature of the log-likelihood, via Fisher information, anticipates precision.\nIn practice, closed forms fail us for three common reasons: there’s no algebraic solution; there’s missing or latent data; or the parameter space is constrained or high-dimensional so the surface is bumpy. Today we’ll build the numerical toolkit that handles these realities.”"
  },
  {
    "objectID": "lectures/lecture-03.html#agenda",
    "href": "lectures/lecture-03.html#agenda",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Agenda",
    "text": "Agenda\n\nOptimization for MLE: Newton–Raphson, Gradient Descent, SGD\nEM algorithm: gene frequencies and incomplete data\nLinkage analysis: two-point, unknown phase EM, multipoint HMM\nLinkage disequilibrium (LD) and its measures (D, D’, r)\nAssociation: single-marker/haplotype tests and basic QC\n\n\nScript:\n“Here’s the plan. We’ll warm up with Newton–Raphson and first‑order methods—how to actually move on a likelihood surface. We’ll then introduce EM as a principled way to deal with missing or latent pieces, anchored by an ABO example.\nNext, we pivot to linkage: first two‑point LOD, then an EM formulation for unknown phase, and finally the multipoint HMM perspective. We’ll connect that to LD, clarify \\(D\\), \\(D'\\) and \\(r\\), and finish with a single‑marker association demo and the QC that keeps results honest.”"
  },
  {
    "objectID": "lectures/lecture-03.html#newton-raphson-method-for-mle",
    "href": "lectures/lecture-03.html#newton-raphson-method-for-mle",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Newton-Raphson Method for MLE",
    "text": "Newton-Raphson Method for MLE\n\nGeneral procedure to find \\(x\\) such that \\(g(x) = 0\\)\nUses Taylor Expansion, for \\(g(x)\\) about a point \\(x = a\\):\n\n\\[\\begin{align*}\ng(x) = g(a) &+\n\\frac{g'(a)(x-a)}{1!} + \\frac{g''(a)(x-a)^2}{2!} \\\\\n&+ ... + \\frac{g^{(n)}(a)(x-a)^n}{n!}\n\\end{align*}\\]\n\nScript:\n“In MLE we set \\(g(\\theta)=\\ell'(\\theta)\\), the score, and look for a root. Newton’s update is\n\\[ \\theta{(t+1)}={(t)}-.\\]\nIn multiple dimensions it’s \\(\\boldsymbol{\\theta}^{(t+1)}=\\boldsymbol{\\theta}^{(t)}-\\mathbf{H}^{-1}\\mathbf{g}\\), where \\(\\mathbf{g}\\) is the gradient and \\(\\mathbf{H}\\) the Hessian.\nThree practical notes. First, Newton is quadratically convergent when you’re close to the solution—that’s why it’s fast. Second, guard against wild steps with damping or line search, and keep feasibility in mind if your parameter must lie on a simplex or in \\([0,0.5]\\). Third, if the Hessian misbehaves, use Fisher scoring (replace \\(-\\mathbf{H}\\) with expected information) or a trust region.”"
  },
  {
    "objectID": "lectures/lecture-03.html#gradient-descent",
    "href": "lectures/lecture-03.html#gradient-descent",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nFirst-order iterative optimization algorithm\nDoes not rely on matrix inversions \\(\\longrightarrow\\) better for high dimensional problems\nVisualizer from UCLA\n\n\nScript:\n“First‑order methods move along the steepest descent direction without ever inverting a matrix. That’s a superpower in high‑dimensional genetics models where Hessians are huge. The tradeoff: convergence is typically linear, not quadratic.\nWhat matters in practice is the step size: constant steps are brittle; backtracking or scheduled decay is safer. Stochastic and mini‑batch variants are essential when your objective decomposes over people, markers, or families. The mental model is simple: noisy but cheap steps that converge ‘on average’.”"
  },
  {
    "objectID": "lectures/lecture-03.html#the-em-algorithm",
    "href": "lectures/lecture-03.html#the-em-algorithm",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "The EM Algorithm",
    "text": "The EM Algorithm\n\nExpectation-Maximization Algorithm\nKey idea: transform an incomplete data problem into a complete data problem\n\n\\[\np\\left(y \\mid \\theta \\right) = \\frac{p\\left(y, z \\mid \\theta \\right)}{p\\left(z \\mid y, \\theta\\right)}\n\\]\n\nScript:\n“EM turns missingness into bookkeeping. We invent latent variables \\(z\\) so that the complete‑data log-likelihood \\(\\log p(y,z\\mid\\theta)\\) is easy to maximize. At iterate \\(t\\):\n\nE‑step: compute \\(Q(\\theta\\mid\\theta^{(t)})=\\mathbb{E}_{z\\mid y,\\theta^{(t)}}[\\log p(y,z\\mid\\theta)]\\).\n\nM‑step: update \\(\\theta^{(t+1)}=\\arg\\max_\\theta Q(\\theta\\mid\\theta^{(t)})\\).\n\nWhy does it work? By Jensen’s inequality, EM monotonically increases the observed log-likelihood. The feel is ‘stable but sometimes slow’; we’ll flag accelerations like ECME and PX‑EM later.”"
  },
  {
    "objectID": "lectures/lecture-03.html#em-for-gene-frequencies",
    "href": "lectures/lecture-03.html#em-for-gene-frequencies",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "EM for Gene Frequencies",
    "text": "EM for Gene Frequencies\n\n\n\n\n\nGenotype\nPhenotype\nObserved Counts\nGenotype Frequency\n\n\n\n\n\\(AA\\)\n\\(A\\)\n\\(n_A\\)\n\\(p_A^2\\)\n\n\n\\(AO\\)\n\\(A\\)\n\n\\(2 p_A p_O\\)\n\n\n\\(AB\\)\n\\(AB\\)\n\\(n_{AB}\\)\n\\(2 p_A p_B\\)\n\n\n\\(BB\\)\n\\(B\\)\n\\(n_B\\)\n\\(p_B^2\\)\n\n\n\\(BO\\)\n\\(B\\)\n\n\\(2 p_B p_O\\)\n\n\n\\(OO\\)\n\\(O\\)\n\\(n_O\\)\n\\(p_O^2\\)\n\n\n\n\n\n\nScript:\n“Here’s the ABO mapping. The assumption is Hardy–Weinberg and random mating, so genotype frequencies are the usual \\(p^2\\), \\(2pq\\), \\(q^2\\). The ambiguity sits exactly in the A and B phenotypes: each collapses over two genotypes. EM will split those cells into expected counts using the current allele frequencies and then re‑estimate the frequencies as if those expected counts were observed.”"
  },
  {
    "objectID": "lectures/lecture-03.html#em-for-gene-frequencies-1",
    "href": "lectures/lecture-03.html#em-for-gene-frequencies-1",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "EM for Gene Frequencies",
    "text": "EM for Gene Frequencies\n\nWe don’t know if \\(n_A\\) is from \\(AA\\) or \\(AO\\)! (And similarly for \\(n_B\\))\nHowever, given our data model, we can find values of \\((\\hat{p}_A, \\hat{p}_B)\\) via EM\n\n\nScript:\n“Think of each A phenotype as a soft label between AA and AO, with probabilities determined by the current \\((p_A,p_O)\\). Same story for B. That’s the essence of the E‑step.”"
  },
  {
    "objectID": "lectures/lecture-03.html#em-for-gene-frequencies-2",
    "href": "lectures/lecture-03.html#em-for-gene-frequencies-2",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "EM for Gene Frequencies",
    "text": "EM for Gene Frequencies\n\nStart with initial \\((p_A^{(0)}, p_B^{(0)})\\) and set \\(p_O^{(0)} = 1 - p_A^{(0)} - p_B^{(0)}\\).\nE-step: allocate ambiguous phenotype counts:\n\n\\(A\\) phenotype: \\(\\tilde n_{AA} = n_A \\frac{(p_A^{(t)})^2}{(p_A^{(t)})^2 + 2 p_A^{(t)} p_O^{(t)}}\\), \\(\\tilde n_{AO} = n_A \\frac{2 p_A^{(t)} p_O^{(t)}}{(p_A^{(t)})^2 + 2 p_A^{(t)} p_O^{(t)}}\\).\n\\(B\\) phenotype: \\(\\tilde n_{BB} = n_B \\frac{(p_B^{(t)})^2}{(p_B^{(t)})^2 + 2 p_B^{(t)} p_O^{(t)}}\\), \\(\\tilde n_{BO} = n_B \\frac{2 p_B^{(t)} p_O^{(t)}}{(p_B^{(t)})^2 + 2 p_B^{(t)} p_O^{(t)}}\\).\n\n\n\nScript:\n“These are just conditional probabilities. Given the person has phenotype A, what’s the chance they’re AA versus AO under the current \\((p_A,p_O)\\)? Multiply by \\(n_A\\) to get expected genotype counts \\(\\tilde n_{AA}\\) and \\(\\tilde n_{AO}\\). Do the same for B. That’s the E‑step.”"
  },
  {
    "objectID": "lectures/lecture-03.html#em-for-gene-frequencies-3",
    "href": "lectures/lecture-03.html#em-for-gene-frequencies-3",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "EM for Gene Frequencies",
    "text": "EM for Gene Frequencies\n\nM-step (with \\(N\\) total individuals):\n\n\\[\\begin{aligned}\np_A^{(t+1)} &= \\frac{2\\tilde n_{AA} + \\tilde n_{AO} + n_{AB}}{2N},\\\\\np_B^{(t+1)} &= \\frac{2\\tilde n_{BB} + \\tilde n_{BO} + n_{AB}}{2N},\\\\\np_O^{(t+1)} &= 1 - p_A^{(t+1)} - p_B^{(t+1)}\n\\end{aligned}\\]\n\nIterate until parameter changes are below a tolerance.\n\n\nScript:\n“Now treat the expected genotype table as complete and recount alleles: two from each homozygote and one from each heterozygote, divided by \\(2N\\). That gives updated \\((p_A,p_B,p_O)\\). Iterate E and M until the changes are tiny and the observed log‑likelihood is no longer increasing.”"
  },
  {
    "objectID": "lectures/lecture-03.html#em-for-gene-frequencies-4",
    "href": "lectures/lecture-03.html#em-for-gene-frequencies-4",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "EM for Gene Frequencies",
    "text": "EM for Gene Frequencies\n\n\n\n\n\nGenotype\nPhenotype\nObserved Counts\nGenotype Frequency\n\n\n\n\n\\(AA\\)\n\\(A\\)\n\\(725\\)\n\\(p_A^2\\)\n\n\n\\(AO\\)\n\\(A\\)\n\n\\(2 p_A p_O\\)\n\n\n\\(AB\\)\n\\(AB\\)\n\\(72\\)\n\\(2 p_A p_B\\)\n\n\n\\(BB\\)\n\\(B\\)\n\\(258\\)\n\\(p_B^2\\)\n\n\n\\(BO\\)\n\\(B\\)\n\n\\(2 p_B p_O\\)\n\n\n\\(OO\\)\n\\(O\\)\n\\(1073\\)\n\\(p_O^2\\)\n\n\n\n\n\n\nScript:\n“With these sample counts—A = 725, AB = 72, B = 258, O = 1073—we can run EM. I’ll show convergence behavior next, and I want you to notice two things: the monotone increase in log‑likelihood and the impact of starting values.”"
  },
  {
    "objectID": "lectures/lecture-03.html#em-starting-values-matter",
    "href": "lectures/lecture-03.html#em-starting-values-matter",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "EM: Starting Values Matter",
    "text": "EM: Starting Values Matter\n\nDifferent initial \\((p_A, p_B, p_O)\\) can change speed (not the final MLE)\nStrategies:\n\nEqual: \\((1/3, 1/3, 1/3)\\)\nUnbalanced guess: \\((0.01, 0.98, 0.01)\\)\nSmart (use \\(p_O^{(0)} = \\sqrt{n_O/N}\\), solve for \\((p_A^{(0)}, p_B^{(0)})\\) from \\(2p_A p_B\\))\n\nTolerance of \\(1 \\times 10^{-6}\\)\n\n\nScript:\n“The ‘smart start’ uses two near‑identities: \\(n_O \\approx N p_O^2\\) gives \\(p_O^{(0)}\\approx\\sqrt{n_O/N}\\); then \\(n_{AB}\\approx 2N p_A p_B\\) plus \\(p_A+p_B=1-p_O^{(0)}\\) gives a quadratic for \\((p_A^{(0)},p_B^{(0)})\\). This gets you close to the basin of attraction and trims iterations substantially. Equal and wild unbalanced starts are good stress tests.”"
  },
  {
    "objectID": "lectures/lecture-03.html#em-starting-values-matter-1",
    "href": "lectures/lecture-03.html#em-starting-values-matter-1",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "EM: Starting Values Matter",
    "text": "EM: Starting Values Matter\n\n\n\n\n\nStrategy\nIterations\n\\(\\hat{p}_A\\)\n\\(\\hat{p}_B\\)\n\\(\\hat{p}_O\\)\n\n\n\n\nEqual\n7\n0.209131\n0.080801\n0.710068\n\n\nSmart\n4\n0.209131\n0.080801\n0.710068\n\n\nUnbalanced\n8\n0.209131\n0.080801\n0.710068\n\n\n\n\n\n\nScript:\n“Read this table left to right. Each row is a starting strategy; the last three columns are the converged allele frequency estimates. The point is not that the answers differ—they don’t—but that the iterations column can vary a lot. You can buy free speed by initializing intelligently.”"
  },
  {
    "objectID": "lectures/lecture-03.html#em-parameter-trajectories",
    "href": "lectures/lecture-03.html#em-parameter-trajectories",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "EM Parameter Trajectories",
    "text": "EM Parameter Trajectories\n\n\nScript:\n“Here are the trajectories for \\(p_A\\). Different starts head toward the same fixed point, and the paths are smooth. If you ever see oscillation or jaggedness here, it’s a warning sign about numerical underflow or a bad update rule.”"
  },
  {
    "objectID": "lectures/lecture-03.html#when-does-em-fail",
    "href": "lectures/lecture-03.html#when-does-em-fail",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "When does EM fail?",
    "text": "When does EM fail?\n\nNon-identifiability / flat likelihood: Ridges in segregation or linkage models; EM wanders or stalls.\nLocal maxima & starts: Multiple modes (e.g., mixture of penetrance classes); poor initialization traps EM.\nBoundary degeneracy: Rare allele/component weight driven to 0; variance or frequency estimates collapse.\nModel misspecification: Violated assumptions (e.g., Hardy-Weinberg, stratification) give misleading “convergence.”\n\n\nScript:\n“EM is not magic. If your model is not identifiable, EM will happily march along a ridge forever. If the likelihood is multimodal, bad starts can trap you. Near boundaries—think vanishing allele frequency—updates can collapse and stick. And if assumptions like HWE are wrong, EM will converge…to the wrong place. The antidotes are multiple starts, likelihood profiling, and stress‑testing the assumptions.”"
  },
  {
    "objectID": "lectures/lecture-03.html#linkage-vs-association-testing",
    "href": "lectures/lecture-03.html#linkage-vs-association-testing",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Linkage vs Association Testing",
    "text": "Linkage vs Association Testing\n\nLinkage: Tracks co-segregation of markers and traits within families to map disease genes.\n\nNull hypothesis: no linkage (independent assortment), \\(\\theta = 0.5\\).\n\nAssociation: Tests for correlation between variants and traits in populations to pinpoint causal loci.\n\nNull hypothesis: no association (e.g., \\(\\beta=0\\) or OR=1).\n\n\n\nScript:\n“Two mapping mindsets. Linkage works within families, asks whether a trait and a marker co‑segregate more than chance, and summarizes evidence on a centimorgan scale with LOD curves. It’s robust to population structure. Association works across unrelateds, tests regression coefficients or odds ratios, and resolves at kilobase scale, but is sensitive to confounding. We need both lenses.”"
  },
  {
    "objectID": "lectures/lecture-03.html#two-point-linkage-terms",
    "href": "lectures/lecture-03.html#two-point-linkage-terms",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Two-Point Linkage: Terms",
    "text": "Two-Point Linkage: Terms\n\nIndependent assortment (recall): when loci are unlinked, transmissions are independent and the chance a crossover separates them is \\(\\theta=0.5\\) (Mendel’s Second Law).\nInformative transmission/meiosis: a parent \\(\\to\\) child transmission where the transmitting parent is heterozygous at the loci of interest and the genotypes allow us to tell whether a crossover occurred.\n\nExample: parent \\(Aa/Bb\\) transmits \\(AB\\) or \\(ab\\) (nonrecombinant) vs \\(Ab\\) or \\(aB\\) (recombinant).\n\n\n\nScript:\n“An informative meiosis is one where you can tell whether a crossover happened. If the transmitting parent is not heterozygous at both loci, you can’t tell—so that meiosis doesn’t contribute information about \\(\\theta\\).”"
  },
  {
    "objectID": "lectures/lecture-03.html#two-point-linkage-terms-1",
    "href": "lectures/lecture-03.html#two-point-linkage-terms-1",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Two-Point Linkage: Terms",
    "text": "Two-Point Linkage: Terms\n\nRecombinant fraction \\(\\theta\\): the probability that a crossover occurs between two loci in a single meiosis; \\(0 \\le \\theta \\le 0.5\\).\nNonrecombinant transmission: the child receives an allele combination that matches one of the transmitting parent’s original allele pairs (no crossover between the loci).\nRecombinant transmission: the child receives a new combination relative to the transmitting parent’s original pair (a crossover occurred between the loci).\n\n\nScript:\n“Operationally, nonrecombinants copy one of the parental haplotypes; recombinants mix them. Our complete‑data likelihoods will be binomial in the counts of these two outcomes.”"
  },
  {
    "objectID": "lectures/lecture-03.html#two-point-linkage-repulsion-vs-coupling",
    "href": "lectures/lecture-03.html#two-point-linkage-repulsion-vs-coupling",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Two-Point Linkage: Repulsion vs Coupling",
    "text": "Two-Point Linkage: Repulsion vs Coupling\n\n\nScript:\n“Phase matters. In coupling, \\(AB/ab\\), nonrecombinants are \\(\\{AB,ab\\}\\) and recombinants are \\(\\{Ab,aB\\}\\). In repulsion, \\(Ab/aB\\), those roles flip. When phase is unknown, we’ll treat it as latent and estimate a posterior weight on the two configurations.”"
  },
  {
    "objectID": "lectures/lecture-03.html#two-point-linkage-direct-counting",
    "href": "lectures/lecture-03.html#two-point-linkage-direct-counting",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Two-Point Linkage: Direct Counting",
    "text": "Two-Point Linkage: Direct Counting\n\nCount \\(R\\) recombinants and \\(NR\\) nonrecombinants; total informative \\(I=R+NR\\).\nPoint estimate: \\(\\hat{\\theta}=R/I\\).\nLOD from counts (vs. independence at \\(\\theta=0.5\\)):\n\n\\[\n\\mathrm{LOD}(\\theta) = \\log_{10}\\!\\left\\{ \\frac{\\theta^{R} (1-\\theta)^{NR}}{0.5^{I}} \\right\\}\n\\]\n\nThis is a likelihood ratio test!\n\n\nScript:\n“In the simplest case, count recombinants \\(R\\) and informatives \\(I\\), and compute \\(\\hat\\theta=R/I\\). The LOD is a base‑10 log likelihood ratio against \\(\\theta=0.5\\). The curve peaks at \\(\\hat\\theta\\) and tells you where the evidence lies.”"
  },
  {
    "objectID": "lectures/lecture-03.html#worked-example-direct-counting-lod",
    "href": "lectures/lecture-03.html#worked-example-direct-counting-lod",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Worked Example: Direct Counting LOD",
    "text": "Worked Example: Direct Counting LOD\n\nSuppose \\(I=40\\) informative meioses with \\(R=12\\) recombinants.\nMLE \\(\\hat{\\theta}=R/I=12/40=0.3\\).\nFor \\(\\theta=0.3\\)\n\n\\[\n\\mathrm{LOD}(0.3) = \\log_{10}\\!\\left\\{ \\frac{0.3^{12} \\cdot 0.7^{28}}{0.5^{40}} \\right\\}.\n\\]\n\nScript:\n“With \\(R=12\\) out of \\(I=40\\), \\(\\hat\\theta=0.30\\). Plugging into the LOD gives about \\(1.43\\), i.e., roughly 27:1 odds for linkage at the MLE. That’s suggestive, but below the classical \\(LOD\\ge 3\\) bar. Modern practice often uses permutation‑based thresholds instead of a hard rule.”"
  },
  {
    "objectID": "lectures/lecture-03.html#worked-example-direct-counting-lod-1",
    "href": "lectures/lecture-03.html#worked-example-direct-counting-lod-1",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Worked Example: Direct Counting LOD",
    "text": "Worked Example: Direct Counting LOD\n\nR &lt;- 12\nN &lt;- 40\nthetahat &lt;- R / N\n\nlod &lt;- log10((thetahat^R * (1 - thetahat)^(N - R)) / (0.5^N))\nlod\n\n[1] 1.4294\n\n10^lod\n\n[1] 26.87819\n\n\n\nMaximize \\(\\mathrm{LOD}(\\theta)\\) over \\(\\theta \\in [0,0.5]\\) to estimate the recombination fraction; \\(LOD \\ge 3\\) and \\(\\leq -2\\) are classic heuristics (context-dependent).\n\n\nScript:\n“This code reproduces the arithmetic: LOD around 1.43, odds around 27:1. The maximizer of the LOD is exactly \\(R/I\\). The \\(\\pm\\) thresholds are heuristics, not laws—use context and proper genome‑wide calibration.”"
  },
  {
    "objectID": "lectures/lecture-03.html#worked-example-direct-counting-lod-2",
    "href": "lectures/lecture-03.html#worked-example-direct-counting-lod-2",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Worked Example: Direct Counting LOD",
    "text": "Worked Example: Direct Counting LOD\n\nUnder \\(H_0:\\theta=0.5\\) (a boundary point), \\(D = 2\\ln(10)\\cdot \\mathrm{LOD}(\\hat\\theta)\\) has the mixture limit \\(\\tfrac{1}{2}\\chi^2_0 + \\tfrac{1}{2}\\chi^2_1\\); thus \\(p = \\tfrac{1}{2}\\, \\Pr(\\chi^2_1 \\ge D)\\).\n\n\nD &lt;- 2 * log(10) * lod\npval &lt;- 0.5 * pchisq(D, df = 1, lower.tail = FALSE)\npval\n\n[1] 0.005148931\n\n\n\nScript:\n“At the boundary \\(\\theta=0.5\\), Wilks’ theorem gives a mixture null: half the mass at zero degrees of freedom and half at one. Concretely, compute \\(D=2\\ln(10)\\cdot \\mathrm{LOD}\\) and take \\(p=\\tfrac{1}{2}\\Pr(\\chi^2_1\\ge D)\\). This is the right conversion for two‑point tests against \\(\\theta=0.5\\).”"
  },
  {
    "objectID": "lectures/lecture-03.html#two-point-linkage-em-with-unknown-phase",
    "href": "lectures/lecture-03.html#two-point-linkage-em-with-unknown-phase",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Two-Point Linkage: EM with Unknown Phase",
    "text": "Two-Point Linkage: EM with Unknown Phase\n\nTransmitting parent: double het \\(Aa/Bb\\) with unknown phase.\nMate: Aa/bb (heterozygous at \\(A\\), homozygous at \\(B\\)).\n\nObserved per child: two-locus genotypes \\((A\\text{-genotype}, B\\text{-genotype})\\).\n\n\nScript:\n“Now a case where EM is actually necessary. The mate is bb, so the child’s \\(B\\) allele comes entirely from the transmitting parent. That’s helpful. But the mate is Aa at \\(A\\), so whenever a child is Aa, we can’t tell whether the transmitting parent contributed \\(A\\) or \\(a\\). Those are ambiguous transmissions between two haplotypes—exactly the kind of soft labeling EM was built for.”"
  },
  {
    "objectID": "lectures/lecture-03.html#two-point-linkage-em-with-unknown-phase-1",
    "href": "lectures/lecture-03.html#two-point-linkage-em-with-unknown-phase-1",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Two-Point Linkage: EM with Unknown Phase",
    "text": "Two-Point Linkage: EM with Unknown Phase\n\nMissing-data view: the parental phase (coupling vs repulsion) is not observed. We make it a complete-data problem by introducing a weight \\(w\\in[0,1]\\) that blends the two phase-specific models.\nModel (given recombination fraction \\(\\theta\\)):\n\nCoupling: \\(P(\\text{NR})=1-\\theta\\), \\(P(\\text{R})=\\theta\\) (split equally across the two categories).\nRepulsion: \\(P(\\text{NR})=\\theta\\), \\(P(\\text{R})=1-\\theta\\).\n\n\n\nScript:\n“Think of phase as a two‑component latent class: coupling vs repulsion. We’ll carry a single scalar, \\(w\\), the posterior weight on coupling. Under coupling, nonrecombinants are more likely by \\(1-\\theta\\); under repulsion the roles swap. EM will update \\(w\\) and then update \\(\\theta\\) from weighted recombinant counts.”"
  },
  {
    "objectID": "lectures/lecture-03.html#two-point-linkage-em-with-unknown-phase-2",
    "href": "lectures/lecture-03.html#two-point-linkage-em-with-unknown-phase-2",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Two-Point Linkage: EM with Unknown Phase",
    "text": "Two-Point Linkage: EM with Unknown Phase\nE-step (update the phase weight)\n\\[\nw = \\frac{(1-\\theta^{(t)})^{\\,n_{\\text{NR}}}\\,(\\theta^{(t)})^{\\,n_{\\text{R}}}}\n{(1-\\theta^{(t)})^{\\,n_{\\text{NR}}}\\,(\\theta^{(t)})^{\\,n_{\\text{R}}} + (\\theta^{(t)})^{\\,n_{\\text{NR}}}\\,(1-\\theta^{(t)})^{\\,n_{\\text{R}}}}\n\\]\nand use \\(1-w\\) as the weight on the repulsion configuration.\n\nScript:\n“This is the posterior weight on coupling: likelihood under coupling divided by the sum of likelihoods under both phases. In code, use logs and a log‑sum‑exp trick to avoid underflow if \\(n_{\\text{NR}}\\) or \\(n_{\\text{R}}\\) is large.”\n\nM-step (update \\(\\theta\\))\n\\[\\begin{align*}\n\\theta^{(t+1)}\n&= \\frac{\\text{(weighted recombinants)}}{N}\n= \\frac{w\\,n_{\\text{R}} + (1-w)\\,n_{\\text{NR}}}{N}.\n\\end{align*}\\]\n\nScript:\n“The M‑step is ‘recombinants over informatives’, but with a twist: we average the recombinant counts under coupling and repulsion using the current \\(w\\). That keeps the intuition intact.”"
  },
  {
    "objectID": "lectures/lecture-03.html#from-two-point-em-to-multipoint",
    "href": "lectures/lecture-03.html#from-two-point-em-to-multipoint",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "From Two-Point EM to Multipoint",
    "text": "From Two-Point EM to Multipoint\n\nSame objective: estimate recombination while marginalizing over latent transmissions/phase under current \\(\\theta\\).\nE-step engine: compute required state weights via Elston–Stewart peeling (pedigrees) or Lander–Green forward–backward (marker HMM) when you have multiple markers and missing genotypes.\nQuantities needed: \\(\\mathbb E[\\#\\,\\text{recombinants between adjacent markers}]\\) and \\(\\mathbb E[\\#\\,\\text{informative transmissions}]\\) under current map.\nUse in practice: either (a) plug these expectations into an EM-style update, or (b) more commonly, scan positions to build a LOD curve and report peak and 1-LOD interval.\n\n\nScript:\n“In multipoint linkage, the hidden object is the inheritance state along the chromosome. The forward–backward algorithm gives us posterior state probabilities; from those we compute expected recombinant counts between markers and assemble LOD curves across the map. It’s the same EM logic, just powered by an HMM.”"
  },
  {
    "objectID": "lectures/lecture-03.html#multipoint-linkage-hmm",
    "href": "lectures/lecture-03.html#multipoint-linkage-hmm",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Multipoint Linkage (HMM)",
    "text": "Multipoint Linkage (HMM)\n\nMarkers along a chromosome define an HMM over inheritance states; adjacent recombination rates drive transitions.\nCombine penetrance with the marker HMM to compute \\(L(\\theta)\\) efficiently as you slide along the map (forward–backward yields the needed state probabilities even with missing phase/genotypes).\nPros: more information than two-point; better localization. Cons: requires a genetic map and error modeling.\n\n\nScript:\n“Two practical cautions: include a genotyping‑error model—even a small error rate flattens LODs if ignored—and consider sex‑specific maps or interference if your study demands it. Otherwise, the HMM machinery scales cleanly.”"
  },
  {
    "objectID": "lectures/lecture-03.html#missing-data-em-in-linkage",
    "href": "lectures/lecture-03.html#missing-data-em-in-linkage",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Missing-Data EM in Linkage",
    "text": "Missing-Data EM in Linkage\n\nComplete data: informative meioses labeled as recombinant/nonrecombinant; incomplete data: untyped genotypes/phase.\nE-step: compute \\(E[\\text{\\# recombinants}]\\) and \\(\\mathrm E[\\text{\\# informative meioses}]\\) given current \\(\\theta^{(t)}\\) via peeling/HMM.\nM-step: \\(\\displaystyle \\theta^{(t+1)} = \\frac{\\mathrm E[R\\mid\\text{data},\\theta^{(t)}]}{\\mathrm E[I\\mid\\text{data},\\theta^{(t)}]}\\).\nIterate across \\(\\theta\\) grid to produce a LOD curve; maximize or report support interval.\n\n\nScript:\n“This slide distills the recipe: compute expected recombinants and informatives under the current map, then update \\(\\theta\\) as their ratio. In practice we report the LOD curve and a 1‑LOD support interval rather than a single point estimate.”"
  },
  {
    "objectID": "lectures/lecture-03.html#practical-issues-in-linkage",
    "href": "lectures/lecture-03.html#practical-issues-in-linkage",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Practical Issues in Linkage",
    "text": "Practical Issues in Linkage\n\nModel specification: penetrance, phenocopy, allele frequencies; misspecification can distort LOD.\nMarker selection: highly polymorphic markers increase information; account for sex-specific recombination if needed.\nComputational trade-offs: many markers and large pedigrees require approximations or pruning; consider parametric vs nonparametric linkage.\n\n\nScript:\n“Linkage analyses are only as good as their trait model. Probe sensitivity to penetrance and phenocopy. Use polymorphic markers to maximize information, and thin dense SNPs if LD bloats state spaces. For large pedigrees, Elston–Stewart pruning is your friend.”"
  },
  {
    "objectID": "lectures/lecture-03.html#linkage-disequilibrium",
    "href": "lectures/lecture-03.html#linkage-disequilibrium",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Linkage Disequilibrium",
    "text": "Linkage Disequilibrium\n\nLet alleles at two markers be denoted \\(A, a\\) and \\(B, b\\)\nDefine allele frequencies by \\(P_A, P_a, P_B, P_b\\)\nDefine frequencies of haplotypes as \\(P_{AB}, P_{Ab}, P_{aB}, P_{ab}\\)\nDefine linkage equilibrium as independence in the \\(2 \\times 2\\) table\n\nFor example, \\(P_{AB} = P_A \\times P_B\\)\n\n\n\nScript:\n“LD is simply statistical dependence between alleles on the same haplotype. Think forces: drift and finite population size create dependence, recombination erodes it, selection and admixture can inflate it.”"
  },
  {
    "objectID": "lectures/lecture-03.html#linkage-disequilibrium-1",
    "href": "lectures/lecture-03.html#linkage-disequilibrium-1",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Linkage Disequilibrium",
    "text": "Linkage Disequilibrium\n\nDefine the LD coefficient as \\(D\\)\n\n\\[D = p_{AB} - p_A p_B\\]\n\nWhat are we looking at here?\n\n\nScript:\n“Algebraically, \\(D\\) is the covariance between the allele indicators on a random haplotype. If \\(D&gt;0\\), \\(AB\\) is over‑represented relative to independence; if \\(D&lt;0\\), it’s under‑represented.”"
  },
  {
    "objectID": "lectures/lecture-03.html#linkage-disequilibrium-2",
    "href": "lectures/lecture-03.html#linkage-disequilibrium-2",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Linkage Disequilibrium",
    "text": "Linkage Disequilibrium\n\nDefine the LD coefficient as \\(D\\)\n\n\\[D = p_{AB} - p_A p_B\\]\n\nNote that\n\n\\(\\operatorname{E}[A] = p_A, \\operatorname{E}[B] = p_B\\)\n\\(\\operatorname{E}[AB] = p_{AB}\\)\n\\(\\operatorname{Cov}(A, B) = \\operatorname{E}[AB] - \\operatorname{E}[A]\\operatorname{E}[B]\\)\n\n\n\nScript:\n“Formalizing that intuition: let \\(A,B\\in\\{0,1\\}\\) indicate the presence of alleles on a haplotype. Then \\(D=\\operatorname{Cov}(A,B)\\). This is why correlation \\(r\\) shows up naturally on the next slide.”"
  },
  {
    "objectID": "lectures/lecture-03.html#linkage-disequilibrium-3",
    "href": "lectures/lecture-03.html#linkage-disequilibrium-3",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Linkage Disequilibrium",
    "text": "Linkage Disequilibrium\n\n\\(D\\), or \\(\\operatorname{Cov}(A, B)\\), is scale-dependent\nNormalized measures:\n\n\\(D' = D / D_{\\max}\\), where \\(D_{\\max} = \\min(p_A p_b, p_a p_B)\\) if \\(D &gt; 0\\)\nPearson correlation:\n\n\n\\[\\begin{align}\nr\n&= \\frac{\\operatorname{Cov}(A, B)}{\\sqrt{\\operatorname{Var}(A) \\operatorname{Var}(B)}} \\\\\n&= \\frac{D}{\\sqrt{p_A(1 - p_A) p_B(1 - p_B)}}\n\\end{align}\\]\n\nScript:\n“\\(D\\) depends on allele frequencies, so we normalize. Two popular choices: \\(D'\\) and \\(r\\). For association power, \\(r\\)—and really \\(r^2\\)—matters most because it directly scales the noncentrality parameter: effect sizes at a tag SNP are attenuated by about \\(r\\) relative to the causal.”"
  },
  {
    "objectID": "lectures/lecture-03.html#goal-of-association-ld-link",
    "href": "lectures/lecture-03.html#goal-of-association-ld-link",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Goal of Association & LD Link",
    "text": "Goal of Association & LD Link\n\nGoal: test the null of no effect (e.g., \\(\\beta=0\\) or OR\\(=1\\)) to identify regions harboring causal variants.\nWe test markers, not necessarily the causal variant. If a marker has nonzero LD (correlation \\(r\\)) with a causal variant, its association test can detect signal from that causal.\nStronger \\(|r|\\) implies a stronger expected signal at the marker; if \\(r\\approx 0\\), the marker carries no information about that causal.\nSign matters: \\(r&lt;0\\) flips the observed effect direction at the marker relative to the causal.\nIn practice, nearby markers within LD blocks show clustered p-values and similar effect signs.\n\n\nScript:\n“Here’s the bridge: association tests fire at markers. If a marker correlates with a causal by \\(r\\), the observed effect is roughly \\(r\\) times the causal effect, and power scales with \\(r^2\\). That’s why p‑values cluster within LD blocks and why effect signs can flip if \\(r&lt;0\\).”"
  },
  {
    "objectID": "lectures/lecture-03.html#single-marker-association-core-test-linear",
    "href": "lectures/lecture-03.html#single-marker-association-core-test-linear",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Single-Marker Association: Core Test (Linear)",
    "text": "Single-Marker Association: Core Test (Linear)\n\nLinear trait: \\(Y=\\alpha+\\beta G+\\gamma^T C+\\varepsilon\\); test \\(H_0{:}\\,\\beta=0\\).\nGenotype encodings: additive (0/1/2) or genotypic (AA/Aa/aa); include dominance to test non-additivity.\nBinary outcomes (case–control) and logistic models will be covered later.\n\n\nScript:\n“Regression 101 with genetics details. Code genotypes additively unless biology suggests dominance or recessivity; add a dominance term if you want to test non‑additivity. Always include relevant covariates—sex, age, and PCs for ancestry—because stratification can fake associations.”"
  },
  {
    "objectID": "lectures/lecture-03.html#basic-qc-pipeline-variant-level",
    "href": "lectures/lecture-03.html#basic-qc-pipeline-variant-level",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Basic QC Pipeline (Variant-level)",
    "text": "Basic QC Pipeline (Variant-level)\n\nCall rate/missingness (e.g., variant missingness &lt; 2%).\nHardy–Weinberg equilibrium in controls (e.g., P &gt; 1e−6), mindful of true deviations.\nMinor allele frequency threshold (e.g., MAF ≥ 0.01 unless rare-variant methods used).\nDifferential missingness across case/control; strand/allele checks.\n\n\nScript:\n“QC is prevention. Missingness filters remove low‑quality sites; HWE in controls flags genotyping problems; MAF thresholds avoid unstable estimates unless you’re using rare‑variant methods. Don’t forget strand and allele checks—mismatches there create phantom effects.”"
  },
  {
    "objectID": "lectures/lecture-03.html#basic-qc-pipeline-sample-level",
    "href": "lectures/lecture-03.html#basic-qc-pipeline-sample-level",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Basic QC Pipeline (Sample-level)",
    "text": "Basic QC Pipeline (Sample-level)\n\nSample call rate; sex checks from X chr; heterozygosity outliers.\nRelatedness/duplicates via IBD; ancestry via PCA; remove outliers or adjust with PCs.\nDuplicates/cryptic relatedness: retain one per pair or use mixed models.\n\n\nScript:\n“At the sample level, verify sex against X‑chromosome heterozygosity, look for heterozygosity outliers, and identify relatives with IBD. Use PCA to quantify ancestry and either exclude outliers or adjust downstream. Mixed models help with relatedness, but don’t use them as an excuse to skip QC.”"
  },
  {
    "objectID": "lectures/lecture-03.html#summary-key-takeaways",
    "href": "lectures/lecture-03.html#summary-key-takeaways",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Summary & Key Takeaways",
    "text": "Summary & Key Takeaways\n\nLikelihood optimization: Newton–Raphson (score/Hessian), Gradient Descent/SGD (scalable), and EM (latent-data) are core tools.\nEM for ABO gene frequencies: initialization affects speed; log-likelihood increases monotonically until convergence.\nLinkage: two-point LOD and unknown phase via EM; multipoint mapping framed as an HMM for efficient inference with missing data.\nLD basics: r and r^2 quantify correlation between markers and explain why nearby markers can show similar association signals when a causal variant is unobserved\n\n\nScript:\n“Take‑home messages. Newton and first‑order methods are your optimization workhorses. EM turns missingness into a tractable expectation–maximization cycle and increases the log‑likelihood monotonically. Linkage is about family transmissions; association is about population correlation. And \\(r^2\\) is the currency that converts LD into expected association signal.”"
  },
  {
    "objectID": "lectures/lecture-03.html#hands-on-lab-03-applied",
    "href": "lectures/lecture-03.html#hands-on-lab-03-applied",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Hands-On: Lab 03 (Applied)",
    "text": "Hands-On: Lab 03 (Applied)\n\nImplement ABO EM and compare starts; plot log-likelihood gap.\nTwo-point linkage: LOD grid search + EM with unknown phase.\nTwo-SNP haplotype EM; compute LD (D, r^2).\nMini association demo: linear regression + HWE QC in controls.\n\nSee labs/unsolved/lab-03.R for the scaffold (with TODOs) and labs/solved/lab-03.R for a worked solution.\n\nScript:\n“For the lab: keep a log of starting values and attained log‑likelihoods—this is good hygiene with EM. In the two‑point EM, verify that ambiguous transmissions are being split sensibly as \\(\\theta\\) and \\(w\\) update. In the haplotype EM, watch for boundary estimates at very low MAFs. And in the association mini‑demo, confirm that including PCs meaningfully reduces ancestry structure before you trust the p‑values.”"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Helpful Links",
    "section": "",
    "text": "Linear Algebra Primer\nProbability Primer\nStatistical Inference Primer\nStatistics Cheatsheet\nEM Algorithm Video",
    "crumbs": [
      "Useful Links"
    ]
  },
  {
    "objectID": "links.html#mathematical-resources",
    "href": "links.html#mathematical-resources",
    "title": "Helpful Links",
    "section": "",
    "text": "Linear Algebra Primer\nProbability Primer\nStatistical Inference Primer\nStatistics Cheatsheet\nEM Algorithm Video",
    "crumbs": [
      "Useful Links"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Schedule",
    "section": "",
    "text": "Article links direct to files hosted on the Zotero group library\n\n\n\nWeek\nLecture\nReadings\nAssignment\n\n\n\n\n1\nFoundations: Mendelian genetics & statistical basics\nMendel’s laws, Hardy–Weinberg equilibrium, \\(\\chi^2\\)goodness‑of‑fit. Sampling distributions; linking population parameters to sample estimates. One‑locus likelihood: building and interpreting likelihood functions.\n\nSorensen Chapter 1\nEdwards, A. W. F. (2008), “G. H. Hardy (1908) and Hardy–Weinberg Equilibrium,” Genetics.\nIntroduction to Probability Theory\n\nProblem Set 01\n\n\n2\nHeritability, segregation, and the gene-mapping toolkit\nNarrow and broad sense heritability; variance-component interpretation. Segregation analysis and modelling genetic inheritance without marker data.\n\nSorensen Chapter 2.1-2.2, 2.4, 2.8\nVisscher, P. M., et al., (2008), “Heritability in the genomics era — concepts and misconceptions,” Nature Reviews Genetics.\n\nProblem Set 02\n\n\n3\nLikelihood algorithms & practical gene mapping\nNewton-Raphson, EM, and stochastic gradient algorithms for complex likelihoods. Pedigree linkage analysis, LOD-score calculation, and missing-data EM steps. Single-marker and haplotype association tests with basic quality control\n\nSorensen, Chapter 3\nLander, E. S., and Green, P. (1987), “Construction of multilocus genetic linkage maps in humans.,” Proceedings of the National Academy of Sciences of the United States of America.\n\nProblem Set 03\n\n\n4\nPopulation structure & Bayesian fundamentals\nDetecting and correcting for population stratification and admixture confounding. Priors, posteriors, and the Bayes-frequentist debate in genetic inference. Bayesian admixture/STRUCTURE-style modelling implemented in Stan.\n\nSorensen 4.1-4.5, 4.7-4.8, 5.1\nPorras-Hurtado, L. et al., (2013), “An overview of STRUCTURE: applications, parameter settings, and supporting software,” Frontiers in Genetics.\nLawson, D. J. et al., (2018), “A tutorial on how not to over-interpret STRUCTURE and ADMIXTURE bar plots,” Nature Communications.\n\nProblem Set 04\n\n\n5\nGWAS at Scale – design, LMMs, and meta-analysis\nEnd to end GWAS workflow: sample QC, variant QC, power, pitfalls. Linear mixed models (BOLT LMM/REML concepts) and SAIGE for imbalance/relatedness. Fixed/random effects meta-analysis; genomic inflation and calibration.\n\nLoh, P.-R. et al., (2015), “Efficient Bayesian mixed model analysis increases association power in large cohorts,” Nature Genetics.\n\nProblem Set 05\n\n\n6\nPrediction models in genetics\n\nSorensen Chapters 6, 7.1-7.2, 10.1, 10.5, 11.3-11.5\nWu, T. T., Chen, Y. F., Hastie, T., Sobel, E., and Lange, K. (2009), “Genome-wide association analysis by lasso penalized logistic regression,” Bioinformatics, 25, 714–721. https://doi.org/10.1093/bioinformatics/btp041.\n\n\n\n\n7\nMultiple testing & false-discovery control\n\nSorensen Chapter 8\nOtani, T., Noma, H., Nishino, J., and Matsui, S. (2018), “Re-assessment of multiple testing strategies for more efficient genome-wide association studies,” European Journal of Human Genetics, Nature Publishing Group, 26, 1038–1048. https://doi.org/10.1038/s41431-018-0125-3.\n\n\n\n\n8\nBinary traits and family-based association tests\n\nSorensen Chapter 9\nZhou, W., Bi, W., Zhao, Z., Dey, K. K., Jagadeesh, K. A., Karczewski, K. J., Daly, M. J., Neale, B. M., and Lee, S. (2022), “SAIGE-GENE+ improves the efficiency and accuracy of set-based rare variant association tests,” Nature Genetics, Nature Publishing Group, 54, 1466–1469. https://doi.org/10.1038/s41588-022-01178-w.\n\n\n\n\n9\nCausal inference & functional integration (Mendelian randomization)\n\nSanderson, E., Glymour, M. M., Holmes, M. V., Kang, H., Morrison, J., Munafò, M. R., Palmer, T., Schooling, C. M., Wallace, C., Zhao, Q., and Davey Smith, G. (2022), “Mendelian randomization,” Nature Reviews Methods Primers, 2, 1–21. https://doi.org/10.1038/s43586-021-00092-5.\n\n\n\n\n10\nAdvanced AI Topics in Statistical Genetics: Language Models for Genomics\nRecommended:\n\nJi et al., 2021\nAvsec et al., 2021\nCheng et al., 2023\nBaghbanzadeh et al., 2025\nMollerus et al., 2025",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "assignments/assignment-02.html",
    "href": "assignments/assignment-02.html",
    "title": "Assignment 02",
    "section": "",
    "text": "Due before class on Wednesday, September 10th.\nRequirements:"
  },
  {
    "objectID": "assignments/assignment-02.html#problem-1-missing-heritability-and-rare-variants-40-pts",
    "href": "assignments/assignment-02.html#problem-1-missing-heritability-and-rare-variants-40-pts",
    "title": "Assignment 02",
    "section": "Problem 1: Missing Heritability and Rare Variants (40 pts)",
    "text": "Problem 1: Missing Heritability and Rare Variants (40 pts)\nRecall (Young 2019): “The first challenge is one of precision. The information used to estimate heritability from rare variants by GREML-WGS comes from the variation in sharing of rare variants among distantly related pairs of individuals. However, distantly related individuals typically do not share any particular rare variant, so the variation in rare variant sharing is low. This means that large samples with high quality WGS data are required to obtain precise estimates, and such samples are not common yet. Based on the only existing application of GREML-WGS, a sample size of ~40,000 would produce estimates precise enough to be statistically distinguished from other heritability estimates. It is likely that this challenge will be overcome shortly, since samples of similar magnitude already exist.”\n\nAssume the probability that two distantly related individuals share a rare variant is p=0.001. Assume a sample size of n=40,000 individuals.\n\nCalculate the expected number of pairs of individuals in this sample who share a rare variant. How many total pairs of individuals exist in the sample?\nIf we estimate that rare variants contribute h^2_{\\text{rare}}=0.10 to heritability, calculate the standard error of this estimate given the sample size. Use the formula \\text{SE}(h^2) \\approx \\frac{2}{\\sqrt{n_{\\text{eff}}}}, where n_{\\text{eff}} is the effective number of independent observations (approximately the number of pairs sharing rare variants).\nCalculate a 95% confidence interval for the heritability estimate. Does this confidence interval allow us to distinguish between h^2_{\\text{rare}}=0.10 and h^2_{\\text{common}}=0.25\n\nBriefly explain in 2-3 sentences how the “missing heritability” problem relates to rare variants, and why larger samples with whole-genome sequencing may be needed to resolve this question."
  },
  {
    "objectID": "assignments/assignment-02.html#problem-2-parentoffspring-regression-with-assortative-mating-40-pts",
    "href": "assignments/assignment-02.html#problem-2-parentoffspring-regression-with-assortative-mating-40-pts",
    "title": "Assignment 02",
    "section": "Problem 2: Parent–offspring regression with assortative mating (40 pts)",
    "text": "Problem 2: Parent–offspring regression with assortative mating (40 pts)\nLet Y=A+E with \\operatorname{Var}(A)=\\sigma_A^2, \\operatorname{Var}(E)=\\sigma_E^2, random environments, and phenotypic mate correlation \\operatorname{corr}(Y_{\\text{father}},Y_{\\text{mother}})=r_m.\n\nShow that \\operatorname{Var}(Y_{\\text{mid-parent}})=\\frac{1}{2}(1+r_m)\\operatorname{Var}(Y).\nGiven the regression slope \\beta=\\frac{\\operatorname{Cov}(Y_o,Y_{mp})}{\\operatorname{Var}(Y_{mp})}, show that \\beta \\;=\\; \\frac{\\sigma_A^2}{\\sigma_A^2+\\sigma_E^2}\\cdot\\frac{1}{1+r_m} \\;=\\; \\frac{h^2}{1+r_m}. Interpret the direction of bias in \\beta for r_m&gt;0.\nFor h^2=0.5, compute \\beta for r_m=0,0.1,0.3,0.5. Comment on the practical impact of assortative mating on parent–offspring regression."
  },
  {
    "objectID": "assignments/assignment-02.html#problem-3-derivation-of-falconers-formula-from-the-ace-model-20-pts",
    "href": "assignments/assignment-02.html#problem-3-derivation-of-falconers-formula-from-the-ace-model-20-pts",
    "title": "Assignment 02",
    "section": "Problem 3: Derivation of Falconer’s Formula from the ACE Model (20 pts)",
    "text": "Problem 3: Derivation of Falconer’s Formula from the ACE Model (20 pts)\nBackground: The ACE model is a foundational tool in quantitative genetics for partitioning phenotypic variance (V_P) into three components: additive genetic effects (A), common or shared environmental effects (C), and unique or non-shared environmental effects (E). Under this model, the total variance is given by V_P = V_A + V_C + V_E.\nThe intraclass correlations for a trait between monozygotic (MZ) and dizygotic (DZ) twins are given by:\nr_{\\text{MZ}} = \\frac{\\operatorname{Cov}(Y_1, Y_2 \\mid \\text{MZ})}{V_P}\nr_{\\text{DZ}} = \\frac{\\operatorname{Cov}(Y_1, Y_2 \\mid \\text{DZ})}{V_P}\nAssume that:\n\nMating is random (no assortative mating).\nThere are no gene-environment interactions or correlations.\nThe equal environments assumption holds (MZ and DZ twins experience their shared environments to a similar degree).\nGenetic effects are purely additive (no dominance or epistasis).\n\nAssuming the ACE model, demonstrate that the narrow-sense heritability (h^2 = V_A/V_P) can be estimated as twice the difference between the MZ and DZ twin correlations.\nShow that:\nh^2 = 2(r_{\\text{MZ}} - r_{\\text{DZ}})"
  },
  {
    "objectID": "assignments/assignment-03.html",
    "href": "assignments/assignment-03.html",
    "title": "Assignment 03",
    "section": "",
    "text": "Requirements:"
  },
  {
    "objectID": "assignments/assignment-03.html#problem-1-em-for-abo-gene-frequencies-derivation-inference-and-sensitivity-25-pts",
    "href": "assignments/assignment-03.html#problem-1-em-for-abo-gene-frequencies-derivation-inference-and-sensitivity-25-pts",
    "title": "Assignment 03",
    "section": "Problem 1: EM for ABO gene frequencies; derivation, inference, and sensitivity (25 pts)",
    "text": "Problem 1: EM for ABO gene frequencies; derivation, inference, and sensitivity (25 pts)\n\nPart A (15 pts)\nConsider phenotype counts from the ABO system under Hardy–Weinberg equilibrium (HWE): n_A, n_{AB}, n_B, n_O with total N. Let genotype frequencies be p_A^2, 2p_Ap_O, 2p_Ap_B, p_B^2, 2p_Bp_O, p_O^2 and p_O = 1 - p_A - p_B.\nDerive the EM updates shown in lecture. Specifically, show that the E‑step allocations for A and B phenotypes are\n\n\\tilde n_{AA} = n_A \\dfrac{p_A^2}{p_A^2 + 2p_Ap_O}\n\n\n\\tilde n_{AO} = n_A \\dfrac{2p_Ap_O}{p_A^2 + 2p_Ap_O}\n\nand similarly for BB, BO, and that the M‑step is \np_A^{(t+1)}=\\frac{2\\tilde n_{AA}+\\tilde n_{AO}+n_{AB}}{2N}\n \np_B^{(t+1)}=\\frac{2\\tilde n_{BB}+\\tilde n_{BO}+n_{AB}}{2N}\n \np_O^{(t+1)}=1-p_A^{(t+1)}-p_B^{(t+1)}\n\nHint (how to derive EM generally):\n\nChoose latent variables so the complete data are simple. Here, treat latent genotype counts, \n(n_{AA}, n_{AO}, n_{AB}, n_{BB}, n_{BO}, n_{OO})\n\n\nas missing, with only phenotype totals observed.\n\nWrite the complete‑data log‑likelihood\n\n\\begin{align*}\n\\ell_c(p_A,p_B) =\n& n_{AA}\\log p_A^2 + n_{AO}\\log(2p_Ap_O) + n_{AB}\\log(2p_Ap_B) + \\\\\n& n_{BB}\\log p_B^2 + n_{BO}\\log(2p_Bp_O) + n_{OO}\\log p_O^2\n\\end{align*}\nusing p_O=1-p_A-p_B.\n\nE‑step: replace latent counts by their conditional expectations given the observed phenotypes under current parameters, e.g., for phenotype A, and form Q(p\\,|\\,p^{(t)}) = \\mathbb E[\\ell_c(p)\\mid\\text{data}, p^{(t)}].\nM‑step: M-step: Maximize Q(p|p^{(t)}) subject to p_A + p_B + p_O = 1. Hint: Consider rewriting the objective in terms of allele counts rather than genotype counts.\n\n\n\nPart B (10 pts)\nConsider two biallelic SNPs with haplotypes \\{ab, aB, Ab, AB\\} under HWE and unphased genotypes (g_1,g_2)\\in\\{0,1,2\\}^2. Note that (ab, ab) yields (0,0), (ab, aB) or (ab, Ab) yields (0,1), (AB, AB) yields (2,2), etc.\nShow that if every observed genotype is (1,1), then P\\{(1,1)\\}=2\\,\\big(p_{ab}p_{AB}+p_{aB}p_{Ab}\\big) and the likelihood depends only on the cross‑sum S=p_{ab}p_{AB}+p_{aB}p_{Ab} (a ridge; parameters not identifiable)."
  },
  {
    "objectID": "assignments/assignment-03.html#problem-2-twopoint-linkage-lod-and-support-intervals-25-pts",
    "href": "assignments/assignment-03.html#problem-2-twopoint-linkage-lod-and-support-intervals-25-pts",
    "title": "Assignment 03",
    "section": "Problem 2: Two‑point linkage — LOD and support intervals (25 pts)",
    "text": "Problem 2: Two‑point linkage — LOD and support intervals (25 pts)\nSuppose one heterozygous transmitting parent (A/a and B/b) is crossed to an aabb mate, yielding child haplotype counts (n_{AB}, n_{Ab}, n_{aB}, n_{ab}). Let n_{\\text{NR}}=n_{AB}+n_{ab} and n_{\\text{R}}=n_{Ab}+n_{aB}.\n\nPart A: LOD from counts (10 pts).\nShow that for a given recombination fraction \\theta\\in(0,0.5) the two‑point LOD relative to independence (\\theta=0.5) is \n\\mathrm{LOD}(\\theta)\n= \\log_{10}\\!\\left\\{ \\frac{\\theta^{\\,n_{\\text{R}}} (1-\\theta)^{\\,n_{\\text{NR}}}}{0.5^{\\,n_{\\text{R}}+n_{\\text{NR}}}} \\right\\}.\n\nDerive the MLE \\hat\\theta and show it equals n_{\\text{R}}/(n_{\\text{R}}+n_{\\text{NR}}) when 0&lt;\\hat\\theta&lt;0.5.\n\n\nPart B: Unknown phase and LD‑informed LOD (15 pts)\nA heterozygous transmitting parent (A/a,\\;B/b) has unknown phase: either coupling (AB/ab) or repulsion (Ab/aB). Let \nn_{\\mathrm{NR}} = n_{AB}+n_{ab},\\qquad\nn_{\\mathrm{R}}  = n_{Ab}+n_{aB},\\qquad\nN=n_{\\mathrm{NR}}+n_{\\mathrm{R}}.\n\nLet w=\\Pr\\{\\text{coupling }(AB/ab)\\} and 1-w=\\Pr\\{\\text{repulsion }(Ab/aB)\\}.\n(i) Mixture likelihood (5 pts).\nShow that with unknown phase the observed‑data likelihood is a mixture of the two phase‑specific binomial likelihoods: \nL(\\theta; w)\n= w\\,(1-\\theta)^{\\,n_{\\mathrm{NR}}}\\,\\theta^{\\,n_{\\mathrm{R}}}\n\\;+\\;\n(1-w)\\,(1-\\theta)^{\\,n_{\\mathrm{R}}}\\,\\theta^{\\,n_{\\mathrm{NR}}}.\n Hence the two‑point LOD relative to independence (\\theta=0.5) is \n\\mathrm{LOD}(\\theta; w)\n= \\log_{10}\\!\\left\\{\n\\frac{w\\,(1-\\theta)^{\\,n_{\\mathrm{NR}}}\\theta^{\\,n_{\\mathrm{R}}}\n+(1-w)\\,(1-\\theta)^{\\,n_{\\mathrm{R}}}\\theta^{\\,n_{\\mathrm{NR}}}}\n{0.5^{\\,N}}\n\\right\\}.\n\n(ii) Linking LD to w (5 pts).\nLet population haplotype frequencies be p=(p_{ab}, p_{aB}, p_{Ab}, p_{AB}) (sum to 1).\nCondition on the parent being the double heterozygote (g_1,g_2)=(1,1). Use Bayes’ rule to show \nw\n= \\Pr\\{(ab,AB)\\mid(1,1)\\}\n= \\frac{p_{ab}\\,p_{AB}}{p_{ab}\\,p_{AB}+p_{aB}\\,p_{Ab}},\n\n\n1-w\n= \\frac{p_{aB}\\,p_{Ab}}{p_{ab}\\,p_{AB}+p_{aB}\\,p_{Ab}}.\n\nDefine D = p_{ab}p_{AB}-p_{aB}p_{Ab} and note that \\operatorname{sign}(D) indicates whether coupling (D&gt;0) or repulsion (D&lt;0) phase is a priori more likely.\n(iii) Quick numerical check (5 pts).\nTake p^\\star=(0.40,\\,0.10,\\,0.25,\\,0.25).\nCompute w and 1-w. Then, using the example counts (n_{AB}, n_{Ab}, n_{aB}, n_{ab})=(18, 5, 4, 17) (so N=44, n_{\\mathrm{R}}=9, n_{\\mathrm{NR}}=35), evaluate and compare \\mathrm{LOD}(\\hat\\theta; w=\\tfrac12) versus \\mathrm{LOD}(\\hat\\theta; w) at \\hat\\theta = n_{\\mathrm{R}}/N.\nBriefly explain (one sentence) how LD information (w\\neq\\tfrac12) can increase or decrease the peak LOD when phase is unknown."
  },
  {
    "objectID": "assignments/assignment-03.html#problem-3-twosnp-haplotype-em-and-ld-measures-25-pts",
    "href": "assignments/assignment-03.html#problem-3-twosnp-haplotype-em-and-ld-measures-25-pts",
    "title": "Assignment 03",
    "section": "Problem 3: Two‑SNP haplotype EM and LD measures (25 pts)",
    "text": "Problem 3: Two‑SNP haplotype EM and LD measures (25 pts)\n\nPart A (10 pts)\nFor two biallelic SNPs with haplotypes \\{ab, aB, Ab, AB\\} at frequencies \\{p_{ab},p_{aB},p_{Ab},p_{AB}\\} (summing to 1), enumerate the possible haplotype pairs consistent with each unphased genotype (g_1,g_2)\\in\\{0,1,2\\}^2.\nShow that only (g_1,g_2)=(1,1) is ambiguous with two possible pairs: (ab,AB) and (aB,Ab).\n\n\nPart B (10 pts)\nSimulate N=1000 individuals from true haplotype frequencies p^{\\star}=(0.40,0.10,0.25,0.25) and estimate \\hat p via EM from a uniform start. Report \\hat p and absolute errors |\\hat p - p^{\\star}|.\nNote that the E‑step weights for (1,1) are proportional to p_{ab}p_{AB} and p_{aB}p_{Ab}, and the M‑step update is p^{(t+1)}=\\text{(expected hap counts)}/(2N).\nHere is a sample R code snippet to get you started:\n\nset.seed(8878)\n\nN &lt;- 1000\np_true &lt;- c(ab = 0.40, aB = 0.10, Ab = 0.25, AB = 0.25)\n\n# helper: draw N unordered haplotype pairs, then make unphased genotypes (g1,g2)\ndraw_genotypes &lt;- function(N, p) {\n    # code haplotypes to allele counts (B allele) at SNP1, SNP2\n    H &lt;- rbind(ab = c(0, 0), aB = c(0, 1), Ab = c(1, 0), AB = c(1, 1))\n    hap1 &lt;- sample(rownames(H), size = N, replace = TRUE, prob = p)\n    hap2 &lt;- sample(rownames(H), size = N, replace = TRUE, prob = p)\n    G &lt;- H[hap1, ] + H[hap2, ] # N x 2 matrix with entries in {0,1,2}\n    as.data.frame(G) |&gt;\n        setNames(c(\"g1\", \"g2\"))\n}\n\n# simulate data\ndat &lt;- draw_genotypes(N, p_true)\n\n\n\nPart C (5 pts)\nCompute D = p_{11} - p_{B1}p_{B2} with p_{11}=p_{AB}, p_{B1}=p_{Ab}+p_{AB}, p_{B2}=p_{aB}+p_{AB}. Report D and r^2 = D^2/(p_{B1}(1-p_{B1})p_{B2}(1-p_{B2})). Comment on how LD would affect single‑marker association at either SNP."
  },
  {
    "objectID": "assignments/assignment-03.html#problem-4-singlemarker-association-with-qc-and-ld-attenuation-25-pts",
    "href": "assignments/assignment-03.html#problem-4-singlemarker-association-with-qc-and-ld-attenuation-25-pts",
    "title": "Assignment 03",
    "section": "Problem 4: Single‑marker association with QC and LD attenuation (25 pts)",
    "text": "Problem 4: Single‑marker association with QC and LD attenuation (25 pts)\nSimulate n=2000 unrelated individuals. Let a causal biallelic SNP C have MAF 0.30 and generate a quantitative trait Y with additive effect size \\beta_C=0.50 (per allele) and noise \\epsilon\\sim\\mathcal N(0,1). Let a tag SNP T be in LD with C such that r=\\operatorname{corr}(G_C,G_T)=0.8 and both are in HWE. Let T have MAF 0.30.\n\nPart A (15 pts)\nGenerate (G_C,G_T,Y) by first simulating haplotypes for (C,T) with a chosen LD structure that yields r\\approx 0.8, then form genotypes and Y=\\beta_C G_C + \\epsilon. Fit simple linear models Y\\sim G_C and Y\\sim G_T and report \\hat\\beta_C and \\hat\\beta_T, alongside their 95\\% confidence intervals.\n\n\nPart B (10 pts)\nIntroduce a basic QC step: test HWE in the controls of a case–control subsample formed by thresholding Y at its 80th percentile to define cases (cases are the top 20% of Y). Compute an exact or \\chi^2 HWE p‑value in controls for T; state whether you would flag T using a threshold of 10^{-6} and why QC is typically done in controls only."
  },
  {
    "objectID": "assignments/assignment-01.html",
    "href": "assignments/assignment-01.html",
    "title": "Assignment 01",
    "section": "",
    "text": "Make a Zotero account using the guide here. Make sure you use your GW email address, as this will provide unlimited cloud storage for PDFs. Once you have created your account, email chiraaggohel@gwu.edu your username.\n(Laird, 2.4) How many genotypes are possible with a 3-allele marker? With K alleles?\n(Laird, 2.6) Consider a recessive Mendelian disease, where in the population, P(\\text{an individual has 2 disease variants}) = 0.000001.\n\nWhat is the probability that a randomly selected person is affected? Suppose that the randomly selected person is affected. What does that imply about the probability that their sibling is also affected (you can assume that having either one or two parents with two variants is so rare that you can ignore them)?\nNow answer both of these questions assuming the penetrance is only \\frac{1}{2}, i.e., P(\\text{disease} | 2 \\text{ variants}) = \\frac{1}{2}, but the phenocopy rate is still zero.\n\nConsider a sample size of n of unrelated haploid individuals is obtained from some population with the objective of estimating allele frequency at a biallelic locus. The sample contains x copies of A, and n-x copies of a.\n\nPlot the probability distribution of X given n = 30, and \\theta = .1. Plot the probability distribution of X given n = 1000, and \\theta = .1.\nLets say we observed 30 samples, with 10 copies of allele A. Plot the likelihood function for \\theta\nWhat is the MLE of \\theta?\nLet’s say n = 1000, and x = 100. What is the sampling variance of \\hat{\\theta}?\nLet’s say n = 100, and x = 10. What is the sampling variance of \\hat{\\theta}? Why is this different than the result above?\n\nRefer to equations (1.3) and (1.5) in Sorensen. Say you observe 8 individuals, and 1 copies of genotype X. Assume that X \\sim \\textsf{Binom}(n, \\theta).\n\n\nCompute \\hat{\\theta}\nCompute \\hat{\\text{Var}}(\\hat{\\theta})\nProvide a 95% Wald confidence interval for \\theta\nWrite an interpretation of this confidence interval. What problem does this reveal about the Wald confidence interval?\nCompute a 95% Wilson confidence interval for \\theta. Documentation for this can be found here. Hint: you will need the fastR2 package.\n\n\nRefer to slides 13 and 14 in lecture 1. Write a one to two sentence answer for how a researcher would try to answer each question.\nWhat is your math background? What is your programming background?"
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Office hours are held on Friday’s from 10am-12pm. You can book a 30 minute slot using this link. If you need to meet outside of these hours, please email me at chiraaggohel@gwu.edu.",
    "crumbs": [
      "FAQ"
    ]
  },
  {
    "objectID": "faq.html#office-hours",
    "href": "faq.html#office-hours",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Office hours are held on Friday’s from 10am-12pm. You can book a 30 minute slot using this link. If you need to meet outside of these hours, please email me at chiraaggohel@gwu.edu.",
    "crumbs": [
      "FAQ"
    ]
  },
  {
    "objectID": "faq.html#course-location",
    "href": "faq.html#course-location",
    "title": "Frequently Asked Questions",
    "section": "Course Location",
    "text": "Course Location\nMilken SPH, 300C. The zoom link is available on Blackboard, and the Google Calendar invite.",
    "crumbs": [
      "FAQ"
    ]
  },
  {
    "objectID": "faq.html#the-required-paper-is-not-open-source-how-do-i-access-it",
    "href": "faq.html#the-required-paper-is-not-open-source-how-do-i-access-it",
    "title": "Frequently Asked Questions",
    "section": "The required paper is not open source, how do I access it?",
    "text": "The required paper is not open source, how do I access it?\nVia our shared Zotero library. If you don’t have access, please email chiraaggohel@gwu.edu to be added.",
    "crumbs": [
      "FAQ"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This graduate-level course builds a cohesive toolkit for analyzing complex genetic data, weaving together Mendelian and population-genetic principles, likelihood theory, and Bayesian inference with MCMC. Lectures progress from pedigree linkage and genome-wide association study (GWAS) design to population-structure correction, multiple-testing control, and genomic prediction using BLUP, penalized regressions, and non-parametric learners such as random forests and neural networks. Binary-trait modelling introduces logistic mixed models, AUC evaluation, and family-based tests, while hands-on R/Bioconductor labs guide students from Hardy–Weinberg simulations to full GWAS and polygenic-score pipelines. Weekly problem sets blend mathematical derivations with coding; a capstone project requires analyzing public whole-genome or single-cell data and presenting findings in a conference-style talk. By course end, participants can translate biological questions into formal statistical models, implement inference algorithms on high-dimensional data, control error rates in large-scale studies, and critically evaluate predictive models and their uncertainties.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "This graduate-level course builds a cohesive toolkit for analyzing complex genetic data, weaving together Mendelian and population-genetic principles, likelihood theory, and Bayesian inference with MCMC. Lectures progress from pedigree linkage and genome-wide association study (GWAS) design to population-structure correction, multiple-testing control, and genomic prediction using BLUP, penalized regressions, and non-parametric learners such as random forests and neural networks. Binary-trait modelling introduces logistic mixed models, AUC evaluation, and family-based tests, while hands-on R/Bioconductor labs guide students from Hardy–Weinberg simulations to full GWAS and polygenic-score pipelines. Weekly problem sets blend mathematical derivations with coding; a capstone project requires analyzing public whole-genome or single-cell data and presenting findings in a conference-style talk. By course end, participants can translate biological questions into formal statistical models, implement inference algorithms on high-dimensional data, control error rates in large-scale studies, and critically evaluate predictive models and their uncertainties.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-prerequisites",
    "href": "syllabus.html#course-prerequisites",
    "title": "Syllabus",
    "section": "Course Prerequisites",
    "text": "Course Prerequisites\n\nPUBH 6860: Principles of Bioinformatics",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-learning-objectives",
    "href": "syllabus.html#course-learning-objectives",
    "title": "Syllabus",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\n\nAnalyze Mendelian, population-genetic, and demographic models to quantify inheritance patterns, linkage, association, and population structure in diverse organisms.\nApply likelihood, Bayesian, and Markov-chain Monte Carlo techniques to estimate parameters and test hypotheses in genome-scale datasets.\nEvaluate genome-wide association studies and genomic-prediction pipelines, controlling false-discovery rates and assessing prediction bias, variance, and uncertainty.\nSynthesize multi-source genomic, phenotypic, and environmental data into reproducible R/Bioconductor workflows that meet FAIR and open-science standards.\nDesign and implement statistical learning models-shrinkage regressions, mixed models, and non-parametric methods-to predict complex traits and interpret model performance in a biological context.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#textbooks",
    "href": "syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\n\nRequired\n\nStatistical Learning in Genetics: An Introduction Using R, Daniel Sorensen, 2nd Edition (Available via the GWU Library)\n\n\n\nRecommended\n\nHandbook of Statistical Genomics, David J. Balding, Ida Moltke, John Marioni, 4th Edition (Available via the GWU Library)\nThe Fundamentals of Modern Statistical Genetics, Nan M. Laird and Christoph Lange, 1st Edition (Available via the GWU Library)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#technology-requirements",
    "href": "syllabus.html#technology-requirements",
    "title": "Syllabus",
    "section": "Technology Requirements",
    "text": "Technology Requirements\nStudents should have a desktop or laptop (Windows, macOS, or Linux) with at least 8 GB RAM, 20 GB free disk space, a reliable broadband connection (≥ 10 Mbps), and working webcam, microphone, and speakers or headphones for synchronous Zoom sessions. They must be able to navigate the university’s LMS through a modern web browser to download readings, submit assignments, and join discussion boards; install and update R (4.5 or newer), RStudio (or VS Code with the R extension), Bioconductor packages, Git, and Zoom; and use basic Git commands (clone, commit, push) to submit version-controlled lab work. We will use Zotero for file sharing and collaboration. Familiarity with screen sharing, breakout rooms, captioning, and recording in Zoom is expected. Optional but recommended tools include an SSH client or VPN for connecting to campus HPC resources and a PDF reader that supports annotation. All course materials follow WCAG 2.1 guidelines, and RStudio offers high-contrast and screen-reader modes; students who require further accommodations should contact Disability Services before the first week.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assignments-and-descriptions",
    "href": "syllabus.html#assignments-and-descriptions",
    "title": "Syllabus",
    "section": "Assignments and Descriptions",
    "text": "Assignments and Descriptions\n\n\n\nAssignment Type\n% of total grade\n\n\n\n\nProblem Sets\n60\n\n\nClass participation (defined below)\n20\n\n\nResearch Project (Final Exam)\n20\n\n\n\n\nStandard SPH Graduate Grading Scale\n\nA: 94-100%\nA-: 90-93%\nB+: 87-89%\nB: 84-86%\nB-: 80-83%\nC+: 77-79%\nC: 73-76%\nC-: 70-72%\nF: Below 70%\n\n\n\nProblem Sets\nProblem sets are key to exploring the concepts introduced both in class and in the textbook. These are meant to provide an opportunity to more deeply understand concepts and put them into practice and provide an opportunity for data manipulation. This is part of the ‘lab’ component of the course and you will be given time in class to work on assignments and collaborate. However, these problem sets will take substantial time outside of class to complete, so please plan accordingly.\n\n\nResearch Project\nStudents will choose a unique project to work on for the final third of the semester. At the end of the semester, students will submit a written project report in the form of a scientific research paper. Students are strongly encouraged to work in groups of up to three on their projects, however such groups will be expected to make proportionally more substantial contributions with clearly delineated responsibilities for each member’s contributions. Projects should be based on a research topic related to statistical genetics, but can be computational, methodological, or applied in nature. The Lab write-ups will follow the standard form of a scientific paper to gain experience in writing. Specifically, we will follow the format of the journal Genetics, the leading journal in the field. The paper MUST BE BASED ON THE PRIMARY LITERATURE IN THE APPROPRIATE REFEREED SCIENTIFIC JOURNALS, and it should adhere to the following format:\n\nBegin the paper with an original title, followed by your name, the course, and the date. All papers should be typed, single-spaced, and in 12 pt. font.\nThe paper should have the following sections:\n\nIntroduction – here you state the general problem or issue you are addressing.\nMaterials and Methods – describe the methods used to obtain data, analyze data, and test hypotheses associated with the data.\nResults – describe the results of the data analysis and hypothesis testing.\nDiscussion – here you draw conclusions about the problem you studied; this section should include a synthesis of ideas.\nLiterature Cited – List the relevant literature you have read and used to support your arguments/analyze your data. The literature cited should be in the format of the journal Genetics.\n\n\nAspects of the project will be required throughout the course with a final research project submitted in the form of a research paper during finals. See course outline for due dates for each part of project.\n\n\nWorkload\nIn this course, you will be expected to spend 5 or more hours per week in independent learning which can include reviewing assigned material, preparing for class discussions, working on assignments, and group work. In addition, 1.5 hours per week will be spent in class computational lab and 4.5 hours working on asynchronous materials provided online. The total workload for this course will be at least 112.5 hours.\n\n\nClass Policy: Statistical Genetics is Interdisciplinary and Quantitative\nThis course is highly interdisciplinary, quantitative, and technical. We will discuss algorithms, concepts, and methods from biology, computer science, and statistics. You will be asked to learn about and apply technical concepts in areas that you may have limited familiarity with. In our experience, to succeed in this class, you will have to commit to repeatedly engaging with concepts to build understanding and refine your understanding through class discussions, outside reading, and homework assignments. Willingness to think quantitatively/probabilistically is required to succeed in this course. There are a diversity of talent sets in this class and you each bring something unique to the table. Collaborate and find someone with the skills they might share if you are lacking in a particular area. Building effective teams with broad skillsets is a hallmark of effective statistical genetics.\n\n\nClass Policy: Participation and Discussion\nTeaching and learning require a team effort. We expect you to show up to synchronous sessions (on time) and be prepared for discussions. This means, you have gone through the asynchronous material and started on the homework assignment (problem sets) and come to the synchronous session with questions. You are strongly encouraged to ask questions during synchronous session to help complete your homework assignments and share thoughts and progress on a research project. Statistical genetics is an exciting and broad area with no shortage of ethical and societal implications. We welcome your points of view and respectful discussion. We also strongly encourage cooperation among students to help in each other’s understanding of the material, but homework assignments must be your own work. We would greatly appreciate any feedback on any aspects of this course, both positive and negative!\n20% of your grade is ‘Participation’. This is both a quantitative assessment of your responses to asynchronous materials as well as participation in the live synchronous sessions. Statistical Genetics is a demanding discipline that requires students to think critically and utilize high-level analytical skills regarding complex issues. The discipline requires such mastery not only in well-articulated written work, but also in thoughtful discussions between and among students and instructors. Receiving full points for participation is not simply a matter of showing up and turning work in on time. Outstanding participation grades require truly thoughtful, insightful, and well-argued contributions and leadership in class and in asynchronous prompts that demonstrate a high level of mastery of the course material.\n\n\nClass Policy: Late Work\nLate work will be accepted but with a 1% deduction per hour for the first 5 hours up to 5% deduction per day for unexcused late homework submission. All homework will be due at 11:59 pm on the designated due date unless otherwise specified. Homework assignments (Problem Sets) will typically be distributed via blackboard with a week to complete each assignment.\n\n\nClass Policy: Make-up Work/Make-up Exams\nAny student who experiences significant family or personal illness or emergency after the final withdrawal date and is unable to complete course work should ask the instructor for an incomplete for the course. Each case will be managed on an individual basis. The Incomplete Policy must be followed as outlined in the GWSPH Student Handbook.\n\n\nClass Policy: Generative Artificial Intelligence (GAI)\nStudents are permitted to use GAI tools to generate outlines of scripts and papers to help get started, but we expect heavy editing (and commenting in code) subsequently done by the student to verify functionality and understanding of code and implications of research results. Students are permitted to use GAI for coding assignments to get started but must further comment, edit, and validate their code. Any use of GAI must be acknowledged in each assignment with details on the distinction between the GAI material and the student contribution for any given assignment along with details of the prompts supplied to the GAI tool, acknowledgement of the GAI tool including tool name and version. Note also the GW University Policy on GAI.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "lectures/lecture-02.html#agenda",
    "href": "lectures/lecture-02.html#agenda",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Agenda",
    "text": "Agenda\n\nMLE Review\nNarrow-and broad-sense heritability: definitions and interpretation\nEstimating h^2 from pedigrees\nVariance components via pedigrees (LMM with A-matrix)\nBinary traits: liability-threshold, observed vs. liability scales\nFamilial aggregation for binary traits: \\lambda_R (concept \\rightarrow model)\nSegregation analysis: modeling inheritance without markers\nAscertainment as conditioning (truncation viewpoint)"
  },
  {
    "objectID": "lectures/lecture-02.html#mle-essentials",
    "href": "lectures/lecture-02.html#mle-essentials",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "MLE essentials",
    "text": "MLE essentials\n\n\n\nProbability vs Likelihood:\nL(\\theta\\mid y)\\propto p(y\\mid\\theta) (fix y, vary \\theta).\nLog-likelihood / score / information:\n\\ell(\\theta)=\\sum_i \\log p(y_i\\mid\\theta),\nS(\\theta)=\\partial\\ell/\\partial\\theta,\nI(\\theta)=-\\partial^2\\ell/\\partial\\theta\\partial\\theta^\\top.\nMLE: \\hat\\theta=\\arg\\max_\\theta \\ell(\\theta).\nLarge-sample:\n\\hat\\theta \\approx \\mathcal N\\!\\big(\\theta_0,\\; i(\\theta_0)^{-1}\\big),\ni(\\theta)=\\mathbb E[I(\\theta\\mid Y)].\nInvariance: MLE of g(\\theta) is g(\\hat\\theta).\n\n\n\n\n\n\n\n\nTip\n\n\nRegression as a likelihood\nIf e\\sim\\mathcal N(0,\\sigma^2 I), then OLS = MLE for \\beta;\n\\hat\\sigma^2=\\tfrac{1}{n}\\sum \\hat e_i^2.\n\n\n\n\n\n\n\n\n\nWarning\n\n\nPitfalls\nNon-unique maxima, flat ridges, boundary solutions, small-sample failures of asymptotics."
  },
  {
    "objectID": "lectures/lecture-02.html#lrt-unrestricted-vs-restricted-mle",
    "href": "lectures/lecture-02.html#lrt-unrestricted-vs-restricted-mle",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "LRT (unrestricted vs restricted MLE)",
    "text": "LRT (unrestricted vs restricted MLE)\n\nGoal: test H_0\\!:\\,\\theta=\\theta_0 in a model with log-likelihood \\ell(\\theta,\\eta) and nuisance \\eta.\nUnrestricted MLE: (\\hat\\theta,\\hat\\eta) = \\arg\\max_{\\theta,\\eta} \\ell(\\theta,\\eta).\nRestricted MLE under H_0: \\hat\\eta_0 = \\arg\\max_{\\eta} \\ell(\\theta_0,\\eta).\nTest statistic (fit improvement): \\Lambda = 2\\{\\ell(\\hat\\theta,\\hat\\eta) - \\ell(\\theta_0,\\hat\\eta_0)\\}.\nInterpretation: how much better the unrestricted fit is than the restricted fit; large values argue against H_0.\nAsymptotics: \\Lambda \\overset{d}{\\to} \\chi^2_q with q constraints (here q{=}1). p-value: 1-F_{\\chi^2_q}(\\Lambda).\n\n\n\n\n\n\n\nWarning\n\n\nVariance-component caveat\nWhen testing a variance component \\sigma^2=0 (boundary), the LRT null is a mixture (e.g., \\tfrac{1}{2}\\chi^2_0 + \\tfrac{1}{2}\\chi^2_1); standard \\chi^2 reference is invalid. Restricted LRTs or score tests are common alternatives."
  },
  {
    "objectID": "lectures/lecture-02.html#aggregation-heritability-and-segregation-analyses",
    "href": "lectures/lecture-02.html#aggregation-heritability-and-segregation-analyses",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Aggregation, heritability, and segregation analyses",
    "text": "Aggregation, heritability, and segregation analyses\n\n\n\nAggregation/heritability analyses: Investigating patterns of phenotypic correlation between relatives\nSegregation analysis: Finding support for a specific genetic model underlying inheritance patterns\n\n\n\n\n\n\n\n\nNote\n\n\nThese analyses do not always use molecular genetic data… so why should we care?"
  },
  {
    "objectID": "lectures/lecture-02.html#a-gap-in-estimation",
    "href": "lectures/lecture-02.html#a-gap-in-estimation",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "A gap in estimation",
    "text": "A gap in estimation\n\n\n\n\n\nYoung, A. I. (2019), “Solving the missing heritability problem,” PLOS Genetics, Public Library of Science, 15, e1008222. https://doi.org/10.1371/journal.pgen.1008222.\n\n\n\n\nAggregation and heritability analyses tend to have much higher heritability estimates of traits than genotyping methods\nUnderstanding these models may help explain this delta"
  },
  {
    "objectID": "lectures/lecture-02.html#discussion",
    "href": "lectures/lecture-02.html#discussion",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Discussion",
    "text": "Discussion\n\nThe author frames the “missing heritability” problem as the discrepancy between estimates from twin studies and those from early GWAS. Based on the text, what is the core assumption of the classical twin study design for estimating heritability? How might a violation of this assumption lead to an overestimation of heritability for certain traits?\nThe author argues that methods like RDR and Sib-Regression are more robust against certain biases. What specific confounding factor, prevalent in population-based genomic studies, are these family-based designs better at controlling for?\nIf, as the author suggests, the true narrow-sense heritability of a trait like height is closer to the 60-70% estimated by RDR and Sib-Regression than the 80% from twin studies, what are the primary sources of the remaining “gap”? Does this completely solve the missing heritability problem, or does it redefine it?"
  },
  {
    "objectID": "lectures/lecture-02.html#heritability-first-principles",
    "href": "lectures/lecture-02.html#heritability-first-principles",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Heritability, first principles",
    "text": "Heritability, first principles\nLet Y = G + E\n\nTrait Variance: \\operatorname{Var}(Y)\nVariance due to genes: \\operatorname{Var}(G)\nVariance due to environment: \\operatorname{Var}(E).\n\nIf we assume independence of G and E, then \\operatorname{Var}(Y) = \\operatorname{Var}(G) + \\operatorname{Var}(E)"
  },
  {
    "objectID": "lectures/lecture-02.html#heritability-first-principles-1",
    "href": "lectures/lecture-02.html#heritability-first-principles-1",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Heritability, first principles",
    "text": "Heritability, first principles\nWe can decompose \\operatorname{Var}(G) into different genetic effects: - Additive effects: A - Dominance effects: D - Epistatic effects: I\nSo we can write:\n\\operatorname{Var}(Y) = \\operatorname{Var}(A) + \\operatorname{Var}(D) + \\operatorname{Var}(I) + \\operatorname{Var}(E)\n\nBroad-sense: H^2 = \\operatorname{Var}(A) + \\operatorname{Var}(D) + \\operatorname{Var}(I) / \\operatorname{Var}(Y)\nNarrow-sense: h^2 =\\operatorname{Var}(A) / \\operatorname{Var}(Y)\nContext matters: h^2 depends on population, environment, and measurement; it is not a trait constant.\n\n\n\n\n\n\n\nWarning\n\n\nFamilial aggregation \\neq heritability. Shared environment and assortment can produce aggregation without genetic causation."
  },
  {
    "objectID": "lectures/lecture-02.html#estimating-h2-from-relatives-marker-free",
    "href": "lectures/lecture-02.html#estimating-h2-from-relatives-marker-free",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Estimating h^2 from relatives (marker-free)",
    "text": "Estimating h^2 from relatives (marker-free)\nParent-offspring regression\n\nY_{\\text{offspring}} = \\alpha + \\beta \\cdot Y_{\\text{mid-parent}} + \\varepsilon\n\n\nBy definition, \\beta = \\frac{\\operatorname{Cov}(Y_{\\text{offspring}}, Y_{\\text{mid-parent}})}{\\operatorname{Var}(Y_{\\text{mid-parent}})}.\nGiven that an offspring inherits half its genes from each parent, \\operatorname{Cov}(Y_{\\text{offspring}}, Y_{\\text{mid-parent}}) = \\frac{1}{2}\\operatorname{Var}(A)"
  },
  {
    "objectID": "lectures/lecture-02.html#estimating-h2-from-relatives-marker-free-1",
    "href": "lectures/lecture-02.html#estimating-h2-from-relatives-marker-free-1",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Estimating h^2 from relatives (marker-free)",
    "text": "Estimating h^2 from relatives (marker-free)\nParent-offspring regression\n\\begin{align*}\n\\operatorname{Var}(Y_{\\text{mid-parent}})\n&= \\operatorname{Var}\\left(\\frac{Y_{\\text{parent1}} + Y_{\\text{parent2}}}{2}\\right) \\\\\n&= \\frac{1}{4}\\left(\\operatorname{Var}(Y) + \\operatorname{Var}(Y)\\right) \\\\\n&= \\frac{1}{2}\\operatorname{Var}(Y)\n\\end{align*}\nPlugging this back into the expression for \\beta:\n\\beta = \\frac{\\frac{1}{2}\\operatorname{Var}(A)}{\\frac{1}{2}\\operatorname{Var}(Y)} = \\frac{\\operatorname{Var}(A)}{\\operatorname{Var}(Y)} = h^2"
  },
  {
    "objectID": "lectures/lecture-02.html#estimating-h2-from-relatives-marker-free-2",
    "href": "lectures/lecture-02.html#estimating-h2-from-relatives-marker-free-2",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Estimating h^2 from relatives (marker-free)",
    "text": "Estimating h^2 from relatives (marker-free)\nTwin Studies\n\nAssume equal environments for MZ and DZ twins, and a simplified model\n\\operatorname{Var}(Y) = \\operatorname{Var}(A) + \\operatorname{Var}(E)\nh^2 = 2(r_{\\text{MZ}} - r_{\\text{DZ}})\n\nwhere r_{\\text{MZ}} and r_{\\text{DZ}} are the correlations between monozygotic and dizygotic twins, respectively."
  },
  {
    "objectID": "lectures/lecture-02.html#pedigree-variance-components-continuous-traits",
    "href": "lectures/lecture-02.html#pedigree-variance-components-continuous-traits",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Pedigree variance components (continuous traits)",
    "text": "Pedigree variance components (continuous traits)\nModel: \n\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{u} + \\boldsymbol{\\varepsilon},\\quad\n\\mathbf{u} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{A}\\sigma_A^2),\\quad\n\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}\\sigma_E^2),\n\nwhere \\mathbf{A} is the pedigree additive relationship matrix with A_{ij}=2\\phi_{ij}.\n\nHeritability: h^2 = \\sigma_A^2 / (\\sigma_A^2 + \\sigma_E^2).\nREML estimates (\\sigma_A^2,\\sigma_E^2) efficiently; classical family estimators are special cases under balanced designs.\nACE (twin) view: \\operatorname{Cov}(\\text{MZ}) = A + C, \\operatorname{Cov}(\\text{DZ}) = \\tfrac{1}{2}A + C; h^2 = \\operatorname{Var}(A)/\\operatorname{Var}(Y), etc.\n\n\n\n\n\n\n\nWarning\n\n\nBoundary testing: testing \\sigma_A^2=0 is on the boundary. The LRT null is the mixture \\tfrac{1}{2}\\chi_0^2 + \\tfrac{1}{2}\\chi_1^2 (or use an RLRT)."
  },
  {
    "objectID": "lectures/lecture-02.html#binary-traits-via-the-liability-threshold-model",
    "href": "lectures/lecture-02.html#binary-traits-via-the-liability-threshold-model",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Binary traits via the liability-threshold model",
    "text": "Binary traits via the liability-threshold model\nIdea: a binary phenotype arises when a continuous liability \\ell crosses a threshold T.\n\nY = \\begin{cases}\n1 & \\text{if } \\ell &gt; T,\\\n0 & \\text{if } \\ell \\le T,\n\\end{cases}\n\n\n\\ell = G + E,\\quad G \\sim \\mathcal{N}(0,\\sigma_G^2),\\ E \\sim \\mathcal{N}(0,\\sigma_E^2)\nPopulation prevalence: K = \\Pr(\\ell &gt; T) = 1 - \\Phi(T) with T = \\Phi^{-1}(1-K).\nLiability-scale heritability: h_\\ell^2 = \\sigma_G^2 / (\\sigma_G^2 + \\sigma_E^2).\n\n\n\n\n\n\n\nWarning\n\n\nAssumptions: normal liability, single threshold, no G\\times E on the liability scale, correct K."
  },
  {
    "objectID": "lectures/lecture-02.html#probit-view-and-logistic-note",
    "href": "lectures/lecture-02.html#probit-view-and-logistic-note",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Probit view and logistic note",
    "text": "Probit view and logistic note\n\nProbit GLM/GLMM corresponds to a normal-liability threshold model; adding pedigree random effects on the probit scale estimates liability-scale variance components."
  },
  {
    "objectID": "lectures/lecture-02.html#observed-vs.-liability-scale-h2",
    "href": "lectures/lecture-02.html#observed-vs.-liability-scale-h2",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Observed vs. liability-scale h^2",
    "text": "Observed vs. liability-scale h^2\nFor an unascertained sample with sample prevalence P=K, \nh_{\\text{obs}}^2 ;\\approx; h_\\ell^2 \\cdot \\frac{\\phi(T)^2}{K(1-K)},\\qquad T=\\Phi^{-1}(1-K).\n\nFor case–control sampling with sample prevalence P \\ne K, \nh_\\ell^2 ;\\approx; h_{\\text{obs}}^2 \\cdot \\frac{K^2(1-K)^2}{\\phi(T)^2\\, P(1-P)}.\n\n\n\n\n\n\n\nTip\n\n\nLow-prevalence traits (K \\ll 0.5) often have h_{\\text{obs}}^2 \\ll h_\\ell^2. Always report the scale, K, and (if applicable) P."
  },
  {
    "objectID": "lectures/lecture-02.html#ascertainment-as-conditioning-truncation-view",
    "href": "lectures/lecture-02.html#ascertainment-as-conditioning-truncation-view",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Ascertainment as conditioning (truncation view)",
    "text": "Ascertainment as conditioning (truncation view)\nIf we only observe Y under an event A (e.g., Y recorded only when Y&gt;T or when a proband is affected), the correct likelihood uses the conditional density \np(y \\mid A, \\theta) ;=; \\frac{p(y \\mid \\theta),\\mathbf{1}{y \\in A}}{\\Pr(A \\mid \\theta)}.\n\n\nIgnoring ascertainment biases parameters (e.g., means, prevalences, and regression slopes).\nThe liability-threshold model and segregation analysis both require conditioning on how families were recruited."
  },
  {
    "objectID": "lectures/lecture-02.html#segregation-analysis-no-markers",
    "href": "lectures/lecture-02.html#segregation-analysis-no-markers",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Segregation analysis (no markers)",
    "text": "Segregation analysis (no markers)\nGoal: compare inheritance models (major gene vs polygenic vs mixed) using family phenotypes.\n\nLikelihood: specify penetrance by genotype \\Rightarrow build family likelihood; condition on ascertainment (e.g., proband affected).\nDominant example (Dd \\times dd): if p_D is the offspring affected probability, then with n children and n_A affected,\nN_A \\sim \\operatorname{Binom}(n, p_D)\n\\log L(p_D \\mid n_A,n) = n_A \\log p_D + (n-n_A)\\log(1-p_D)\nRecessive note (Dd \\times Dd): unaffected genotypes are ambiguous unless carriers are observed (harder identifiability).\nPitfalls: reduced penetrance, phenocopies, ascertainment, HWE assumptions."
  },
  {
    "objectID": "lectures/lecture-02.html#summary-key-takeaways",
    "href": "lectures/lecture-02.html#summary-key-takeaways",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Summary & key takeaways",
    "text": "Summary & key takeaways\n\nHeritability: H^2 vs h^2; interpretation is population- and environment-specific.\nVariance components (pedigrees): LMM with A-matrix unifies family estimators; handle boundary tests correctly.\nBinary traits: liability-threshold,\nSegregation: fit penetrance-based likelihoods; condition on ascertainment.\nMethod habit: when unsure, simulate to check intuition and bias under ascertainment."
  }
]