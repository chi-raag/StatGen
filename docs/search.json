[
  {
    "objectID": "lectures/lecture-01.html#about-the-instructors",
    "href": "lectures/lecture-01.html#about-the-instructors",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "About the Instructors",
    "text": "About the Instructors\n\n\n\n\n\n\nDr. Keith Crandall\n\n\n\n\n\n\n\nDr. Ali Rahnavard\n\n\n\n\n\n\n\nChiraag Gohel"
  },
  {
    "objectID": "lectures/lecture-01.html#introductions",
    "href": "lectures/lecture-01.html#introductions",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Introductions",
    "text": "Introductions\n\nName, Degree, any other ways you like to define yourself (favorite hot dog brand, etc.)\nCurrent research focus or research interests.\nWhat you hope to get out of this course."
  },
  {
    "objectID": "lectures/lecture-01.html#course-site",
    "href": "lectures/lecture-01.html#course-site",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Course Site",
    "text": "Course Site\nhttps://gwcbi.github.io/StatGen/"
  },
  {
    "objectID": "lectures/lecture-01.html#zotero",
    "href": "lectures/lecture-01.html#zotero",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Zotero",
    "text": "Zotero"
  },
  {
    "objectID": "lectures/lecture-01.html#course-logistics-and-expectations",
    "href": "lectures/lecture-01.html#course-logistics-and-expectations",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Course Logistics and Expectations",
    "text": "Course Logistics and Expectations\nGrading Breakdown\n\n\n\nAssignment Type\n% of Grade\n\n\n\n\nProblem Sets\n60%\n\n\nClass Participation\n20%\n\n\nResearch Project\n20%\n\n\n\nTotal Workload: 112.5 hours (5+ hrs/week independent + 6 hrs/week class/async)"
  },
  {
    "objectID": "lectures/lecture-01.html#problem-sets-60-of-grade",
    "href": "lectures/lecture-01.html#problem-sets-60-of-grade",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Problem Sets (60% of grade)",
    "text": "Problem Sets (60% of grade)\n\nWeekly assignments blending mathematical derivations with coding\nCollaborative time provided in class\nIndividual work required – substantial time outside class\nLate penalty: 1% per hour (first 5 hours), then 5% per day\nFormat: Mix of theory problems and R/Bioconductor analysis"
  },
  {
    "objectID": "lectures/lecture-01.html#class-participation-20-of-grade",
    "href": "lectures/lecture-01.html#class-participation-20-of-grade",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Class Participation (20% of grade)",
    "text": "Class Participation (20% of grade)\nMore than just showing up!\n\nCome prepared with questions from async materials\nEngage thoughtfully in discussions and problem-solving\nDemonstrate mastery through insightful contributions\nCollaborate respectfully - help others understand concepts\n\nQuantitative + Qualitative Assessment: Async responses + live session engagement"
  },
  {
    "objectID": "lectures/lecture-01.html#research-project-20-of-grade",
    "href": "lectures/lecture-01.html#research-project-20-of-grade",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Research Project (20% of grade)",
    "text": "Research Project (20% of grade)\nFinal third of semester - Choose your own adventure!\n\n\n\nProject Types\n\n\n\nComputational methods\nApplied analysis\nMethodological development\nAll based on primary literature\n\n\n\n\n\nDeliverables\n\n\n\nScientific paper (Genetics journal format)\nConference-style presentation\nGroups of up to 3 encouraged\nIndividual contributions must be clear"
  },
  {
    "objectID": "lectures/lecture-01.html#course-structure-overview",
    "href": "lectures/lecture-01.html#course-structure-overview",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Course Structure Overview",
    "text": "Course Structure Overview\nFoundation → Scale → Specialization (10 weeks)\n\nWeeks 1-4: Foundations (Mendelian genetics, heritability, likelihood algorithms, population structure & Bayesian methods)\nWeek 5: GWAS at scale (design, linear mixed models, meta-analysis)\nWeeks 6-7: Prediction models & multiple testing control\nWeeks 8-9: Binary traits & causal inference (Mendelian randomization)\nWeek 10: Advanced AI topics in statistical genetics\n\nLab Component: Hands-on R/Bioconductor workflows parallel to lectures"
  },
  {
    "objectID": "lectures/lecture-01.html#technology-setup-requirements",
    "href": "lectures/lecture-01.html#technology-setup-requirements",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Technology Setup Requirements",
    "text": "Technology Setup Requirements\n\nR (4.5+) and IDE installation\nGit setup and basic commands\nZotero for reference management\nZoom\nOptional: HPC access"
  },
  {
    "objectID": "lectures/lecture-01.html#course-textbook",
    "href": "lectures/lecture-01.html#course-textbook",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Course Textbook",
    "text": "Course Textbook\n\nSorensen (2025)"
  },
  {
    "objectID": "lectures/lecture-01.html#what-is-statistical-genetics",
    "href": "lectures/lecture-01.html#what-is-statistical-genetics",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "What is statistical genetics?",
    "text": "What is statistical genetics?\n\n\n\n\n\n\nPopulation Genetics\n\n\n\n\n\n\n\nGenetic Epidemiology\n\n\n\n\n\n\n\nQuantitative Genetics\n\n\n\n\n\n\n\nPopulation genetics is the study of evolutionary processes affecting genetic variation between organisms. A classic example is Darwin’s finches, where beak shape and size vary between islands in the Galapagos, driven by natural selection.\nGenetic Epidemiology is the study of how genetic factors contribute to health and disease in populations. A classic example is the study of genetic variants associated with diseases like diabetes or heart disease.\nQuantitative genetics is the study of continuous traits influenced by multiple genes and environmental factors. A classic example is human height, which is influenced by many genetic variants and environmental factors like nutrition. Shawn Bradley was a basketball player who as 7’6 tall, The study found Bradley had 198 more height-associated genetic variants than the average person in the sample"
  },
  {
    "objectID": "lectures/lecture-01.html#a-statistical-geneticist-may-want-to-know",
    "href": "lectures/lecture-01.html#a-statistical-geneticist-may-want-to-know",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "A statistical geneticist may want to know",
    "text": "A statistical geneticist may want to know\n\nIs there a genetic component contributing to the total variance of these traits?\nIs the genetic component of the traits driven by a few genes located on a particular chromosome, or are there many genes scattered across many chromosomes? How many genes are involved and is this a scientifically sensible question?\nAre the genes detected protein-coding genes, or are there also noncoding genes involved in gene regulation?\nHow is the strength of the signals captured in a statistical analysis related to the two types of genes? What fraction of the total genetic variation is allocated to both types of genes?\nWhat are the frequencies of the genes in the sample? Are the frequencies associated with the magnitude of their effects on the traits?\nWhat is the mode of action of the genes?"
  },
  {
    "objectID": "lectures/lecture-01.html#a-statistical-geneticist-may-want-to-know-1",
    "href": "lectures/lecture-01.html#a-statistical-geneticist-may-want-to-know-1",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "A statistical geneticist may want to know",
    "text": "A statistical geneticist may want to know\n\nWhat proportion of the genetic variance estimated in 1 can be explained by the discovered genes?\nGiven the information on the set of genes carried by an individual, will a genetic score constructed before observing the trait help with early diagnosis and prevention?\nHow should the predictive ability of the score be measured?\nAre there other non-genetic factors that affect the traits, such as smoking behavior, alcohol consumption, blood pressure measurements, body mass index and level of physical exercise?\nCould the predictive ability of the genetic score be improved by incorporation of these non-genetic sources of information, either additively or considering interactions? What is the relative contribution from the different sources of information?"
  },
  {
    "objectID": "lectures/lecture-01.html#what-does-the-data-look-like",
    "href": "lectures/lecture-01.html#what-does-the-data-look-like",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "What does the data look like?",
    "text": "What does the data look like?\nFamily/Pedigree Studies\n\nFamily/Pedigree Studies\n\nFamily studies are used to understand the inheritance patterns of traits and diseases within families. They can help identify genetic factors contributing to diseases by analyzing the occurrence of traits in related individuals.\nPedigree charts visually represent family relationships and the transmission of genetic traits across generations. They are useful for tracking the inheritance of specific traits or diseases within a family."
  },
  {
    "objectID": "lectures/lecture-01.html#what-does-the-data-look-like-1",
    "href": "lectures/lecture-01.html#what-does-the-data-look-like-1",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "What does the data look like?",
    "text": "What does the data look like?\nGenome-Wide Association Studies (GWAS)\n\nGenome-Wide Association Studies\n\nGWAS are studies that look for associations between genetic variants across the genome and specific traits or diseases in a large population. They can help identify genetic factors contributing to complex diseases by analyzing the frequency of genetic variants in affected and unaffected individuals.\nManhattan plots are visual representations of GWAS results, where each point represents a genetic variant and its association with the trait of interest. The x-axis represents the genomic position, while the y-axis represents the significance of the association (usually -log10(p-value)). Peaks in the plot indicate regions of the genome that may be associated with the trait."
  },
  {
    "objectID": "lectures/lecture-01.html#core-abstractions-for-quantitative-biology",
    "href": "lectures/lecture-01.html#core-abstractions-for-quantitative-biology",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Core abstractions for quantitative biology",
    "text": "Core abstractions for quantitative biology\n\nModel-Building\n\nFormulate generative models linking genotype, environment, and phenotype (e.g., linear mixed, Bayesian hierarchical, non-parametric kernels).\nEncode biological structure: linkage & LD, population stratification, dominance/epistasis, multi-omics priors.\nBalance realism and tractability to enable scalable computation on genome-scale data."
  },
  {
    "objectID": "lectures/lecture-01.html#core-abstractions-for-quantitative-biology-1",
    "href": "lectures/lecture-01.html#core-abstractions-for-quantitative-biology-1",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Core abstractions for quantitative biology",
    "text": "Core abstractions for quantitative biology\n\nInference\n\nEstimate unknown parameters and latent effects via likelihood maximisation, EM, MCMC, or SGD\nQuantify uncertainty with standard errors, posterior intervals, and credible sets\nControl false discoveries across millions of tests with FDR/Q-value, permutation, and empirical-Bayes shrinkage."
  },
  {
    "objectID": "lectures/lecture-01.html#core-abstractions-for-quantitative-biology-2",
    "href": "lectures/lecture-01.html#core-abstractions-for-quantitative-biology-2",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Core abstractions for quantitative biology",
    "text": "Core abstractions for quantitative biology\n\nPrediction\n\nUse fitted models for out-of-sample trait prediction: BLUP/GBLUP, ridge/lasso/elastic-net, Bayesian whole-genome regressions, random forests, neural nets.\nEvaluate accuracy (MSE, AUC), bias–variance trade-off, calibration, and portability across ancestries or cell types.\nTranslate genomic predictions into actionable scores for breeding, risk stratification, and drug-target prioritisation."
  },
  {
    "objectID": "lectures/lecture-01.html#core-abstractions-for-quantitative-biology-3",
    "href": "lectures/lecture-01.html#core-abstractions-for-quantitative-biology-3",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Core abstractions for quantitative biology",
    "text": "Core abstractions for quantitative biology\n\nInterpretation & Validation\n\nIntegrate functional annotations, eQTL, and single-cell data to refine biological mechanisms.\nPerform replication, cross-cohort meta-analysis, and sensitivity analyses to population assumptions.\nCommunicate findings with clear visualisations and reproducible workflows (R/Bioconductor, Git, notebooks)."
  },
  {
    "objectID": "lectures/lecture-01.html#early-work-in-the-field",
    "href": "lectures/lecture-01.html#early-work-in-the-field",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Early work in the field",
    "text": "Early work in the field\n\n\n\n\n\n\n\n\n\n\n\n\nGregor Mendel (1822-1884) was an Austrian monk and scientist who is considered the father of modern genetics. He conducted experiments on pea plants and discovered the fundamental laws of inheritance, including the concepts of dominant and recessive traits, segregation, and independent assortment.\nMendel’s work laid the foundation for the field of genetics, but it was largely ignored during his lifetime. It wasn’t until the early 20th century that his findings were rediscovered and integrated into the broader understanding of biology.\nIt is hypothesized that Charles Darwin would have begun his work on evolution much earlier if he had access to Mendel’s work on inheritance."
  },
  {
    "objectID": "lectures/lecture-01.html#genetics-statistics-and-eugenics-have-always-been-closely-linked",
    "href": "lectures/lecture-01.html#genetics-statistics-and-eugenics-have-always-been-closely-linked",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Genetics, statistics, and eugenics, have always been closely linked",
    "text": "Genetics, statistics, and eugenics, have always been closely linked\n\n\n\n\n\n\nStandard eugenics scheme of descent\n\n\n\n\n\n\n\nFrancis Galton and his work, “Inquires into Human Faculty and its Development”\n\n\n\n\n\n\n\nFrancis Galton (1822-1911) was an English polymath who made significant contributions to various fields, including statistics, psychology, and anthropology. He is best known for his work on eugenics, a now-discredited movement that aimed to improve the genetic quality of the human population through selective breeding.\nGalton was a cousin of Charles Darwin and was influenced by Darwin’s theory of evolution. He believed that intelligence and other traits were hereditary and could be measured and quantified. He developed statistical methods, such as correlation and regression, to study the inheritance of traits.\nGalton’s work on eugenics has been widely criticized for its ethical implications and its association with discriminatory practices, such as forced sterilizations and racial segregation"
  },
  {
    "objectID": "lectures/lecture-01.html#the-ethics-of-genetics-and-genomics-iswas-of-great-importance",
    "href": "lectures/lecture-01.html#the-ethics-of-genetics-and-genomics-iswas-of-great-importance",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "The ethics of genetics and genomics is/(was) of great importance!",
    "text": "The ethics of genetics and genomics is/(was) of great importance!"
  },
  {
    "objectID": "lectures/lecture-01.html#some-vocabulary",
    "href": "lectures/lecture-01.html#some-vocabulary",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Some vocabulary",
    "text": "Some vocabulary\n\nTrait/Phenotype: A measurable characteristic of an organism, such as height, weight, or disease status.\nGene: Unit of inheritance\nGenotype: The genetic constitution of an individual, often represented by specific alleles at particular loci.\nAllele: A variant form of a gene that can exist at a specific locus on a chromosome.\nLocus: A specific, fixed position on a chromosome where a particular gene or genetic marker is located.\nDiploid: Having two sets of chromosomes (one from each parent)\nPenetrance: Likelihood of expressing a phenotype given a genotype\nPolymorphism: The occurrence of two or more genetically determined forms in a population, such as single nucleotide polymorphisms (SNPs) or copy number variations (CNVs).\nGenetic Marker: A specific DNA sequence with a known location on a chromosome that can be used to identify individuals or species, often used in genetic mapping or association studies."
  },
  {
    "objectID": "lectures/lecture-01.html#some-vocabulary-1",
    "href": "lectures/lecture-01.html#some-vocabulary-1",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Some vocabulary",
    "text": "Some vocabulary\n\nMendelian Disease: A disease caused by a mutation in a single gene, following Mendelian inheritance patterns (dominant, recessive, X-linked).\nLinkage Disequilibrium (LD): The non-random association of alleles at different loci, indicating that certain allele combinations occur together more frequently than expected by chance.\nHeritability: The proportion of phenotypic variance in a trait that can be attributed to genetic variance, often estimated through twin or family studies.\nGenome-Wide Association Study (GWAS): A study that looks for associations between genetic variants across the genome and specific traits or diseases in a large population.\nPolygenic Score (PGS): A score that aggregates the effects of multiple genetic variants to predict an individual’s genetic predisposition to a trait or disease.\nQuantitative Trait Locus (QTL): A region of the genome that is associated with a quantitative trait, often identified through linkage or association mapping.\nEpistasis: The interaction between genes where the effect of one gene is modified by one or more other genes, influencing the expression of a trait."
  },
  {
    "objectID": "lectures/lecture-01.html#genetic-variant-example",
    "href": "lectures/lecture-01.html#genetic-variant-example",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Genetic Variant Example",
    "text": "Genetic Variant Example\n\nLaird and Lange (2011)"
  },
  {
    "objectID": "lectures/lecture-01.html#probability-refresher",
    "href": "lectures/lecture-01.html#probability-refresher",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Probability Refresher",
    "text": "Probability Refresher\nRandom Variables and Distributions\n\nA random variable is a numerical summary of randomness (e.g., the count of (A) alleles in a sample). We will denote random variables with uppercase letters (e.g., X, Y).\nWe use a model (e.g., binomial/multinomial) to describe how data vary from sample to sample.\n\nThis is otherwise known as a probability mass function (pmf) for discrete variables, or a probability density function (pdf) for continuous variables\n\nKey ideas:\n\nExpectation (mean) and variance: long‑run average and spread.\n\nE[X] = \\sum x \\cdot P(X = x) (discrete) or E[X] = \\int x f(x) dx (continuous)\n\\textsf{Var}(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2\n\nStandard error (SE): typical sampling variability of an estimator.\n\n\\textsf{SE} = \\frac{\\sigma}{\\sqrt{n}} (uncertainty in sample estimates)\n\nCentral Limit Theorem (CLT): averages and proportions are roughly normal for large (n).\n\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) as n \\to \\infty"
  },
  {
    "objectID": "lectures/lecture-01.html#probability-distributions-vs-sampling-distributions",
    "href": "lectures/lecture-01.html#probability-distributions-vs-sampling-distributions",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Probability Distributions vs Sampling Distributions",
    "text": "Probability Distributions vs Sampling Distributions\n\nA probability distribution describes how a random variable behaves in the population.\n\nExample: X \\sim \\textsf{Binomial}(n, p)\n\nA sampling distribution describes how a statistic (e.g., sample mean, sample proportion) behaves across repeated samples from the population.\n\nExample: \\hat{p} \\sim N\\left(p, \\frac{(p(1-p))}{n}\\right) as n \\to \\infty"
  },
  {
    "objectID": "lectures/lecture-01.html#probability-refresher-1",
    "href": "lectures/lecture-01.html#probability-refresher-1",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Probability Refresher",
    "text": "Probability Refresher\nKey Distributions in Statistical Genetics\n\n\n\nBinomial Distribution X \\sim \\textsf{Binomial}(n, p) P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\n\nUsed for: Allele counts\n\n\n\n\nNormal Distribution X \\sim N(\\mu, \\sigma^2) f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\n\nUsed for: Quantitative traits, effect sizes"
  },
  {
    "objectID": "lectures/lecture-01.html#conditional-probability-and-independence",
    "href": "lectures/lecture-01.html#conditional-probability-and-independence",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Conditional Probability and Independence",
    "text": "Conditional Probability and Independence\n\nConditional Probability: P(A|B) = \\frac{P(A \\cap B)}{P(B)}\nBayes’ Theorem: P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\nIndependence: P(A \\cap B) = P(A) \\cdot P(B) \\iff A and B are independent\n\nConditional probability and Bayes’ Theorem are crucial for understanding rare disease genetics! For example, consider a rare disease with a prevalence of 1 in 10,000. If a genetic test has a sensitivity of 99% and a specificity of 99%, we can use Bayes’ Theorem to calculate the probability that an individual has the disease given a positive test result.\n\n\\begin{align*}\nP(Disease|Positive) &= \\frac{P(Positive|Disease)P(Disease)}{P(Positive)} \\\\\n&= \\frac{(0.99)(0.0001)}{(0.99)(0.0001) + (0.01)(0.9999)} \\\\\n&\\approx 0.0098\n\\end{align*}"
  },
  {
    "objectID": "lectures/lecture-01.html#parameter-estimation",
    "href": "lectures/lecture-01.html#parameter-estimation",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Parameter Estimation",
    "text": "Parameter Estimation\n\nImagine a geneticist is studying allele ages (i.e., how many generations ago an allele arose via mutation) under a simplified model where new mutations arise randomly and uniformly over a fixed window — say, the last \\theta generations.\nWe could model X \\sim \\textsf{Uniform}(0, \\theta).\nWe have our observed sample \\{x_1,x_2, \\ldots, x_n\\}, ages in generations of n observed alleles.\n\nFor concreteness, let our sample be \\{0, 1, 8, 4, 3, 7, 7, 6\\}.\n\nHow should we estimate \\theta ?"
  },
  {
    "objectID": "lectures/lecture-01.html#genetic-models",
    "href": "lectures/lecture-01.html#genetic-models",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Genetic Models",
    "text": "Genetic Models\n\nUnderstanding random variables, sampling distributions, and bias/variance of estimators helps motivate biological questions, and understand how they can be answered\nLet y be the expression of a trait, G be the additive contribution of genetic variables, and an environmental value E;\nWhat assumptions do we make via the following models?\n\n\n\n\n\\begin{align}\n\ny &= G + E \\\\\ny &= \\beta_0 + \\beta_1 G + \\beta_2 E + \\epsilon \\\\\ny &= \\beta_0 + \\beta_1 G + \\beta_2 E + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2) \\\\\ny &= \\beta_0 + \\beta_1 G + \\beta_2 E + \\beta_3 \\left(G \\times E\\right) + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2)\n\n\\end{align}"
  },
  {
    "objectID": "lectures/lecture-01.html#modeling",
    "href": "lectures/lecture-01.html#modeling",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Modeling",
    "text": "Modeling\n\nImagine that we have a sample size of n unrelated haploid individuals from some population\nWe want to estimate allele frequencies for a biallelic SNP, say A/a\nIn our sample, we observe x individuals with allele A and n - x individuals with allele a\nLet p be the frequency of allele A and q = 1 - p be the frequency of allele a\n\nPr(X = x| n, p) = \\binom{n}{x} p^x q^{n - x}\n\nLets say we observe n = 27 and x = 11. How do we estimate p?"
  },
  {
    "objectID": "lectures/lecture-01.html#likelihood-functions",
    "href": "lectures/lecture-01.html#likelihood-functions",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Likelihood Functions",
    "text": "Likelihood Functions\n\nThe likelihood function is a function of the parameters of a statistical model, given specific observed data.\nIt represents the plausibility of different parameter values based on the observed data.\nFor a given set of data, the likelihood function is defined as: L(\\theta | data) = P(data | \\theta)\nIn our case, the likelihood function for p is: L(p | x = 11, n = 27) = P(X = 11 | n = 27, p) = \\binom{27}{11} p^{11} (1 - p)^{16}\nWe are asking, “how likely is it that we would observe this data for different values of p?”"
  },
  {
    "objectID": "lectures/lecture-01.html#maximum-likelihood-estimator",
    "href": "lectures/lecture-01.html#maximum-likelihood-estimator",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Maximum Likelihood Estimator",
    "text": "Maximum Likelihood Estimator\nOur likelihood function for p is\nL(P|x = 11, n = 27) = {27 \\choose 11} p^{11} q^{27-11}\n\n\n\nn &lt;- 27\nx &lt;- 11\np_seq &lt;- seq(0, 1, length.out = 200)\nlikelihood &lt;- dbinom(x, n, p_seq)\nplot &lt;- ggplot(\n    data.frame(p = p_seq, likelihood = likelihood),\n    aes(x = p, y = likelihood)\n) +\n    geom_line(linewidth = 1) +\n    labs(\n        x = \"Probability of Success (p)\",\n        y = \"Likelihood\",\n        title = \"Binomial Likelihood Function\"\n    ) +\n    geom_vline(\n        xintercept = x / n,\n        color = \"red\",\n        linetype = \"dashed\"\n    ) +\n    theme_light(base_size = 18) +\n    theme(panel.grid = element_blank())"
  },
  {
    "objectID": "lectures/lecture-01.html#modeling-but-make-it-bayesian",
    "href": "lectures/lecture-01.html#modeling-but-make-it-bayesian",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Modeling, but make it bayesian",
    "text": "Modeling, but make it bayesian\n\nIn Bayesian statistics, we treat parameters as random variables and use probability distributions to represent our uncertainty about them.\nWe start with a prior distribution that reflects our beliefs about the parameter before seeing the data.\nWe then use the observed data to update our beliefs and obtain a posterior distribution.\nThe posterior distribution combines the information from the prior and the likelihood of the observed data using Bayes’ theorem: P(p | data) = \\frac{P(data | p) P(p)}{P(data)}\nLet’s say we have run a previous experiment and observed n = 20 individuals and x = 3 individuals with allele A (again, bilelic SNP).\nWe can use this information to inform our prior distribution for p"
  },
  {
    "objectID": "lectures/lecture-01.html#modeling-but-make-it-bayesian-1",
    "href": "lectures/lecture-01.html#modeling-but-make-it-bayesian-1",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Modeling, but make it bayesian",
    "text": "Modeling, but make it bayesian\n\n\n\np &lt;- seq(0, 1, length.out = 1000)\n# Prior (Beta distribution from previous experiment)\nprior &lt;- dbeta(p, 3 + 1, 17 + 1)\n# Likelihood (Beta distribution from current data)\nlikelihood &lt;- dbeta(p, 11 + 1, 16 + 1)\n# Posterior (Beta distribution combining both)\nposterior &lt;- dbeta(p, 14 + 1, 33 + 1)\n# Combine into a data frame\nplot_data &lt;- data.frame(\n    p = rep(p, 3),\n    density = c(prior, likelihood, posterior),\n    type = rep(c(\"Prior\", \"Likelihood\", \"Posterior\"), each = length(p))\n)\n# Add flat prior distribution\nflat_prior &lt;- dbeta(p, 1, 1)\nflat_posterior &lt;- dbeta(p, 12, 17)\nplot_data &lt;- rbind(\n    plot_data,\n    data.frame(\n        p = rep(p, 3),\n        density = c(flat_prior, likelihood, flat_posterior),\n        type = rep(c(\"Prior\", \"Likelihood\", \"Posterior\"), each = length(p))\n    )\n)\n# Add a column to distinguish between informative and flat scenarios\nplot_data$scenario &lt;- c(rep(\"Informative\", 3000), rep(\"Flat\", 3000))\n\n\n\n\n\n\np1 &lt;- ggplot(\n    plot_data %&gt;% filter(scenario == \"Informative\"),\n    aes(x = p, y = density, color = type)\n) +\n    geom_line(linewidth = 2) +\n    geom_vline(xintercept = 11 / 27, linetype = \"dashed\", color = \"red\") +\n    labs(\n        x = \"Allele Frequency (p)\",\n        y = \"Density\",\n        title = \"Bayesian Update with Informative Prior\"\n    ) +\n    scale_color_viridis_d() +\n    theme_minimal(base_size = 15)\n\n\n\np2 &lt;- ggplot(\n  plot_data %&gt;% filter(scenario == \"Flat\"),\n  aes(x = p, y = density, color = type, linetype = type)\n) +\n  geom_line(linewidth = 2) +\n  scale_linetype_manual(values = c(\"Prior\" = \"dotted\", \"Likelihood\" = \"solid\", \"Posterior\" = \"dashed\")) +\n  geom_vline(xintercept = 11 / 27, linetype = \"dotdash\", color = \"red\") +\n  labs(\n    x = \"Allele Frequency (p)\",\n    y = \"Density\",\n    title = \"Bayesian Update with Flat Prior\"\n  ) +\n  theme_minimal(base_size = 15) +\n  scale_color_viridis_d() +\n  annotate(\"text\",\n    x = 0.7, y = max(flat_posterior) * 0.8,\n    label = \"Likelihood ≈ Posterior\", color = \"gray30\"\n  )"
  },
  {
    "objectID": "lectures/lecture-01.html#modeling-but-make-it-bayesian-2",
    "href": "lectures/lecture-01.html#modeling-but-make-it-bayesian-2",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Modeling, but make it Bayesian",
    "text": "Modeling, but make it Bayesian"
  },
  {
    "objectID": "lectures/lecture-01.html#mendels-laws",
    "href": "lectures/lecture-01.html#mendels-laws",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Mendel’s Laws",
    "text": "Mendel’s Laws\n\n\nMendel’s First Law (Segregation)\nOne allele of each parent is randomly transmitted, with probability 1/2, to the offspring; the alleles unite randomly to form the offspring’s genotype.\nMendel’s Second Law (Independent Assortment)\nAt different loci, alleles are transmitted independently (when loci are unlinked).\n\n\n\n\n\nMendel’s First Law, the Law of Segregation, states that during the formation of gametes (sperm and egg cells), the two alleles for a given gene separate, so that each gamete receives only one allele. This means that each parent contributes one allele to their offspring, and the combination of these alleles determines the offspring’s genotype.\nMendel’s Second Law, the Law of Independent Assortment, states that alleles for different genes are transmitted independently of one another, provided the genes are located on different chromosomes or are far apart on the same chromosome. This means that the inheritance of one trait does not influence the inheritance of another trait, allowing for a variety of genetic combinations in the offspring."
  },
  {
    "objectID": "lectures/lecture-01.html#worked-example-segregation-31-ratio",
    "href": "lectures/lecture-01.html#worked-example-segregation-31-ratio",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Worked Example: Segregation (3:1 Ratio)",
    "text": "Worked Example: Segregation (3:1 Ratio)\nCross: Aa × Aa\n\n\n\nPunnett Square\n\n\n\n\n\n\nA\na\n\n\n\n\nA\nAA\nAa\n\n\na\nAa\naa\n\n\n\n\n\n\n\nExpected Ratios\n\n\n\nGenotypic: 1 AA : 2 Aa : 1 aa\nPhenotypic: 3 dominant : 1 recessive\nProbabilities: P(AA)=\\frac14, P(Aa)=\\frac12, P(aa)=\\frac14"
  },
  {
    "objectID": "lectures/lecture-01.html#test-cross-example",
    "href": "lectures/lecture-01.html#test-cross-example",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Test Cross Example",
    "text": "Test Cross Example\nProving law of segregation with a test cross\nExample data:\n\nCross: Aa × aa\nObserved: 21 dominant, 28 recessive\nNull hypothesis: Probability of transmission is 1:1.\nAlternative hypothesis: Probability of transmission is not 1:1.\nHow to test this?\n\nOur question is related to observation of proportions –&gt; use a chi-squared goodness-of-fit test or a binomial test."
  },
  {
    "objectID": "lectures/lecture-01.html#test-cross-example-1",
    "href": "lectures/lecture-01.html#test-cross-example-1",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Test Cross Example",
    "text": "Test Cross Example\n\nobserved_testcross &lt;- c(21, 28)     # Dominant, Recessive\nchisq_test &lt;- chisq.test(observed_testcross, p = c(0.5, 0.5))\nchisq_test\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed_testcross\nX-squared = 1, df = 1, p-value = 0.3173"
  },
  {
    "objectID": "lectures/lecture-01.html#testing-via-simulation",
    "href": "lectures/lecture-01.html#testing-via-simulation",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Testing via Simulation",
    "text": "Testing via Simulation\nWe could also simulate our idea of the null hypothesis to get a sense of how extreme our observed data is.\n\nset.seed(123)\nn_sim &lt;- 10000\nN &lt;- 49\nsimulated &lt;- rbinom(n_sim, N, 0.5)\np_value_sim &lt;- mean(simulated &lt;= 21 | simulated &gt;= 28)\np_value_sim\n\n[1] 0.3868\n\nhist(simulated, breaks = 30, main = \"Simulated Test Cross Outcomes\", xlab = \"Number of Dominant Offspring\")\nabline(v = c(21, 28), col = \"red\", lty = 2, lwd = 2)"
  },
  {
    "objectID": "lectures/lecture-01.html#on-simulations",
    "href": "lectures/lecture-01.html#on-simulations",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "On Simulations",
    "text": "On Simulations\n\nDowney (2015)"
  },
  {
    "objectID": "lectures/lecture-01.html#estimation-of-allele-frequencies",
    "href": "lectures/lecture-01.html#estimation-of-allele-frequencies",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Estimation of Allele Frequencies",
    "text": "Estimation of Allele Frequencies\nn_{AA} = Number of individuals with genotype AA n_{Aa} = Number of individuals with genotype Aa n_{aa} = Number of individuals with genotype aa\nwhere, n = n_{AA} + n_{Aa} + n_{aa} = N.\nThe sample proportions of allele A is:\n\n\\hat{p} = \\frac{2n_{AA} + n_{Aa}}{2n}\n\nThe usual standard error for a proportion is \\sqrt{\\hat{p}(1 - \\hat{p})/2n}, but this assumes independence of the 2n sampled chromosomes."
  },
  {
    "objectID": "lectures/lecture-01.html#hardyweinberg-equilibrium-hwe-assumptions",
    "href": "lectures/lecture-01.html#hardyweinberg-equilibrium-hwe-assumptions",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Hardy–Weinberg Equilibrium (HWE): Assumptions",
    "text": "Hardy–Weinberg Equilibrium (HWE): Assumptions\nLet’s reframe the problem in terms of genotype frequencies. First, some assumptions:\n\nRandom mating\nLarge population (no strong drift in one generation)\nNo selection, no migration, no mutation (in the generation under study)\nAccurate genotypes (no systematic genotyping error)\n\nIf these hold, and the allele frequency of A is p (so q = 1-p), we say that the population is in Hardy-Weinberg Equilibrium (HWE). The population genotype probabilities are:\n\nP(AA)=p^2 \\quad P(Aa)=2pq \\quad P(aa)=q^2\n\nAs a consequence, the genotype frequencies in a random sample of n individuals are approximately multinomially distributed with cell probabilities (p^2, 2pq, q^2). Thus, the sampling error of \\hat{p} is approximately \\sqrt{p(1-p)/2n}.\nThis confidence interval is known as a Wald confidence interval."
  },
  {
    "objectID": "lectures/lecture-01.html#testing-for-hwe",
    "href": "lectures/lecture-01.html#testing-for-hwe",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Testing for HWE",
    "text": "Testing for HWE\n\nWith large samples, we can test for HWE using a chi-squared goodness-of-fit test.\n\n\nLaird and Lange (2011)"
  },
  {
    "objectID": "lectures/lecture-01.html#hwe-worked-example",
    "href": "lectures/lecture-01.html#hwe-worked-example",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "HWE: Worked Example",
    "text": "HWE: Worked Example\n\nn &lt;- 212\nn_AA &lt;- 175\nn_Aa &lt;- 33\nn_aa &lt;- 4\n\n# Estimate allele frequency\np_hat &lt;- (2*n_AA + n_Aa) / (2*n)\nq_hat &lt;- 1 - p_hat\np_hat\n\n[1] 0.9033019\n\n# Expected counts\nexpected &lt;- c(n * p_hat^2, 2*n * p_hat*q_hat, n * q_hat^2)\nobserved &lt;- c(n_AA, n_Aa, n_aa)\nexpected\n\n[1] 172.982311  37.035377   1.982311\n\n# Pearson chi-squared\nchisq_val &lt;- sum((observed - expected)^2 / expected)\ndf &lt;- 3 - 1 - 1\np_val &lt;- pchisq(chisq_val, df = df, lower.tail = FALSE)\np_val\n\n[1] 0.1126299"
  },
  {
    "objectID": "lectures/lecture-01.html#hwe-in-practice",
    "href": "lectures/lecture-01.html#hwe-in-practice",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "HWE in Practice",
    "text": "HWE in Practice\n\nHWE is a useful null model for quality control of genotype data.\nDeviations from HWE can indicate genotyping errors, population stratification, or selection"
  },
  {
    "objectID": "lectures/lecture-01.html#sampling-distributions-intuition-via-simulation",
    "href": "lectures/lecture-01.html#sampling-distributions-intuition-via-simulation",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Sampling distributions: intuition via simulation",
    "text": "Sampling distributions: intuition via simulation\n\nset.seed(123)\ntrue_p &lt;- 0.3\nsample_sizes &lt;- c(5, 10, 100, 500)\nn_sims &lt;- 1000\n\nsampling_results &lt;- map_dfr(sample_sizes, function(n) {\n  p_hats &lt;- replicate(n_sims, {\n    # Sample n individuals (2n alleles total)\n    alleles &lt;- rbinom(n, 2, true_p)  # each individual contributes 0,1,2 A alleles\n    sum(alleles) / (2 * n)           # sample allele frequency\n  })\n  tibble(sample_size = n, p_hat = p_hats,\n         theoretical_se = sqrt(true_p * (1 - true_p) / (2 * n)))\n})"
  },
  {
    "objectID": "lectures/lecture-01.html#sampling-distributions-intuition-via-simulation-1",
    "href": "lectures/lecture-01.html#sampling-distributions-intuition-via-simulation-1",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "Sampling distributions: intuition via simulation",
    "text": "Sampling distributions: intuition via simulation\n\n# Plot sampling distributions\nggplot(sampling_results, aes(x = p_hat)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30, alpha = 0.7) +\n  geom_vline(xintercept = true_p, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  facet_wrap(~sample_size_name, scales = \"free_y\") +\n  labs(title = \"Sampling Distribution of p̂\", x = \"Sample Allele Frequency (p̂)\", y = \"Density\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/lecture-01.html#references",
    "href": "lectures/lecture-01.html#references",
    "title": "Lecture 01: Introduction and Foundations",
    "section": "References",
    "text": "References\n\n\n\n\nDowney,A.B. (2015) Think stats: Exploratory data analysis in python 2nd ed. O’Reilly Media, Sebastopol, CA.\n\n\nLaird,N.M. and Lange,C. (2011) The fundamentals of modern statistical genetics Springer, New York, NY.\n\n\nSorensen,D. (2025) Statistical learning in genetics: An introduction using r 2nd ed. Springer, Cham."
  },
  {
    "objectID": "lectures/lecture-03.html#review-likelihood-function-basics",
    "href": "lectures/lecture-03.html#review-likelihood-function-basics",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Review: Likelihood Function Basics",
    "text": "Review: Likelihood Function Basics\n\nLikelihood function: \\(L(\\boldsymbol{\\theta} \\mid \\text{data}) = P(\\text{data} \\mid \\boldsymbol{\\theta})\\)\nMaximum Likelihood Estimation (MLE)\nSimple one-parameter examples\n\nHowever often direct MLE is not possible because\n\nClosed-form solution does not exist\nMissing or latent data\nHigh-dimensional or constrained parameter space leads to non-convex surface with multiple local maxima\n\n\nScript:\n“Let’s start by aligning on what likelihood means. We take the sampling model \\(p(\\text{data}\\mid\\theta)\\) and, after we’ve observed the data, we view it as a function of the parameter \\(\\theta\\). That function is the likelihood \\(L(\\theta)\\). The MLE is simply the value of \\(\\theta\\) that maximizes \\(L\\).\nTwo reminders that will be useful all semester: first, the invariance property—if \\(\\hat\\theta\\) maximizes \\(L(\\theta)\\), then \\(g(\\hat\\theta)\\) maximizes the likelihood of \\(g(\\theta)\\). Second, the curvature of the log-likelihood, via Fisher information, anticipates precision.\nIn practice, closed forms fail us for three common reasons: there’s no algebraic solution; there’s missing or latent data; or the parameter space is constrained or high-dimensional so the surface is bumpy. Today we’ll build the numerical toolkit that handles these realities.”"
  },
  {
    "objectID": "lectures/lecture-03.html#agenda",
    "href": "lectures/lecture-03.html#agenda",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Agenda",
    "text": "Agenda\n\nOptimization for MLE: Newton–Raphson, Gradient Descent, SGD\nEM algorithm: gene frequencies and incomplete data\nLinkage analysis: two-point, unknown phase EM, multipoint HMM\nLinkage disequilibrium (LD) and its measures (D, D’, r)\nAssociation: single-marker/haplotype tests and basic QC\n\n\nScript:\n“Here’s the plan. We’ll warm up with Newton–Raphson and first‑order methods—how to actually move on a likelihood surface. We’ll then introduce EM as a principled way to deal with missing or latent pieces, anchored by an ABO example.\nNext, we pivot to linkage: first two‑point LOD, then an EM formulation for unknown phase, and finally the multipoint HMM perspective. We’ll connect that to LD, clarify \\(D\\), \\(D'\\) and \\(r\\), and finish with a single‑marker association demo and the QC that keeps results honest.”"
  },
  {
    "objectID": "lectures/lecture-03.html#newton-raphson-method-for-mle",
    "href": "lectures/lecture-03.html#newton-raphson-method-for-mle",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Newton-Raphson Method for MLE",
    "text": "Newton-Raphson Method for MLE\n\nGeneral procedure to find \\(x\\) such that \\(g(x) = 0\\)\nUses Taylor Expansion, for \\(g(x)\\) about a point \\(x = a\\):\n\n\\[\\begin{align*}\ng(x) = g(a) &+\n\\frac{g'(a)(x-a)}{1!} + \\frac{g''(a)(x-a)^2}{2!} \\\\\n&+ ... + \\frac{g^{(n)}(a)(x-a)^n}{n!}\n\\end{align*}\\]\n\nScript:\n“In MLE we set \\(g(\\theta)=\\ell'(\\theta)\\), the score, and look for a root. Newton’s update is\n\\[ \\theta{(t+1)}={(t)}-.\\]\nIn multiple dimensions it’s \\(\\boldsymbol{\\theta}^{(t+1)}=\\boldsymbol{\\theta}^{(t)}-\\mathbf{H}^{-1}\\mathbf{g}\\), where \\(\\mathbf{g}\\) is the gradient and \\(\\mathbf{H}\\) the Hessian.\nThree practical notes. First, Newton is quadratically convergent when you’re close to the solution—that’s why it’s fast. Second, guard against wild steps with damping or line search, and keep feasibility in mind if your parameter must lie on a simplex or in \\([0,0.5]\\). Third, if the Hessian misbehaves, use Fisher scoring (replace \\(-\\mathbf{H}\\) with expected information) or a trust region.”"
  },
  {
    "objectID": "lectures/lecture-03.html#gradient-descent",
    "href": "lectures/lecture-03.html#gradient-descent",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nFirst-order iterative optimization algorithm\nDoes not rely on matrix inversions \\(\\longrightarrow\\) better for high dimensional problems\nVisualizer from UCLA\n\n\nScript:\n“First‑order methods move along the steepest descent direction without ever inverting a matrix. That’s a superpower in high‑dimensional genetics models where Hessians are huge. The tradeoff: convergence is typically linear, not quadratic.\nWhat matters in practice is the step size: constant steps are brittle; backtracking or scheduled decay is safer. Stochastic and mini‑batch variants are essential when your objective decomposes over people, markers, or families. The mental model is simple: noisy but cheap steps that converge ‘on average’.”"
  },
  {
    "objectID": "lectures/lecture-03.html#the-em-algorithm",
    "href": "lectures/lecture-03.html#the-em-algorithm",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "The EM Algorithm",
    "text": "The EM Algorithm\n\nExpectation-Maximization Algorithm\nKey idea: transform an incomplete data problem into a complete data problem\n\n\\[\np\\left(y \\mid \\theta \\right) = \\frac{p\\left(y, z \\mid \\theta \\right)}{p\\left(z \\mid y, \\theta\\right)}\n\\]\n\nScript:\n“EM turns missingness into bookkeeping. We invent latent variables \\(z\\) so that the complete‑data log-likelihood \\(\\log p(y,z\\mid\\theta)\\) is easy to maximize. At iterate \\(t\\):\n\nE‑step: compute \\(Q(\\theta\\mid\\theta^{(t)})=\\mathbb{E}_{z\\mid y,\\theta^{(t)}}[\\log p(y,z\\mid\\theta)]\\).\n\nM‑step: update \\(\\theta^{(t+1)}=\\arg\\max_\\theta Q(\\theta\\mid\\theta^{(t)})\\).\n\nWhy does it work? By Jensen’s inequality, EM monotonically increases the observed log-likelihood. The feel is ‘stable but sometimes slow’; we’ll flag accelerations like ECME and PX‑EM later.”"
  },
  {
    "objectID": "lectures/lecture-03.html#em-for-gene-frequencies",
    "href": "lectures/lecture-03.html#em-for-gene-frequencies",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "EM for Gene Frequencies",
    "text": "EM for Gene Frequencies\n\n\n\n\n\nGenotype\nPhenotype\nObserved Counts\nGenotype Frequency\n\n\n\n\n\\(AA\\)\n\\(A\\)\n\\(n_A\\)\n\\(p_A^2\\)\n\n\n\\(AO\\)\n\\(A\\)\n\n\\(2 p_A p_O\\)\n\n\n\\(AB\\)\n\\(AB\\)\n\\(n_{AB}\\)\n\\(2 p_A p_B\\)\n\n\n\\(BB\\)\n\\(B\\)\n\\(n_B\\)\n\\(p_B^2\\)\n\n\n\\(BO\\)\n\\(B\\)\n\n\\(2 p_B p_O\\)\n\n\n\\(OO\\)\n\\(O\\)\n\\(n_O\\)\n\\(p_O^2\\)\n\n\n\n\n\n\nScript:\n“Here’s the ABO mapping. The assumption is Hardy–Weinberg and random mating, so genotype frequencies are the usual \\(p^2\\), \\(2pq\\), \\(q^2\\). The ambiguity sits exactly in the A and B phenotypes: each collapses over two genotypes. EM will split those cells into expected counts using the current allele frequencies and then re‑estimate the frequencies as if those expected counts were observed.”"
  },
  {
    "objectID": "lectures/lecture-03.html#em-for-gene-frequencies-1",
    "href": "lectures/lecture-03.html#em-for-gene-frequencies-1",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "EM for Gene Frequencies",
    "text": "EM for Gene Frequencies\n\nWe don’t know if \\(n_A\\) is from \\(AA\\) or \\(AO\\)! (And similarly for \\(n_B\\))\nHowever, given our data model, we can find values of \\((\\hat{p}_A, \\hat{p}_B)\\) via EM\n\n\nScript:\n“Think of each A phenotype as a soft label between AA and AO, with probabilities determined by the current \\((p_A,p_O)\\). Same story for B. That’s the essence of the E‑step.”"
  },
  {
    "objectID": "lectures/lecture-03.html#em-for-gene-frequencies-2",
    "href": "lectures/lecture-03.html#em-for-gene-frequencies-2",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "EM for Gene Frequencies",
    "text": "EM for Gene Frequencies\n\nStart with initial \\((p_A^{(0)}, p_B^{(0)})\\) and set \\(p_O^{(0)} = 1 - p_A^{(0)} - p_B^{(0)}\\).\nE-step: allocate ambiguous phenotype counts:\n\n\\(A\\) phenotype: \\(\\tilde n_{AA} = n_A \\frac{(p_A^{(t)})^2}{(p_A^{(t)})^2 + 2 p_A^{(t)} p_O^{(t)}}\\), \\(\\tilde n_{AO} = n_A \\frac{2 p_A^{(t)} p_O^{(t)}}{(p_A^{(t)})^2 + 2 p_A^{(t)} p_O^{(t)}}\\).\n\\(B\\) phenotype: \\(\\tilde n_{BB} = n_B \\frac{(p_B^{(t)})^2}{(p_B^{(t)})^2 + 2 p_B^{(t)} p_O^{(t)}}\\), \\(\\tilde n_{BO} = n_B \\frac{2 p_B^{(t)} p_O^{(t)}}{(p_B^{(t)})^2 + 2 p_B^{(t)} p_O^{(t)}}\\).\n\n\n\nScript:\n“These are just conditional probabilities. Given the person has phenotype A, what’s the chance they’re AA versus AO under the current \\((p_A,p_O)\\)? Multiply by \\(n_A\\) to get expected genotype counts \\(\\tilde n_{AA}\\) and \\(\\tilde n_{AO}\\). Do the same for B. That’s the E‑step.”"
  },
  {
    "objectID": "lectures/lecture-03.html#em-for-gene-frequencies-3",
    "href": "lectures/lecture-03.html#em-for-gene-frequencies-3",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "EM for Gene Frequencies",
    "text": "EM for Gene Frequencies\n\nM-step (with \\(N\\) total individuals):\n\n\\[\\begin{aligned}\np_A^{(t+1)} &= \\frac{2\\tilde n_{AA} + \\tilde n_{AO} + n_{AB}}{2N},\\\\\np_B^{(t+1)} &= \\frac{2\\tilde n_{BB} + \\tilde n_{BO} + n_{AB}}{2N},\\\\\np_O^{(t+1)} &= 1 - p_A^{(t+1)} - p_B^{(t+1)}\n\\end{aligned}\\]\n\nIterate until parameter changes are below a tolerance.\n\n\nScript:\n“Now treat the expected genotype table as complete and recount alleles: two from each homozygote and one from each heterozygote, divided by \\(2N\\). That gives updated \\((p_A,p_B,p_O)\\). Iterate E and M until the changes are tiny and the observed log‑likelihood is no longer increasing.”"
  },
  {
    "objectID": "lectures/lecture-03.html#em-for-gene-frequencies-4",
    "href": "lectures/lecture-03.html#em-for-gene-frequencies-4",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "EM for Gene Frequencies",
    "text": "EM for Gene Frequencies\n\n\n\n\n\nGenotype\nPhenotype\nObserved Counts\nGenotype Frequency\n\n\n\n\n\\(AA\\)\n\\(A\\)\n\\(725\\)\n\\(p_A^2\\)\n\n\n\\(AO\\)\n\\(A\\)\n\n\\(2 p_A p_O\\)\n\n\n\\(AB\\)\n\\(AB\\)\n\\(72\\)\n\\(2 p_A p_B\\)\n\n\n\\(BB\\)\n\\(B\\)\n\\(258\\)\n\\(p_B^2\\)\n\n\n\\(BO\\)\n\\(B\\)\n\n\\(2 p_B p_O\\)\n\n\n\\(OO\\)\n\\(O\\)\n\\(1073\\)\n\\(p_O^2\\)\n\n\n\n\n\n\nScript:\n“With these sample counts—A = 725, AB = 72, B = 258, O = 1073—we can run EM. I’ll show convergence behavior next, and I want you to notice two things: the monotone increase in log‑likelihood and the impact of starting values.”"
  },
  {
    "objectID": "lectures/lecture-03.html#em-starting-values-matter",
    "href": "lectures/lecture-03.html#em-starting-values-matter",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "EM: Starting Values Matter",
    "text": "EM: Starting Values Matter\n\nDifferent initial \\((p_A, p_B, p_O)\\) can change speed (not the final MLE)\nStrategies:\n\nEqual: \\((1/3, 1/3, 1/3)\\)\nUnbalanced guess: \\((0.01, 0.98, 0.01)\\)\nSmart (use \\(p_O^{(0)} = \\sqrt{n_O/N}\\), solve for \\((p_A^{(0)}, p_B^{(0)})\\) from \\(2p_A p_B\\))\n\nTolerance of \\(1 \\times 10^{-6}\\)\n\n\nScript:\n“The ‘smart start’ uses two near‑identities: \\(n_O \\approx N p_O^2\\) gives \\(p_O^{(0)}\\approx\\sqrt{n_O/N}\\); then \\(n_{AB}\\approx 2N p_A p_B\\) plus \\(p_A+p_B=1-p_O^{(0)}\\) gives a quadratic for \\((p_A^{(0)},p_B^{(0)})\\). This gets you close to the basin of attraction and trims iterations substantially. Equal and wild unbalanced starts are good stress tests.”"
  },
  {
    "objectID": "lectures/lecture-03.html#em-starting-values-matter-1",
    "href": "lectures/lecture-03.html#em-starting-values-matter-1",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "EM: Starting Values Matter",
    "text": "EM: Starting Values Matter\n\n\n\n\n\nStrategy\nIterations\n\\(\\hat{p}_A\\)\n\\(\\hat{p}_B\\)\n\\(\\hat{p}_O\\)\n\n\n\n\nEqual\n7\n0.209131\n0.080801\n0.710068\n\n\nSmart\n4\n0.209131\n0.080801\n0.710068\n\n\nUnbalanced\n8\n0.209131\n0.080801\n0.710068\n\n\n\n\n\n\nScript:\n“Read this table left to right. Each row is a starting strategy; the last three columns are the converged allele frequency estimates. The point is not that the answers differ—they don’t—but that the iterations column can vary a lot. You can buy free speed by initializing intelligently.”"
  },
  {
    "objectID": "lectures/lecture-03.html#em-parameter-trajectories",
    "href": "lectures/lecture-03.html#em-parameter-trajectories",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "EM Parameter Trajectories",
    "text": "EM Parameter Trajectories\n\n\nScript:\n“Here are the trajectories for \\(p_A\\). Different starts head toward the same fixed point, and the paths are smooth. If you ever see oscillation or jaggedness here, it’s a warning sign about numerical underflow or a bad update rule.”"
  },
  {
    "objectID": "lectures/lecture-03.html#when-does-em-fail",
    "href": "lectures/lecture-03.html#when-does-em-fail",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "When does EM fail?",
    "text": "When does EM fail?\n\nNon-identifiability / flat likelihood: Ridges in segregation or linkage models; EM wanders or stalls.\nLocal maxima & starts: Multiple modes (e.g., mixture of penetrance classes); poor initialization traps EM.\nBoundary degeneracy: Rare allele/component weight driven to 0; variance or frequency estimates collapse.\nModel misspecification: Violated assumptions (e.g., Hardy-Weinberg, stratification) give misleading “convergence.”\n\n\nScript:\n“EM is not magic. If your model is not identifiable, EM will happily march along a ridge forever. If the likelihood is multimodal, bad starts can trap you. Near boundaries—think vanishing allele frequency—updates can collapse and stick. And if assumptions like HWE are wrong, EM will converge…to the wrong place. The antidotes are multiple starts, likelihood profiling, and stress‑testing the assumptions.”"
  },
  {
    "objectID": "lectures/lecture-03.html#linkage-vs-association-testing",
    "href": "lectures/lecture-03.html#linkage-vs-association-testing",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Linkage vs Association Testing",
    "text": "Linkage vs Association Testing\n\nLinkage: Tracks co-segregation of markers and traits within families to map disease genes.\n\nNull hypothesis: no linkage (independent assortment), \\(\\theta = 0.5\\).\n\nAssociation: Tests for correlation between variants and traits in populations to pinpoint causal loci.\n\nNull hypothesis: no association (e.g., \\(\\beta=0\\) or OR=1).\n\n\n\nScript:\n“Two mapping mindsets. Linkage works within families, asks whether a trait and a marker co‑segregate more than chance, and summarizes evidence on a centimorgan scale with LOD curves. It’s robust to population structure. Association works across unrelateds, tests regression coefficients or odds ratios, and resolves at kilobase scale, but is sensitive to confounding. We need both lenses.”"
  },
  {
    "objectID": "lectures/lecture-03.html#two-point-linkage-terms",
    "href": "lectures/lecture-03.html#two-point-linkage-terms",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Two-Point Linkage: Terms",
    "text": "Two-Point Linkage: Terms\n\nIndependent assortment (recall): when loci are unlinked, transmissions are independent and the chance a crossover separates them is \\(\\theta=0.5\\) (Mendel’s Second Law).\nInformative transmission/meiosis: a parent \\(\\to\\) child transmission where the transmitting parent is heterozygous at the loci of interest and the genotypes allow us to tell whether a crossover occurred.\n\nExample: parent \\(Aa/Bb\\) transmits \\(AB\\) or \\(ab\\) (nonrecombinant) vs \\(Ab\\) or \\(aB\\) (recombinant).\n\n\n\nScript:\n“An informative meiosis is one where you can tell whether a crossover happened. If the transmitting parent is not heterozygous at both loci, you can’t tell—so that meiosis doesn’t contribute information about \\(\\theta\\).”"
  },
  {
    "objectID": "lectures/lecture-03.html#two-point-linkage-terms-1",
    "href": "lectures/lecture-03.html#two-point-linkage-terms-1",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Two-Point Linkage: Terms",
    "text": "Two-Point Linkage: Terms\n\nRecombinant fraction \\(\\theta\\): the probability that a crossover occurs between two loci in a single meiosis; \\(0 \\le \\theta \\le 0.5\\).\nNonrecombinant transmission: the child receives an allele combination that matches one of the transmitting parent’s original allele pairs (no crossover between the loci).\nRecombinant transmission: the child receives a new combination relative to the transmitting parent’s original pair (a crossover occurred between the loci).\n\n\nScript:\n“Operationally, nonrecombinants copy one of the parental haplotypes; recombinants mix them. Our complete‑data likelihoods will be binomial in the counts of these two outcomes.”"
  },
  {
    "objectID": "lectures/lecture-03.html#two-point-linkage-repulsion-vs-coupling",
    "href": "lectures/lecture-03.html#two-point-linkage-repulsion-vs-coupling",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Two-Point Linkage: Repulsion vs Coupling",
    "text": "Two-Point Linkage: Repulsion vs Coupling\n\n\nScript:\n“Phase matters. In coupling, \\(AB/ab\\), nonrecombinants are \\(\\{AB,ab\\}\\) and recombinants are \\(\\{Ab,aB\\}\\). In repulsion, \\(Ab/aB\\), those roles flip. When phase is unknown, we’ll treat it as latent and estimate a posterior weight on the two configurations.”"
  },
  {
    "objectID": "lectures/lecture-03.html#two-point-linkage-direct-counting",
    "href": "lectures/lecture-03.html#two-point-linkage-direct-counting",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Two-Point Linkage: Direct Counting",
    "text": "Two-Point Linkage: Direct Counting\n\nCount \\(R\\) recombinants and \\(NR\\) nonrecombinants; total informative \\(I=R+NR\\).\nPoint estimate: \\(\\hat{\\theta}=R/I\\).\nLOD from counts (vs. independence at \\(\\theta=0.5\\)):\n\n\\[\n\\mathrm{LOD}(\\theta) = \\log_{10}\\!\\left\\{ \\frac{\\theta^{R} (1-\\theta)^{NR}}{0.5^{I}} \\right\\}\n\\]\n\nThis is a likelihood ratio test!\n\n\nScript:\n“In the simplest case, count recombinants \\(R\\) and informatives \\(I\\), and compute \\(\\hat\\theta=R/I\\). The LOD is a base‑10 log likelihood ratio against \\(\\theta=0.5\\). The curve peaks at \\(\\hat\\theta\\) and tells you where the evidence lies.”"
  },
  {
    "objectID": "lectures/lecture-03.html#worked-example-direct-counting-lod",
    "href": "lectures/lecture-03.html#worked-example-direct-counting-lod",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Worked Example: Direct Counting LOD",
    "text": "Worked Example: Direct Counting LOD\n\nSuppose \\(I=40\\) informative meioses with \\(R=12\\) recombinants.\nMLE \\(\\hat{\\theta}=R/I=12/40=0.3\\).\nFor \\(\\theta=0.3\\)\n\n\\[\n\\mathrm{LOD}(0.3) = \\log_{10}\\!\\left\\{ \\frac{0.3^{12} \\cdot 0.7^{28}}{0.5^{40}} \\right\\}.\n\\]\n\nScript:\n“With \\(R=12\\) out of \\(I=40\\), \\(\\hat\\theta=0.30\\). Plugging into the LOD gives about \\(1.43\\), i.e., roughly 27:1 odds for linkage at the MLE. That’s suggestive, but below the classical \\(LOD\\ge 3\\) bar. Modern practice often uses permutation‑based thresholds instead of a hard rule.”"
  },
  {
    "objectID": "lectures/lecture-03.html#worked-example-direct-counting-lod-1",
    "href": "lectures/lecture-03.html#worked-example-direct-counting-lod-1",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Worked Example: Direct Counting LOD",
    "text": "Worked Example: Direct Counting LOD\n\nR &lt;- 12\nN &lt;- 40\nthetahat &lt;- R / N\n\nlod &lt;- log10((thetahat^R * (1 - thetahat)^(N - R)) / (0.5^N))\nlod\n\n[1] 1.4294\n\n10^lod\n\n[1] 26.87819\n\n\n\nMaximize \\(\\mathrm{LOD}(\\theta)\\) over \\(\\theta \\in [0,0.5]\\) to estimate the recombination fraction; \\(LOD \\ge 3\\) and \\(\\leq -2\\) are classic heuristics (context-dependent).\n\n\nScript:\n“This code reproduces the arithmetic: LOD around 1.43, odds around 27:1. The maximizer of the LOD is exactly \\(R/I\\). The \\(\\pm\\) thresholds are heuristics, not laws—use context and proper genome‑wide calibration.”"
  },
  {
    "objectID": "lectures/lecture-03.html#worked-example-direct-counting-lod-2",
    "href": "lectures/lecture-03.html#worked-example-direct-counting-lod-2",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Worked Example: Direct Counting LOD",
    "text": "Worked Example: Direct Counting LOD\n\nUnder \\(H_0:\\theta=0.5\\) (a boundary point), \\(D = 2\\ln(10)\\cdot \\mathrm{LOD}(\\hat\\theta)\\) has the mixture limit \\(\\tfrac{1}{2}\\chi^2_0 + \\tfrac{1}{2}\\chi^2_1\\); thus \\(p = \\tfrac{1}{2}\\, \\Pr(\\chi^2_1 \\ge D)\\).\n\n\nD &lt;- 2 * log(10) * lod\npval &lt;- 0.5 * pchisq(D, df = 1, lower.tail = FALSE)\npval\n\n[1] 0.005148931\n\n\n\nScript:\n“At the boundary \\(\\theta=0.5\\), Wilks’ theorem gives a mixture null: half the mass at zero degrees of freedom and half at one. Concretely, compute \\(D=2\\ln(10)\\cdot \\mathrm{LOD}\\) and take \\(p=\\tfrac{1}{2}\\Pr(\\chi^2_1\\ge D)\\). This is the right conversion for two‑point tests against \\(\\theta=0.5\\).”"
  },
  {
    "objectID": "lectures/lecture-03.html#two-point-linkage-em-with-unknown-phase",
    "href": "lectures/lecture-03.html#two-point-linkage-em-with-unknown-phase",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Two-Point Linkage: EM with Unknown Phase",
    "text": "Two-Point Linkage: EM with Unknown Phase\n\nTransmitting parent: double het \\(Aa/Bb\\) with unknown phase.\nMate: Aa/bb (heterozygous at \\(A\\), homozygous at \\(B\\)).\n\nObserved per child: two-locus genotypes \\((A\\text{-genotype}, B\\text{-genotype})\\).\n\n\nScript:\n“Now a case where EM is actually necessary. The mate is bb, so the child’s \\(B\\) allele comes entirely from the transmitting parent. That’s helpful. But the mate is Aa at \\(A\\), so whenever a child is Aa, we can’t tell whether the transmitting parent contributed \\(A\\) or \\(a\\). Those are ambiguous transmissions between two haplotypes—exactly the kind of soft labeling EM was built for.”"
  },
  {
    "objectID": "lectures/lecture-03.html#two-point-linkage-em-with-unknown-phase-1",
    "href": "lectures/lecture-03.html#two-point-linkage-em-with-unknown-phase-1",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Two-Point Linkage: EM with Unknown Phase",
    "text": "Two-Point Linkage: EM with Unknown Phase\n\nMissing-data view: the parental phase (coupling vs repulsion) is not observed. We make it a complete-data problem by introducing a weight \\(w\\in[0,1]\\) that blends the two phase-specific models.\nModel (given recombination fraction \\(\\theta\\)):\n\nCoupling: \\(P(\\text{NR})=1-\\theta\\), \\(P(\\text{R})=\\theta\\) (split equally across the two categories).\nRepulsion: \\(P(\\text{NR})=\\theta\\), \\(P(\\text{R})=1-\\theta\\).\n\n\n\nScript:\n“Think of phase as a two‑component latent class: coupling vs repulsion. We’ll carry a single scalar, \\(w\\), the posterior weight on coupling. Under coupling, nonrecombinants are more likely by \\(1-\\theta\\); under repulsion the roles swap. EM will update \\(w\\) and then update \\(\\theta\\) from weighted recombinant counts.”"
  },
  {
    "objectID": "lectures/lecture-03.html#two-point-linkage-em-with-unknown-phase-2",
    "href": "lectures/lecture-03.html#two-point-linkage-em-with-unknown-phase-2",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Two-Point Linkage: EM with Unknown Phase",
    "text": "Two-Point Linkage: EM with Unknown Phase\nE-step (update the phase weight)\n\\[\nw = \\frac{(1-\\theta^{(t)})^{\\,n_{\\text{NR}}}\\,(\\theta^{(t)})^{\\,n_{\\text{R}}}}\n{(1-\\theta^{(t)})^{\\,n_{\\text{NR}}}\\,(\\theta^{(t)})^{\\,n_{\\text{R}}} + (\\theta^{(t)})^{\\,n_{\\text{NR}}}\\,(1-\\theta^{(t)})^{\\,n_{\\text{R}}}}\n\\]\nand use \\(1-w\\) as the weight on the repulsion configuration.\n\nScript:\n“This is the posterior weight on coupling: likelihood under coupling divided by the sum of likelihoods under both phases. In code, use logs and a log‑sum‑exp trick to avoid underflow if \\(n_{\\text{NR}}\\) or \\(n_{\\text{R}}\\) is large.”\n\nM-step (update \\(\\theta\\))\n\\[\\begin{align*}\n\\theta^{(t+1)}\n&= \\frac{\\text{(weighted recombinants)}}{N}\n= \\frac{w\\,n_{\\text{R}} + (1-w)\\,n_{\\text{NR}}}{N}.\n\\end{align*}\\]\n\nScript:\n“The M‑step is ‘recombinants over informatives’, but with a twist: we average the recombinant counts under coupling and repulsion using the current \\(w\\). That keeps the intuition intact.”"
  },
  {
    "objectID": "lectures/lecture-03.html#from-two-point-em-to-multipoint",
    "href": "lectures/lecture-03.html#from-two-point-em-to-multipoint",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "From Two-Point EM to Multipoint",
    "text": "From Two-Point EM to Multipoint\n\nSame objective: estimate recombination while marginalizing over latent transmissions/phase under current \\(\\theta\\).\nE-step engine: compute required state weights via Elston–Stewart peeling (pedigrees) or Lander–Green forward–backward (marker HMM) when you have multiple markers and missing genotypes.\nQuantities needed: \\(\\mathbb E[\\#\\,\\text{recombinants between adjacent markers}]\\) and \\(\\mathbb E[\\#\\,\\text{informative transmissions}]\\) under current map.\nUse in practice: either (a) plug these expectations into an EM-style update, or (b) more commonly, scan positions to build a LOD curve and report peak and 1-LOD interval.\n\n\nScript:\n“In multipoint linkage, the hidden object is the inheritance state along the chromosome. The forward–backward algorithm gives us posterior state probabilities; from those we compute expected recombinant counts between markers and assemble LOD curves across the map. It’s the same EM logic, just powered by an HMM.”"
  },
  {
    "objectID": "lectures/lecture-03.html#multipoint-linkage-hmm",
    "href": "lectures/lecture-03.html#multipoint-linkage-hmm",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Multipoint Linkage (HMM)",
    "text": "Multipoint Linkage (HMM)\n\nMarkers along a chromosome define an HMM over inheritance states; adjacent recombination rates drive transitions.\nCombine penetrance with the marker HMM to compute \\(L(\\theta)\\) efficiently as you slide along the map (forward–backward yields the needed state probabilities even with missing phase/genotypes).\nPros: more information than two-point; better localization. Cons: requires a genetic map and error modeling.\n\n\nScript:\n“Two practical cautions: include a genotyping‑error model—even a small error rate flattens LODs if ignored—and consider sex‑specific maps or interference if your study demands it. Otherwise, the HMM machinery scales cleanly.”"
  },
  {
    "objectID": "lectures/lecture-03.html#missing-data-em-in-linkage",
    "href": "lectures/lecture-03.html#missing-data-em-in-linkage",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Missing-Data EM in Linkage",
    "text": "Missing-Data EM in Linkage\n\nComplete data: informative meioses labeled as recombinant/nonrecombinant; incomplete data: untyped genotypes/phase.\nE-step: compute \\(E[\\text{\\# recombinants}]\\) and \\(\\mathrm E[\\text{\\# informative meioses}]\\) given current \\(\\theta^{(t)}\\) via peeling/HMM.\nM-step: \\(\\displaystyle \\theta^{(t+1)} = \\frac{\\mathrm E[R\\mid\\text{data},\\theta^{(t)}]}{\\mathrm E[I\\mid\\text{data},\\theta^{(t)}]}\\).\nIterate across \\(\\theta\\) grid to produce a LOD curve; maximize or report support interval.\n\n\nScript:\n“This slide distills the recipe: compute expected recombinants and informatives under the current map, then update \\(\\theta\\) as their ratio. In practice we report the LOD curve and a 1‑LOD support interval rather than a single point estimate.”"
  },
  {
    "objectID": "lectures/lecture-03.html#practical-issues-in-linkage",
    "href": "lectures/lecture-03.html#practical-issues-in-linkage",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Practical Issues in Linkage",
    "text": "Practical Issues in Linkage\n\nModel specification: penetrance, phenocopy, allele frequencies; misspecification can distort LOD.\nMarker selection: highly polymorphic markers increase information; account for sex-specific recombination if needed.\nComputational trade-offs: many markers and large pedigrees require approximations or pruning; consider parametric vs nonparametric linkage.\n\n\nScript:\n“Linkage analyses are only as good as their trait model. Probe sensitivity to penetrance and phenocopy. Use polymorphic markers to maximize information, and thin dense SNPs if LD bloats state spaces. For large pedigrees, Elston–Stewart pruning is your friend.”"
  },
  {
    "objectID": "lectures/lecture-03.html#linkage-disequilibrium",
    "href": "lectures/lecture-03.html#linkage-disequilibrium",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Linkage Disequilibrium",
    "text": "Linkage Disequilibrium\n\nLet alleles at two markers be denoted \\(A, a\\) and \\(B, b\\)\nDefine allele frequencies by \\(P_A, P_a, P_B, P_b\\)\nDefine frequencies of haplotypes as \\(P_{AB}, P_{Ab}, P_{aB}, P_{ab}\\)\nDefine linkage equilibrium as independence in the \\(2 \\times 2\\) table\n\nFor example, \\(P_{AB} = P_A \\times P_B\\)\n\n\n\nScript:\n“LD is simply statistical dependence between alleles on the same haplotype. Think forces: drift and finite population size create dependence, recombination erodes it, selection and admixture can inflate it.”"
  },
  {
    "objectID": "lectures/lecture-03.html#linkage-disequilibrium-1",
    "href": "lectures/lecture-03.html#linkage-disequilibrium-1",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Linkage Disequilibrium",
    "text": "Linkage Disequilibrium\n\nDefine the LD coefficient as \\(D\\)\n\n\\[D = p_{AB} - p_A p_B\\]\n\nWhat are we looking at here?\n\n\nScript:\n“Algebraically, \\(D\\) is the covariance between the allele indicators on a random haplotype. If \\(D&gt;0\\), \\(AB\\) is over‑represented relative to independence; if \\(D&lt;0\\), it’s under‑represented.”"
  },
  {
    "objectID": "lectures/lecture-03.html#linkage-disequilibrium-2",
    "href": "lectures/lecture-03.html#linkage-disequilibrium-2",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Linkage Disequilibrium",
    "text": "Linkage Disequilibrium\n\nDefine the LD coefficient as \\(D\\)\n\n\\[D = p_{AB} - p_A p_B\\]\n\nNote that\n\n\\(\\operatorname{E}[A] = p_A, \\operatorname{E}[B] = p_B\\)\n\\(\\operatorname{E}[AB] = p_{AB}\\)\n\\(\\operatorname{Cov}(A, B) = \\operatorname{E}[AB] - \\operatorname{E}[A]\\operatorname{E}[B]\\)\n\n\n\nScript:\n“Formalizing that intuition: let \\(A,B\\in\\{0,1\\}\\) indicate the presence of alleles on a haplotype. Then \\(D=\\operatorname{Cov}(A,B)\\). This is why correlation \\(r\\) shows up naturally on the next slide.”"
  },
  {
    "objectID": "lectures/lecture-03.html#linkage-disequilibrium-3",
    "href": "lectures/lecture-03.html#linkage-disequilibrium-3",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Linkage Disequilibrium",
    "text": "Linkage Disequilibrium\n\n\\(D\\), or \\(\\operatorname{Cov}(A, B)\\), is scale-dependent\nNormalized measures:\n\n\\(D' = D / D_{\\max}\\), where \\(D_{\\max} = \\min(p_A p_b, p_a p_B)\\) if \\(D &gt; 0\\)\nPearson correlation:\n\n\n\\[\\begin{align}\nr\n&= \\frac{\\operatorname{Cov}(A, B)}{\\sqrt{\\operatorname{Var}(A) \\operatorname{Var}(B)}} \\\\\n&= \\frac{D}{\\sqrt{p_A(1 - p_A) p_B(1 - p_B)}}\n\\end{align}\\]\n\nScript:\n“\\(D\\) depends on allele frequencies, so we normalize. Two popular choices: \\(D'\\) and \\(r\\). For association power, \\(r\\)—and really \\(r^2\\)—matters most because it directly scales the noncentrality parameter: effect sizes at a tag SNP are attenuated by about \\(r\\) relative to the causal.”"
  },
  {
    "objectID": "lectures/lecture-03.html#goal-of-association-ld-link",
    "href": "lectures/lecture-03.html#goal-of-association-ld-link",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Goal of Association & LD Link",
    "text": "Goal of Association & LD Link\n\nGoal: test the null of no effect (e.g., \\(\\beta=0\\) or OR\\(=1\\)) to identify regions harboring causal variants.\nWe test markers, not necessarily the causal variant. If a marker has nonzero LD (correlation \\(r\\)) with a causal variant, its association test can detect signal from that causal.\nStronger \\(|r|\\) implies a stronger expected signal at the marker; if \\(r\\approx 0\\), the marker carries no information about that causal.\nSign matters: \\(r&lt;0\\) flips the observed effect direction at the marker relative to the causal.\nIn practice, nearby markers within LD blocks show clustered p-values and similar effect signs.\n\n\nScript:\n“Here’s the bridge: association tests fire at markers. If a marker correlates with a causal by \\(r\\), the observed effect is roughly \\(r\\) times the causal effect, and power scales with \\(r^2\\). That’s why p‑values cluster within LD blocks and why effect signs can flip if \\(r&lt;0\\).”"
  },
  {
    "objectID": "lectures/lecture-03.html#single-marker-association-core-test-linear",
    "href": "lectures/lecture-03.html#single-marker-association-core-test-linear",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Single-Marker Association: Core Test (Linear)",
    "text": "Single-Marker Association: Core Test (Linear)\n\nLinear trait: \\(Y=\\alpha+\\beta G+\\gamma^T C+\\varepsilon\\); test \\(H_0{:}\\,\\beta=0\\).\nGenotype encodings: additive (0/1/2) or genotypic (AA/Aa/aa); include dominance to test non-additivity.\nBinary outcomes (case–control) and logistic models will be covered later.\n\n\nScript:\n“Regression 101 with genetics details. Code genotypes additively unless biology suggests dominance or recessivity; add a dominance term if you want to test non‑additivity. Always include relevant covariates—sex, age, and PCs for ancestry—because stratification can fake associations.”"
  },
  {
    "objectID": "lectures/lecture-03.html#basic-qc-pipeline-variant-level",
    "href": "lectures/lecture-03.html#basic-qc-pipeline-variant-level",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Basic QC Pipeline (Variant-level)",
    "text": "Basic QC Pipeline (Variant-level)\n\nCall rate/missingness (e.g., variant missingness &lt; 2%).\nHardy–Weinberg equilibrium in controls (e.g., P &gt; 1e−6), mindful of true deviations.\nMinor allele frequency threshold (e.g., MAF ≥ 0.01 unless rare-variant methods used).\nDifferential missingness across case/control; strand/allele checks.\n\n\nScript:\n“QC is prevention. Missingness filters remove low‑quality sites; HWE in controls flags genotyping problems; MAF thresholds avoid unstable estimates unless you’re using rare‑variant methods. Don’t forget strand and allele checks—mismatches there create phantom effects.”"
  },
  {
    "objectID": "lectures/lecture-03.html#basic-qc-pipeline-sample-level",
    "href": "lectures/lecture-03.html#basic-qc-pipeline-sample-level",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Basic QC Pipeline (Sample-level)",
    "text": "Basic QC Pipeline (Sample-level)\n\nSample call rate; sex checks from X chr; heterozygosity outliers.\nRelatedness/duplicates via IBD; ancestry via PCA; remove outliers or adjust with PCs.\nDuplicates/cryptic relatedness: retain one per pair or use mixed models.\n\n\nScript:\n“At the sample level, verify sex against X‑chromosome heterozygosity, look for heterozygosity outliers, and identify relatives with IBD. Use PCA to quantify ancestry and either exclude outliers or adjust downstream. Mixed models help with relatedness, but don’t use them as an excuse to skip QC.”"
  },
  {
    "objectID": "lectures/lecture-03.html#summary-key-takeaways",
    "href": "lectures/lecture-03.html#summary-key-takeaways",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Summary & Key Takeaways",
    "text": "Summary & Key Takeaways\n\nLikelihood optimization: Newton–Raphson (score/Hessian), Gradient Descent/SGD (scalable), and EM (latent-data) are core tools.\nEM for ABO gene frequencies: initialization affects speed; log-likelihood increases monotonically until convergence.\nLinkage: two-point LOD and unknown phase via EM; multipoint mapping framed as an HMM for efficient inference with missing data.\nLD basics: r and r^2 quantify correlation between markers and explain why nearby markers can show similar association signals when a causal variant is unobserved\n\n\nScript:\n“Take‑home messages. Newton and first‑order methods are your optimization workhorses. EM turns missingness into a tractable expectation–maximization cycle and increases the log‑likelihood monotonically. Linkage is about family transmissions; association is about population correlation. And \\(r^2\\) is the currency that converts LD into expected association signal.”"
  },
  {
    "objectID": "lectures/lecture-03.html#hands-on-lab-03-applied",
    "href": "lectures/lecture-03.html#hands-on-lab-03-applied",
    "title": "Lecture 03: Fitting Likelihoods, Linkage, and Association",
    "section": "Hands-On: Lab 03 (Applied)",
    "text": "Hands-On: Lab 03 (Applied)\n\nImplement ABO EM and compare starts; plot log-likelihood gap.\nTwo-point linkage: LOD grid search + EM with unknown phase.\nTwo-SNP haplotype EM; compute LD (D, r^2).\nMini association demo: linear regression + HWE QC in controls.\n\nSee labs/unsolved/lab-03.R for the scaffold (with TODOs) and labs/solved/lab-03.R for a worked solution.\n\nScript:\n“For the lab: keep a log of starting values and attained log‑likelihoods—this is good hygiene with EM. In the two‑point EM, verify that ambiguous transmissions are being split sensibly as \\(\\theta\\) and \\(w\\) update. In the haplotype EM, watch for boundary estimates at very low MAFs. And in the association mini‑demo, confirm that including PCs meaningfully reduces ancestry structure before you trust the p‑values.”"
  },
  {
    "objectID": "lectures/lecture-04.html#the-bayesian-paradigm",
    "href": "lectures/lecture-04.html#the-bayesian-paradigm",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "The Bayesian Paradigm",
    "text": "The Bayesian Paradigm\n\n\nLet \\theta be unknown parameters, y observed data\nWhile frequentist methods treat \\theta as fixed but unknown, Bayesian methods treat \\theta as random variables\n\n\nSir Thomas Bayes"
  },
  {
    "objectID": "lectures/lecture-04.html#bayes-theorem",
    "href": "lectures/lecture-04.html#bayes-theorem",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\nUncertainty in \\theta is modeled via a prior distribution p(\\theta)\nObserved data y are modeled via a likelihood p(y|\\theta)\nBayes’ theorem combines these to yield the posterior distribution p(\\theta|y) \\propto p(y|\\theta)p(\\theta)"
  },
  {
    "objectID": "lectures/lecture-04.html#frequentist-vs-bayesian-terminology",
    "href": "lectures/lecture-04.html#frequentist-vs-bayesian-terminology",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Frequentist vs Bayesian Terminology",
    "text": "Frequentist vs Bayesian Terminology\n\n\n\n\n\n\n\n\n\n\nConcept\nFrequentist\nBayesian\n\n\n\n\nParameter\nFixed\nRandom\n\n\nData\nRandom\nFixed\n\n\nEstimate\nPoint estimate\nPosterior distribution\n\n\nConfidence Interval\nInterval with coverage\nCredible interval\n\n\nHypothesis Test\nReject/Fail to reject\nPosterior predictive check\n\n\nP-value\nProbability of data under null\nProbability of hypothesis given data\n\n\nLikelihood\nFunction of parameter given data\nFunction of data given parameter"
  },
  {
    "objectID": "lectures/lecture-04.html#interpretation-of-intervals",
    "href": "lectures/lecture-04.html#interpretation-of-intervals",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Interpretation of Intervals",
    "text": "Interpretation of Intervals\nCompare statements on intervals\n\nFrequentist: “Over many repeated samples, 95% of such intervals will contain the true parameter.”\nBayesian: “Given the observed data and prior, there is a 95% probability that the parameter lies within this interval.”"
  },
  {
    "objectID": "lectures/lecture-04.html#bayesian-pros-and-cons",
    "href": "lectures/lecture-04.html#bayesian-pros-and-cons",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Bayesian Pros and Cons",
    "text": "Bayesian Pros and Cons\nPros\n\nIntuitive: direct probability statements about parameters\nFlexibility: complex models, hierarchical structures, small-n\nIncorporate prior knowledge or expert opinion\nFull uncertainty quantification via posterior distributions\n\nCons\n\nComputationally intensive: MCMC, variational inference\nSensitivity to prior choice\nNot the status quo in many fields"
  },
  {
    "objectID": "lectures/lecture-04.html#further-resources",
    "href": "lectures/lecture-04.html#further-resources",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Further Resources",
    "text": "Further Resources\n\n\n\n\n\n\nMcElreath (2020)\n\n\n\n\n\n\n\nGelman et al. (2013)"
  },
  {
    "objectID": "lectures/lecture-04.html#conjugacy",
    "href": "lectures/lecture-04.html#conjugacy",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Conjugacy",
    "text": "Conjugacy\n\nA prior is conjugate to a likelihood if the posterior is in the same family as the prior.\nExample: Beta prior + Binomial likelihood to Beta posterior"
  },
  {
    "objectID": "lectures/lecture-04.html#conjugacy-example",
    "href": "lectures/lecture-04.html#conjugacy-example",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Conjugacy Example",
    "text": "Conjugacy Example\nConsider the allele-frequency estimation setup introduced in Lecture 1.\n\nPrevious experiment: observed x_{\\text{prev}} = 3 successes (allele A) out of n_{\\text{prev}} = 20 trials.\nCurrent data: x = 11 successes out of n = 27 trials.\nParameter of interest: allele (“success”) frequency p."
  },
  {
    "objectID": "lectures/lecture-04.html#step-1-prior",
    "href": "lectures/lecture-04.html#step-1-prior",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Step 1: Prior",
    "text": "Step 1: Prior\nFrom the previous experiment (3 successes out of 20 trials), we can form a Beta prior:\nInterpret \\alpha and \\beta as pseudo-counts of successes and failures, respectively. (Note that \\text{Beta}(1,1) is uniform on [0,1].)\n\\begin{gather*}\np \\sim \\text{Beta}(\\alpha, \\beta) \\\\\n\\alpha = x_{\\text{prev}} + 1 = 4 \\\\\n\\beta = n_{\\text{prev}} - x_{\\text{prev}} + 1 = 18.\n\\end{gather*}"
  },
  {
    "objectID": "lectures/lecture-04.html#step-2-likelihood",
    "href": "lectures/lecture-04.html#step-2-likelihood",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Step 2: Likelihood",
    "text": "Step 2: Likelihood\nCurrent data likelihood (up to proportionality in p): L(p; x,n) \\propto p^{x} (1-p)^{n-x} = p^{11}(1-p)^{16}.\nRecognize this kernel as\n\n\\text{Beta}(x+1, n-x+1)=\\text{Beta}(12,17)"
  },
  {
    "objectID": "lectures/lecture-04.html#step-3-posterior-conjugacy",
    "href": "lectures/lecture-04.html#step-3-posterior-conjugacy",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Step 3: Posterior (Conjugacy)",
    "text": "Step 3: Posterior (Conjugacy)\nMultiply prior and likelihood kernels: p^{\\alpha-1}(1-p)^{\\beta-1}\\times p^{x}(1-p)^{n-x} = p^{(\\alpha + x) - 1}(1-p)^{(\\beta + n - x) - 1}.\nThus\n\\begin{align*}\np | x \\sim \\text{Beta}(\\alpha + x,\\ \\beta + n - x)\n&= \\text{Beta}(4+11,\\ 18+16) \\\\\n&= \\text{Beta}(15,34).\n\\end{align*}"
  },
  {
    "objectID": "lectures/lecture-04.html#posterior-summaries",
    "href": "lectures/lecture-04.html#posterior-summaries",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Posterior Summaries",
    "text": "Posterior Summaries\n\nPosterior mean: E[p|x] = \\frac{15}{15+34} = 0.306 (shrinks slightly toward prior mean \\frac{4}{22}=0.182 relative to MLE \\hat p = 11/27 \\approx 0.407).\nEffective sample size intuition: prior contributes (\\alpha+\\beta-2)=20 pseudo-trials; data contribute 27 real trials."
  },
  {
    "objectID": "lectures/lecture-04.html#visualization",
    "href": "lectures/lecture-04.html#visualization",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Visualization",
    "text": "Visualization"
  },
  {
    "objectID": "lectures/lecture-04.html#non-conjugate-models",
    "href": "lectures/lecture-04.html#non-conjugate-models",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Non-Conjugate Models",
    "text": "Non-Conjugate Models\n\nConjugacy is convenient but limited to simple models\nMany realistic models (e.g., multinomial/Dirichlet with nonlinear transforms) are non-conjugate\nBayesian inference in these models typically requires computational methods like MCMC or variational inference\nRichard McElreath has a great youtube video on MCMC"
  },
  {
    "objectID": "lectures/lecture-04.html#bayesian-revisit-abo-allele-frequencies",
    "href": "lectures/lecture-04.html#bayesian-revisit-abo-allele-frequencies",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Bayesian Revisit: ABO Allele Frequencies",
    "text": "Bayesian Revisit: ABO Allele Frequencies\n\nGoal: Infer allele frequencies (p_A, p_B, p_O) given phenotype counts (n_A, n_AB, n_B, n_O).\nFrequentist (Lecture 03): EM treats latent genotypes for A and B phenotypes.\nBayesian: Place a prior on allele frequencies; integrate (average) over uncertainty rather than impute expected counts."
  },
  {
    "objectID": "lectures/lecture-04.html#data-sufficient-statistics",
    "href": "lectures/lecture-04.html#data-sufficient-statistics",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Data & Sufficient Statistics",
    "text": "Data & Sufficient Statistics\n\nFrom Lecture 03, we have:\nn_A = 725 individuals with A phenotype\nn_{AB} = 72 individuals with AB phenotype\nn_{B} = 258 individuals with B phenotype\nn_{O} = 1073 individuals with O phenotype\nTotal sample size: N = 2128 individuals"
  },
  {
    "objectID": "lectures/lecture-04.html#model-specification",
    "href": "lectures/lecture-04.html#model-specification",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Model Specification",
    "text": "Model Specification\n\nAllele frequency vector: \\boldsymbol p=(p_A,p_B,p_O), \\boldsymbol p\\sim\\text{Dirichlet}(\\boldsymbol\\alpha).\nUnder HWE, genotype frequencies: p_A^2, 2p_A p_O, p_B^2, 2 p_B p_O, 2 p_A p_B, p_O^2.\nPhenotype probabilities (aggregating ambiguous genotypes):\nP(\\text{A}) = p_A^2 + 2 p_A p_O\nP(\\text{AB}) = 2 p_A p_B\nP(\\text{B}) = p_B^2 + 2 p_B p_O\nP(\\text{O}) = p_O^2\nLikelihood: (n_A,n_{AB},n_B,n_O) \\sim \\text{Multinomial}(N, \\boldsymbol q) with \\boldsymbol q above."
  },
  {
    "objectID": "lectures/lecture-04.html#prior-families",
    "href": "lectures/lecture-04.html#prior-families",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Prior Families",
    "text": "Prior Families\n\nWeak Dirichlet(1,1,1) (uniform over allele simplex)\nMild Dirichlet(2,2,2) (light shrink toward center)"
  },
  {
    "objectID": "lectures/lecture-04.html#using-historical-data",
    "href": "lectures/lecture-04.html#using-historical-data",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Using Historical Data",
    "text": "Using Historical Data\n\nConsider global survey means (p_A = 0.26, p_B = 0.09, p_O = 0.65) (Mourant et al., 1976; Yamamoto et al., 2012).\nDirichlet prior: \\mathbf{\\alpha} = k (0.26, 0.09, 0.65)\nEffective sample size idea: acts like observing k allele draws before current data (2N alleles in sample).\nRelative weight vs data (here total alleles = 2N = 4256):\nk = 200 \\to prior weight \\approx 4.5\\% of total information.\nk = 1000 \\to prior weight \\approx 19.0\\% of total information."
  },
  {
    "objectID": "lectures/lecture-04.html#prior-shapes-marginal-densities",
    "href": "lectures/lecture-04.html#prior-shapes-marginal-densities",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Prior Shapes (Marginal Densities)",
    "text": "Prior Shapes (Marginal Densities)"
  },
  {
    "objectID": "lectures/lecture-04.html#how-to-fit-these-models",
    "href": "lectures/lecture-04.html#how-to-fit-these-models",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "How to fit these models?",
    "text": "How to fit these models?\n\n\n\n\nSo we have our data, likelihood, and priors\nWe can use MCMC to sample from the posterior distribution of allele frequencies\nThere exist many software packages to do this! We will use Stan (Carpenter et al., 2017) via the cmdstanr R package"
  },
  {
    "objectID": "lectures/lecture-04.html#stan-model",
    "href": "lectures/lecture-04.html#stan-model",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Stan Model",
    "text": "Stan Model\nStep 1: Phenotype Probabilities\n1functions {\n2    vector abo_pheno_probs(vector p) {\n3        real pA = p[1];\n        real pB = p[2];\n        real pO = p[3];\n        vector[4] q;\n        q[1] = pA * pA + 2 * pA * pO; // A phenotype\n        q[2] = 2 * pA * pB;            // AB phenotype\n        q[3] = pB * pB + 2 * pB * pO; // B phenotype\n        q[4] = pO * pO;               // O phenotype\n        return q;\n        }                                            \n}\n\n1\n\nDeclare the functions block (optional in Stan, but lets us encapsulate logic)\n\n2\n\nDefine a helper that maps allele frequencies to phenotype probabilities\n\n3\n\nExtract components (for readability in later expressions)"
  },
  {
    "objectID": "lectures/lecture-04.html#stan-model-1",
    "href": "lectures/lecture-04.html#stan-model-1",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Stan Model",
    "text": "Stan Model\nStep 1: Phenotype Probabilities\nfunctions {\n    vector abo_pheno_probs(vector p) {\n        real pA = p[1];\n        real pB = p[2];\n        real pO = p[3];\n4        vector[4] q;\n5        q[1] = pA * pA + 2 * pA * pO; // A phenotype\n        q[2] = 2 * pA * pB;            // AB phenotype\n        q[3] = pB * pB + 2 * pB * pO; // B phenotype\n        q[4] = pO * pO;               // O phenotype\n6        return q;\n        }                                            \n}\n\n4\n\nAllocate a length-4 vector q (A, AB, B, O) for phenotype probabilities\n\n5\n\nHardy–Weinberg genotype algebra aggregated into phenotype probabilities\n\n6\n\nReturn the vector"
  },
  {
    "objectID": "lectures/lecture-04.html#stan-model-2",
    "href": "lectures/lecture-04.html#stan-model-2",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Stan Model",
    "text": "Stan Model\nStep 2: Data & Transforms\n1data {\n2    int&lt;lower=0&gt; nA;\n    int&lt;lower=0&gt; nAB;\n    int&lt;lower=0&gt; nB;\n    int&lt;lower=0&gt; nO;\n3    vector&lt;lower=0&gt;[3] alpha; # Dirichlet hyperparameters\n}\n\ntransformed data {\n    int&lt;lower=0&gt; N = nA + nAB + nB + nO;\n    array[4] int y = { nA, nAB, nB, nO };\n}\n\n1\n\nRaw observed counts and prior hyperparameters enter in the data block.\n\n2\n\nABO phenotype counts (non-negative integers) per category.\n\n3\n\nDirichlet prior parameters supplied from R (allow different priors via alpha)."
  },
  {
    "objectID": "lectures/lecture-04.html#stan-model-3",
    "href": "lectures/lecture-04.html#stan-model-3",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Stan Model",
    "text": "Stan Model\nStep 2: Data & Transforms\ndata {\n    int&lt;lower=0&gt; nA;\n    int&lt;lower=0&gt; nAB;\n    int&lt;lower=0&gt; nB;\n    int&lt;lower=0&gt; nO;\n    vector&lt;lower=0&gt;[3] alpha; # Dirichlet hyperparameters\n}\n\n4transformed data {\n5    int&lt;lower=0&gt; N = nA + nAB + nB + nO;\n6    array[4] int y = { nA, nAB, nB, nO };\n}\n\n4\n\ntransformed data pre-computes deterministic quantities once\n\n5\n\nTotal sample size N used for reference or diagnostics\n\n6\n\nAssemble counts into an array to pass to the multinomial"
  },
  {
    "objectID": "lectures/lecture-04.html#stan-model-4",
    "href": "lectures/lecture-04.html#stan-model-4",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Stan Model",
    "text": "Stan Model\nStep 3: Parameters & Derived q\n1parameters {\n2    simplex[3] p;  # (pA, pB, pO) on 2-simplex\n}\n\n3transformed parameters {\n4    vector[4] q = abo_pheno_probs(p);\n}\n\n1\n\nDeclare unknown quantities to infer in parameters\n\n2\n\nsimplex[3] enforces positivity and sum-to-one constraints automatically\n\n3\n\ntransformed parameters recomputes per-draw derived values\n\n4\n\nReuse helper to obtain phenotype probabilities from allele frequencies"
  },
  {
    "objectID": "lectures/lecture-04.html#stan-model-step-4-prior-likelihood",
    "href": "lectures/lecture-04.html#stan-model-step-4-prior-likelihood",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Stan Model (Step 4: Prior & Likelihood)",
    "text": "Stan Model (Step 4: Prior & Likelihood)\n1model {\n2    p ~ dirichlet(alpha);\n3    y ~ multinomial(q);\n}\n\n1\n\nThe model block contains all sampling statements contributing to log density\n\n2\n\nDirichlet prior on allele frequencies p\n\n3\n\nMultinomial likelihood over phenotype counts with probabilities q."
  },
  {
    "objectID": "lectures/lecture-04.html#stan-model-step-5-generated-quantities",
    "href": "lectures/lecture-04.html#stan-model-step-5-generated-quantities",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Stan Model (Step 5: Generated Quantities)",
    "text": "Stan Model (Step 5: Generated Quantities)\n1generated quantities {\n2    real log_lik = multinomial_lpmf(y | q);\n3    vector[6] geno_freq;\n4    real pA = p[1]; real pB = p[2]; real pO = p[3];\n5    geno_freq[1] = pA * pA;        # AA\n    geno_freq[2] = 2 * pA * pO;    # AO\n    geno_freq[3] = pB * pB;        # BB\n    geno_freq[4] = 2 * pB * pO;    # BO\n    geno_freq[5] = 2 * pA * pB;    # AB\n    geno_freq[6] = pO * pO;        # OO\n}\n\n1\n\nPost-processing: quantities saved per posterior draw\n\n2\n\nStore log-likelihood for model comparison / LOO / WAIC\n\n3\n\nAllocate genotype frequency vector\n\n4\n\nLocal aliases improve clarity when computing genotype frequencies\n\n5\n\nHardy–Weinberg genotype probabilities"
  },
  {
    "objectID": "lectures/lecture-04.html#compile-stan-model",
    "href": "lectures/lecture-04.html#compile-stan-model",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Compile Stan Model",
    "text": "Compile Stan Model\n\nmod_path &lt;- file.path(\"stan\", \"abo_multinomial.stan\")\nabo_mod &lt;- cmdstan_model(mod_path)\n\n\nStan uses C++ on the backend, so we need to compile the model once before fitting\ncmdstan_model() handles compilation and returns a model object for sampling"
  },
  {
    "objectID": "lectures/lecture-04.html#fit-weak-prior-dirichlet111",
    "href": "lectures/lecture-04.html#fit-weak-prior-dirichlet111",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Fit: Weak Prior (Dirichlet(1,1,1))",
    "text": "Fit: Weak Prior (Dirichlet(1,1,1))\n\ncounts &lt;- list(nA = 725, nAB = 72, nB = 258, nO = 1073)\n1fit_weak &lt;- abo_mod$sample(\n2    data = list(\n        nA = counts$nA, nAB = counts$nAB, nB = counts$nB, nO = counts$nO,\n        alpha = rep(1, 3)\n    ),\n    seed = 8878,\n3    chains = 4, parallel_chains = 4,\n    iter_warmup = 1000, iter_sampling = 1000,\n    refresh = 0\n)\n\n\n1\n\nExecute the MCMC sampler using the sample() method on the compiled model object\n\n2\n\nPass data from R to Stan as a named list\n\n3\n\nSpecify MCMC sampler settings\n\n\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 1.6 seconds."
  },
  {
    "objectID": "lectures/lecture-04.html#model-diagnostics",
    "href": "lectures/lecture-04.html#model-diagnostics",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n\nfit_weak$summary()\n\n# A tibble: 18 × 10\n   variable       mean   median      sd     mad       q5      q95  rhat ess_bulk\n   &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 lp__       -2.31e+3 -2.31e+3 9.83e-1 7.26e-1 -2.31e+3 -2.31e+3  1.00    1880.\n 2 p[1]        2.09e-1  2.09e-1 6.60e-3 6.52e-3  1.99e-1  2.20e-1  1.00    4495.\n 3 p[2]        8.10e-2  8.09e-2 4.23e-3 4.26e-3  7.41e-2  8.81e-2  1.00    2125.\n 4 p[3]        7.10e-1  7.10e-1 7.27e-3 7.18e-3  6.98e-1  7.22e-1  1.00    3368.\n 5 q[1]        3.41e-1  3.41e-1 9.80e-3 9.84e-3  3.25e-1  3.57e-1  1.00    4376.\n 6 q[2]        3.39e-2  3.39e-2 1.92e-3 1.92e-3  3.08e-2  3.70e-2  1.00    2338.\n 7 q[3]        1.22e-1  1.21e-1 6.27e-3 6.21e-3  1.11e-1  1.32e-1  1.00    2210.\n 8 q[4]        5.04e-1  5.04e-1 1.03e-2 1.02e-2  4.87e-1  5.21e-1  1.00    3368.\n 9 log_lik    -1.16e+1 -1.13e+1 9.81e-1 7.14e-1 -1.36e+1 -1.07e+1  1.00    1867.\n10 geno_freq…  4.38e-2  4.37e-2 2.76e-3 2.73e-3  3.94e-2  4.85e-2  1.00    4495.\n11 geno_freq…  2.97e-1  2.97e-1 7.10e-3 7.08e-3  2.85e-1  3.08e-1  1.00    4242.\n12 geno_freq…  6.58e-3  6.55e-3 6.87e-4 6.90e-4  5.50e-3  7.76e-3  1.00    2125.\n13 geno_freq…  1.15e-1  1.15e-1 5.59e-3 5.53e-3  1.06e-1  1.24e-1  1.00    2225.\n14 geno_freq…  3.39e-2  3.39e-2 1.92e-3 1.92e-3  3.08e-2  3.70e-2  1.00    2338.\n15 geno_freq…  5.04e-1  5.04e-1 1.03e-2 1.02e-2  4.87e-1  5.21e-1  1.00    3368.\n16 pA          2.09e-1  2.09e-1 6.60e-3 6.52e-3  1.99e-1  2.20e-1  1.00    4495.\n17 pB          8.10e-2  8.09e-2 4.23e-3 4.26e-3  7.41e-2  8.81e-2  1.00    2125.\n18 pO          7.10e-1  7.10e-1 7.27e-3 7.18e-3  6.98e-1  7.22e-1  1.00    3368.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;"
  },
  {
    "objectID": "lectures/lecture-04.html#posterior-summaries-intervals-weak-prior",
    "href": "lectures/lecture-04.html#posterior-summaries-intervals-weak-prior",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Posterior Summaries & Intervals (Weak Prior)",
    "text": "Posterior Summaries & Intervals (Weak Prior)\n\npost_weak &lt;- fit_weak$draws(variables = c(\"p\")) |&gt; as_draws_df()\nweak_summ &lt;- post_weak %&gt;% summarise(\n    mean_pA = mean(`p[1]`), mean_pB = mean(`p[2]`), mean_pO = mean(`p[3]`),\n    sd_pA = sd(`p[1]`), sd_pB = sd(`p[2]`), sd_pO = sd(`p[3]`)\n)\nweak_ci &lt;- post_weak %&gt;% summarise(\n    pA_low = quantile(`p[1]`, 0.025), pA_high = quantile(`p[1]`, 0.975),\n    pB_low = quantile(`p[2]`, 0.025), pB_high = quantile(`p[2]`, 0.975),\n    pO_low = quantile(`p[3]`, 0.025), pO_high = quantile(`p[3]`, 0.975)\n)"
  },
  {
    "objectID": "lectures/lecture-04.html#posterior-histograms-weak-prior",
    "href": "lectures/lecture-04.html#posterior-histograms-weak-prior",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Posterior Histograms (Weak Prior)",
    "text": "Posterior Histograms (Weak Prior)"
  },
  {
    "objectID": "lectures/lecture-04.html#em-vs-bayesian-point-estimates",
    "href": "lectures/lecture-04.html#em-vs-bayesian-point-estimates",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "EM vs Bayesian Point Estimates",
    "text": "EM vs Bayesian Point Estimates\n\n\n# A tibble: 2 × 5\n  method            pA     pB    pO loglik\n  &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 EM             0.209 0.0808 0.710 -2304.\n2 Posterior Mean 0.209 0.0810 0.710    NA"
  },
  {
    "objectID": "lectures/lecture-04.html#consolidated-posterior-comparison",
    "href": "lectures/lecture-04.html#consolidated-posterior-comparison",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Consolidated Posterior Comparison",
    "text": "Consolidated Posterior Comparison"
  },
  {
    "objectID": "lectures/lecture-04.html#population-substructure",
    "href": "lectures/lecture-04.html#population-substructure",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Population Substructure",
    "text": "Population Substructure\n\nFeatures of a population which result from variation of expected allele frequencies across individuals\nStandard allele counting (\\hat{p} = (2n_{AA} + n_{Aa}/2n)) will still be unbiased\nBut, not all subjects may have the same probability of being represented in the sample\nVariance of estimate will be effected"
  },
  {
    "objectID": "lectures/lecture-04.html#population-stratification",
    "href": "lectures/lecture-04.html#population-stratification",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Population Stratification",
    "text": "Population Stratification\n\nIndividuals in a population can be subdivided into mutually exclusive strata\nWithin each strata the allele frequency is the same for all individuals\nIntuitively, we are partitioning a large dataset into multiple smaller datasets"
  },
  {
    "objectID": "lectures/lecture-04.html#population-admixture",
    "href": "lectures/lecture-04.html#population-admixture",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Population Admixture",
    "text": "Population Admixture\n\nWhen individuals in a population have a mixture of different genetic ancestries due to prior mixing of two or more populations\nOften result of migration"
  },
  {
    "objectID": "lectures/lecture-04.html#population-admixture-1",
    "href": "lectures/lecture-04.html#population-admixture-1",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Population Admixture",
    "text": "Population Admixture\n\nFrom Korunes and Goldberg (2021)"
  },
  {
    "objectID": "lectures/lecture-04.html#population-inbreeding",
    "href": "lectures/lecture-04.html#population-inbreeding",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Population Inbreeding",
    "text": "Population Inbreeding\n\nOccurs when there is a preference for mating among relatives in a population or because geographic isolation of subgroups restricts mating choices\nPossibility that an offspring will inherit two copies of the same ancestral allele\nDefine F, the inbreeding coefficient, as the probability that a random individual in the population inherits two copies of the same allele from a common ancestor"
  },
  {
    "objectID": "lectures/lecture-04.html#admixture-as-a-confounder",
    "href": "lectures/lecture-04.html#admixture-as-a-confounder",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Admixture as a Confounder",
    "text": "Admixture as a Confounder\n\nConsider the problem of estimating the effect of a SNP on a disease phenotype: \\beta in P(Y=1) = \\text{logit}^{-1}(\\alpha + \\beta G)\nRecent admixture mixes ancestries within individuals: genotype is a convex combination of source populations\nIf phenotype prevalence differs by ancestry, local or global ancestry proportions act like hidden covariates\nAssociation tests must separate causal signal from ancestry-driven allele frequency differences"
  },
  {
    "objectID": "lectures/lecture-04.html#principal-components-for-structure",
    "href": "lectures/lecture-04.html#principal-components-for-structure",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Principal Components for Structure",
    "text": "Principal Components for Structure\n\nConstruct the standardized genotype matrix Z and compute Z^T Z / M (with M markers)\nTop eigenvectors capture major ancestry gradients\nUse the leading PCs as covariates in association tests or to stratify downstream analyses"
  },
  {
    "objectID": "lectures/lecture-04.html#principal-components-for-structure-1",
    "href": "lectures/lecture-04.html#principal-components-for-structure-1",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Principal Components for Structure",
    "text": "Principal Components for Structure\n\ndata(\"eHGDP\")\neHGDP\n\n/// GENIND OBJECT /////////\n\n // 1,350 individuals; 678 loci; 8,170 alleles; size: 44.1 Mb\n\n // Basic content\n   @tab:  1350 x 8170 matrix of allele counts\n   @loc.n.all: number of alleles per locus (range: 5-35)\n   @loc.fac: locus factor for the 8170 columns of @tab\n   @all.names: list of allele names for each locus\n   @ploidy: ploidy of each individual  (range: 2-2)\n   @type:  codom\n   @call: read.fstat(file = file, missing = missing, quiet = quiet)\n\n // Optional content\n   @pop: population of each individual (group size range: 3-50)\n   @other: a list containing: popInfo"
  },
  {
    "objectID": "lectures/lecture-04.html#principal-components-for-structure-2",
    "href": "lectures/lecture-04.html#principal-components-for-structure-2",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Principal Components for Structure",
    "text": "Principal Components for Structure\n\nhgdp_df &lt;- genind2df(eHGDP, sep = \"/\")\n\n# Convert to allele count matrix, center columns (replace missing with locus means)\ngeno_mat &lt;- scaleGen(eHGDP, center = TRUE, scale = FALSE, NA.method = \"mean\")\npc_fit &lt;- prcomp(geno_mat, center = FALSE, scale. = FALSE)\n\n# Map individuals to geographic regions for coloring\npop_info &lt;- eHGDP@other$popInfo\npop_index &lt;- as.integer(pop(eHGDP))\nregion &lt;- pop_info$Region[pop_index]\n\nplot_df &lt;- data.frame(PC1 = pc_fit$x[, 1], PC2 = pc_fit$x[, 2], Region = region)\nggplot(plot_df, aes(PC1, PC2, color = Region)) +\n    geom_point(alpha = 0.7, size = 1.5) +\n    labs(title = \"Population structure diagnostic\", x = \"PC1\", y = \"PC2\") +\n    theme_minimal()"
  },
  {
    "objectID": "lectures/lecture-04.html#principal-components-for-structure-3",
    "href": "lectures/lecture-04.html#principal-components-for-structure-3",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Principal Components for Structure",
    "text": "Principal Components for Structure"
  },
  {
    "objectID": "lectures/lecture-04.html#structure-admixture",
    "href": "lectures/lecture-04.html#structure-admixture",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "STRUCTURE & ADMIXTURE",
    "text": "STRUCTURE & ADMIXTURE\n\nModel-based clustering methods for population structure inference\nAssume K latent populations with distinct allele frequencies\nEach individual has ancestry proportions \\boldsymbol{\\pi}_i across K populations\nEach genotype is drawn from a mixture of population-specific allele frequencies\nUse maximum likelihood (ADMIXTURE) or Bayesian inference (STRUCTURE) to estimate parameters"
  },
  {
    "objectID": "lectures/lecture-04.html#assumptions",
    "href": "lectures/lecture-04.html#assumptions",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Assumptions",
    "text": "Assumptions\n\nHardy-Weinberg equilibrium within each ancestral population\nLinkage equilibrium between loci within each ancestral population\nLoci are unlinked (or weakly linked)"
  },
  {
    "objectID": "lectures/lecture-04.html#example-output",
    "href": "lectures/lecture-04.html#example-output",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Example Output",
    "text": "Example Output"
  },
  {
    "objectID": "lectures/lecture-04.html#bayesian-admixture",
    "href": "lectures/lecture-04.html#bayesian-admixture",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Bayesian Admixture",
    "text": "Bayesian Admixture\n\nLatent populations k = 1,\\ldots,K possess allele frequencies \\theta_{k\\ell} at locus \\ell\nIndividual ancestry proportions \\boldsymbol{\\pi}_i \\sim \\text{Dirichlet}(\\boldsymbol{\\alpha})\nGenotype y_{i\\ell} \\sim \\text{Binomial}\\left(2, \\sum_{k} \\pi_{ik} \\theta_{k\\ell}\\right) assuming HWE within each ancestral population\nPosterior draws propagate ancestry/allele-frequency uncertainty into association testing, local ancestry, and polygenic prediction"
  },
  {
    "objectID": "lectures/lecture-04.html#identifiability-label-switching",
    "href": "lectures/lecture-04.html#identifiability-label-switching",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Identifiability & Label Switching",
    "text": "Identifiability & Label Switching\n\nProblem: Mixture components are exchangeable → posterior multimodality\nSolution: Anchor loci with informative priors break symmetry\n\nPopulations defined by genetic signatures, not arbitrary labels\nMultiple anchors provide robustness against weak signals"
  },
  {
    "objectID": "lectures/lecture-04.html#stan-model-data-parameters",
    "href": "lectures/lecture-04.html#stan-model-data-parameters",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Stan Model: Data & Parameters",
    "text": "Stan Model: Data & Parameters\ndata {\n  int&lt;lower=1&gt; N;                                // individuals\n  int&lt;lower=1&gt; L;                                // loci\n  int&lt;lower=1&gt; K;                                // ancestral pops\n  array[N, L] int&lt;lower=0, upper=2&gt; y;           // genotypes (0,1,2)\n  int&lt;lower=0&gt; L_soft;\n  array[L_soft] int&lt;lower=1, upper=L&gt; soft_idx;  // e.g., {2}\n  array[K] real&lt;lower=0&gt; a_theta;                // Beta 'a' for non-anchor loci\n  array[K] real&lt;lower=0&gt; b_theta;                // Beta 'b' for non-anchor loci\n  int&lt;lower=1, upper=L&gt; l_star;                  // anchor locus index\n  real&lt;lower=0&gt; conc_pi;                         // shared Dirichlet conc. for pi\n}"
  },
  {
    "objectID": "lectures/lecture-04.html#stan-model-parameters-transformed-parameters",
    "href": "lectures/lecture-04.html#stan-model-parameters-transformed-parameters",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Stan Model: Parameters & Transformed Parameters",
    "text": "Stan Model: Parameters & Transformed Parameters\nparameters {\n  array[N] simplex[K] pi;                        // ancestry proportions\n  ordered[K] eta;                                // ordered logits at anchor locus\n  matrix&lt;lower=0, upper=1&gt;[K, L-1] theta_rest;   // allele freqs for non-anchor loci\n}\n\n\ntransformed parameters {\n  matrix&lt;lower=0, upper=1&gt;[K, L] theta;          // full allele-frequency matrix\n  matrix[N, L] p_mix;                            // mixed allele frequency\n\n  // anchor column (ordered)\n  for (k in 1:K) theta[k, l_star] = inv_logit(eta[k]);\n\n  // fill remaining columns\n  {\n    int c = 1;\n    for (l in 1:L) {\n      if (l == l_star) continue;\n      for (k in 1:K) theta[k, l] = theta_rest[k, c];\n      c += 1;\n    }\n  }\n\n  // mixture expectations\n  for (n in 1:N)\n    for (l in 1:L)\n      p_mix[n, l] = dot_product(pi[n], col(theta, l));\n}"
  },
  {
    "objectID": "lectures/lecture-04.html#stan-model-priors-likelihood",
    "href": "lectures/lecture-04.html#stan-model-priors-likelihood",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Stan Model: Priors & Likelihood",
    "text": "Stan Model: Priors & Likelihood\nmodel {\n  // priors\n  eta ~ normal(0, 2.5);                          // weak prior; ordering gives ID\n  for (k in 1:K)\n    for (c in 1:(L - 1))\n      theta_rest[k, c] ~ beta(a_theta[k], b_theta[k]);\n\n  for (n in 1:N)\n    pi[n] ~ dirichlet(rep_vector(conc_pi, K));\n\n  for (c in 1:L_soft) {\n  int l = soft_idx[c];\n  if (l != l_star) {\n    // comp 1 LOW, comp 2 HIGH at these loci (gentle)\n    target += beta_lpdf(theta[1, l] | 2, 8);\n    target += beta_lpdf(theta[2, l] | 8, 2);\n  }\n}\n\n  // likelihood\n  for (n in 1:N)\n    for (l in 1:L)\n      y[n, l] ~ binomial(2, p_mix[n, l]);\n}"
  },
  {
    "objectID": "lectures/lecture-04.html#stan-model-generated-quantities",
    "href": "lectures/lecture-04.html#stan-model-generated-quantities",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Stan Model: Generated Quantities",
    "text": "Stan Model: Generated Quantities\ngenerated quantities {\n  matrix[N, K] logit_pi;\n  array[N, L] int y_rep;\n\n  for (n in 1:N)\n    for (k in 1:K)\n      logit_pi[n, k] = logit(pi[n, k]);\n\n  for (n in 1:N)\n    for (l in 1:L)\n      y_rep[n, l] = binomial_rng(2, p_mix[n, l]);\n}"
  },
  {
    "objectID": "lectures/lecture-04.html#mathematical-framework",
    "href": "lectures/lecture-04.html#mathematical-framework",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Mathematical Framework",
    "text": "Mathematical Framework\nIndividual-Locus Allele Frequency\nFor individual i at locus \\ell, expected allele frequency: p_{i\\ell} = \\sum_{k=1}^K \\pi_{ik} \\theta_{k\\ell}\nInformation Borrowing Across Loci\nKey insight: Same \\boldsymbol{\\pi}_i parameters appear in likelihood for all loci\nL(\\boldsymbol{\\pi}_i, \\boldsymbol{\\Theta}) = \\prod_{\\ell=1}^L \\text{Binomial}\\left(y_{i\\ell} \\mid 2, \\sum_{k=1}^K \\pi_{ik} \\theta_{k\\ell}\\right)"
  },
  {
    "objectID": "lectures/lecture-04.html#mathematical-framework-1",
    "href": "lectures/lecture-04.html#mathematical-framework-1",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Mathematical Framework",
    "text": "Mathematical Framework\nHierarchical Learning\n\nAnchor loci provide strong identification signal\nRemaining loci contribute cumulative evidence\nPosterior uncertainty propagates through all parameters"
  },
  {
    "objectID": "lectures/lecture-04.html#simulating-admixed-genotypes",
    "href": "lectures/lecture-04.html#simulating-admixed-genotypes",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Simulating Admixed Genotypes",
    "text": "Simulating Admixed Genotypes\n\n1set.seed(887804)\n\n2rdirichlet &lt;- function(n, alpha) {\n    k &lt;- length(alpha)\n    gamma_draws &lt;- matrix(rgamma(n * k, shape = alpha, rate = 1),\n                          ncol = k, byrow = TRUE)\n    sweep(gamma_draws, 1, rowSums(gamma_draws), \"/\")\n}\n\n3N &lt;- 40   # individuals\nL &lt;- 20   # loci\nK &lt;- 2    # ancestral populations\n\n\n1\n\nFixed random seed for consistent results across sessions\n\n2\n\nNormalized gamma variates generate simplex-constrained ancestry proportions\n\n3\n\nSet simulation parameters"
  },
  {
    "objectID": "lectures/lecture-04.html#simulating-admixed-genotypes-1",
    "href": "lectures/lecture-04.html#simulating-admixed-genotypes-1",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Simulating Admixed Genotypes",
    "text": "Simulating Admixed Genotypes\n\n4# Create very clear population differentiation\ntheta_true &lt;- matrix(0, nrow = K, ncol = L)\n\n# Multiple anchor loci: very strong differentiation\ntheta_true[1, 1] &lt;- 0.1  # Pop 1: very low frequency at anchor 1\ntheta_true[2, 1] &lt;- 0.9  # Pop 2: very high frequency at anchor 1\ntheta_true[1, 2] &lt;- 0.1  # Pop 1: very low frequency at anchor 2\ntheta_true[2, 2] &lt;- 0.9  # Pop 2: very high frequency at anchor 2\n\n# Other loci: strong differentiation \ntheta_true[1, 3:L] &lt;- rbeta(L-2, 2, 8)   # Pop 1: much lower overall\ntheta_true[2, 3:L] &lt;- rbeta(L-2, 8, 2)   # Pop 2: much higher overall\n\n\n5pi_true &lt;- rbind(\n  rdirichlet(12, c(90, 10)),  # pop 1\n  rdirichlet(12, c(10, 90)),  # Very pure pop 2\n  rdirichlet(16, c(10, 10))                                    # Admixed individuals\n)\n\n\n4\n\nStrong genetic signatures with multiple anchor loci\n\n5\n\nMix of unadmixed founders and admixed descendants"
  },
  {
    "objectID": "lectures/lecture-04.html#simulating-admixed-genotypes-2",
    "href": "lectures/lecture-04.html#simulating-admixed-genotypes-2",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Simulating Admixed Genotypes",
    "text": "Simulating Admixed Genotypes\n\n6y &lt;- matrix(0L, nrow = N, ncol = L)\nfor (n in 1:N) {\n    for (l in 1:L) {\n        p_mix &lt;- sum(pi_true[n, ] * theta_true[, l])\n        y[n, l] &lt;- rbinom(1, size = 2, prob = p_mix)\n    }\n}\n\n\n6\n\nGenotypes drawn from Binomial(2, p_mix) per individual and locus"
  },
  {
    "objectID": "lectures/lecture-04.html#pca-on-simulated-genotypes",
    "href": "lectures/lecture-04.html#pca-on-simulated-genotypes",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "PCA On Simulated Genotypes",
    "text": "PCA On Simulated Genotypes\n\nCenter genotypes by 2 \\times \\hat{p} and scale by \\sqrt{2 \\cdot \\hat{p} \\cdot (1 - \\hat{p})}\n\n\np_hat &lt;- colMeans(y) / 2\nsd_hat &lt;- sqrt(pmax(1e-6, 2 * p_hat * (1 - p_hat)))\nZ &lt;- scale(y, center = 2 * p_hat, scale = sd_hat)\npc &lt;- prcomp(Z, center = FALSE, scale. = FALSE)\n\npc_df &lt;- data.frame(PC1 = pc$x[, 1], PC2 = pc$x[, 2], pi1 = pi_true[, 1])\nggplot(pc_df, aes(PC1, PC2, color = pi1)) +\n  geom_point(alpha = 0.7, size = 1.6) +\n  scale_color_viridis_c(end = .8) +\n  labs(color = expression(pi[1]), x = \"PC1\", y = \"PC2\") +\n  geom_vline(xintercept = 2.75, linetype = \"dashed\", color = \"gray40\", linewidth = 0.6) +\n  geom_vline(xintercept = -2.75, linetype = \"dashed\", color = \"gray40\", linewidth = 0.6) +\n  annotate(\"text\", x = 4.5, y = 6, label = \"Pop 1\", color = \"gray20\", size = 6) +\n  annotate(\"text\", x = -4.5, y = 6, label = \"Pop 2\", color = \"gray20\", size = 6) +\n  annotate(\"text\", x = 0, y = 6, label = \"Admixed\", color = \"gray20\", size = 6) +\n  theme_bw(base_size = 14) +\n  theme(panel.grid = element_blank())"
  },
  {
    "objectID": "lectures/lecture-04.html#pca-on-simulated-genotypes-1",
    "href": "lectures/lecture-04.html#pca-on-simulated-genotypes-1",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "PCA On Simulated Genotypes",
    "text": "PCA On Simulated Genotypes"
  },
  {
    "objectID": "lectures/lecture-04.html#fitting-the-model",
    "href": "lectures/lecture-04.html#fitting-the-model",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Fitting the Model",
    "text": "Fitting the Model\n\nadmix_mod &lt;- cmdstan_model(file.path(\"stan\", \"structure_admixture.stan\"))\n\nl_star &lt;- 1  # choose your anchor locus (e.g., 1)\n\nadmix_data &lt;- list(\n    N = N, L = L, K = K, y = y,\n    a_theta = rep(1.5, K), b_theta = rep(1.5, K),\n    l_star = l_star, L_soft = 1, soft_idx = c(2),\n    conc_pi = 2.0      \n)\n\nadmix_fit &lt;- admix_mod$sample(\n  data = admix_data,\n  chains = 4, parallel_chains = 4,\n  iter_warmup = 2000, iter_sampling = 2000,\n  seed = 887804, refresh = 0\n)\n\nRunning MCMC with 4 parallel chains...\n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: binomial_lpmf: Probability parameter is 1, but must be in the interval [0, 1] (in '/var/folders/3f/7lk7ddbn19j4f1rzxtlx3z9w0000gn/T/RtmpNcnZ0F/model-176a12d00be28.stan', line 65, column 6 to column 41)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: binomial_lpmf: Probability parameter is 1, but must be in the interval [0, 1] (in '/var/folders/3f/7lk7ddbn19j4f1rzxtlx3z9w0000gn/T/RtmpNcnZ0F/model-176a12d00be28.stan', line 65, column 6 to column 41)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 2 finished in 14.6 seconds.\nChain 4 finished in 14.6 seconds.\nChain 1 finished in 14.8 seconds.\nChain 3 finished in 15.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 14.9 seconds.\nTotal execution time: 15.5 seconds."
  },
  {
    "objectID": "lectures/lecture-04.html#stan-output",
    "href": "lectures/lecture-04.html#stan-output",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Stan Output",
    "text": "Stan Output\n\ndm &lt;- admix_fit$draws(variables = c(\"theta\",\"pi\"), format = \"draws_df\")\n\nlstar &lt;- admix_data$l_star\nL &lt;- ncol(y)\nnon_anchor &lt;- setdiff(1:L, lstar)\n\n# Orientation score per draw: average sign over non-anchor loci\nsign_per_draw &lt;- rowMeans(sapply(non_anchor, function(l)\n  sign(dm[[sprintf(\"theta[2,%d]\", l)]] - dm[[sprintf(\"theta[1,%d]\", l)]])))\n\n# Reference orientation: the majority sign across all draws\nref_sign &lt;- ifelse(mean(sign_per_draw) &gt;= 0, -1, 1)\n\n# Decide which DRAWS to flip (not just which chains)\nflip_draw &lt;- sign_per_draw * ref_sign &lt; 0\n\n# Helper: swap theta rows and pi columns for those draws\nswap_block &lt;- function(df, pat1, pat2, idx){\n  i1 &lt;- grep(pat1, names(df)); i2 &lt;- grep(pat2, names(df))\n  tmp &lt;- df[idx, i1, drop=FALSE]\n  df[idx, i1] &lt;- df[idx, i2, drop=FALSE]\n  df[idx, i2] &lt;- tmp\n  df\n}\n\n# Swap theta rows\ndm &lt;- swap_block(dm, \"^theta\\\\[1,\", \"^theta\\\\[2,\", flip_draw)\n# Swap pi columns\ndm &lt;- swap_block(dm, \"^pi\\\\[[0-9]+,1\\\\]$\", \"^pi\\\\[[0-9]+,2\\\\]$\", flip_draw)"
  },
  {
    "objectID": "lectures/lecture-04.html#aligned-posteriors",
    "href": "lectures/lecture-04.html#aligned-posteriors",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Aligned Posteriors",
    "text": "Aligned Posteriors\n\n# Plot aligned posteriors with true values\ntrue_line &lt;- function(pars, truths) data.frame(variable = pars, truth = truths)\n\nmcmc_hist(dm, pars = c(\"pi[1,1]\")) + \n  geom_vline(data = true_line(c(\"pi[1,1]\"), c(pi_true[1,1])),\n             aes(xintercept = truth), linetype = \"dashed\", colour = \"red\") +\nxlim(0, 1)"
  },
  {
    "objectID": "lectures/lecture-04.html#credible-intervals",
    "href": "lectures/lecture-04.html#credible-intervals",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Credible Intervals",
    "text": "Credible Intervals\n\nmcmc_intervals(dm, pars = c(\"pi[1,1]\", \"pi[3,1]\", \"pi[14,1]\", \"pi[38,1]\"))"
  },
  {
    "objectID": "lectures/lecture-04.html#summary-next-steps",
    "href": "lectures/lecture-04.html#summary-next-steps",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Summary & Next Steps",
    "text": "Summary & Next Steps\n\nPopulation structure influences genetic analyses\nBayesian framing exposes prior choices, enables posterior uncertainty on structure (ABO example, admixture Stan model)\nBayesian modeling as attempting to capture the data-generating process"
  },
  {
    "objectID": "lectures/lecture-04.html#reference",
    "href": "lectures/lecture-04.html#reference",
    "title": "Lecture 04: Population Structure and Bayesian Methods in Statistical Genetics",
    "section": "Reference",
    "text": "Reference\n\n\n\n\nCarpenter,B. et al. (2017) Stan: A probabilistic programming language. Journal of Statistical Software, 76, 1–32.\n\n\nGelman,A. et al. (2013) Bayesian data analysis 3rd ed. Chapman; Hall/CRC, Boca Raton, FL.\n\n\nKorunes,K.L. and Goldberg,A. (2021) Human genetic admixture. PLOS Genetics, 17, e1009374.\n\n\nMcElreath,R. (2020) Statistical rethinking: A bayesian course with examples in r and stan 2nd ed. Chapman; Hall/CRC, Boca Raton, FL.\n\n\nMourant,A.E. et al. (1976) The distribution of the human blood groups and other polymorphisms 2nd ed. Oxford University Press, London.\n\n\nYamamoto,F. et al. (2012) ABO research in the modern era of genomics. Transfusion Medicine Reviews, 26, 103–118."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This graduate-level course builds a cohesive toolkit for analyzing complex genetic data, weaving together Mendelian and population-genetic principles, likelihood theory, and Bayesian inference with MCMC. Lectures progress from pedigree linkage and genome-wide association study (GWAS) design to population-structure correction, multiple-testing control, and genomic prediction using BLUP, penalized regressions, and non-parametric learners such as random forests and neural networks. Binary-trait modelling introduces logistic mixed models, AUC evaluation, and family-based tests, while hands-on R/Bioconductor labs guide students from Hardy–Weinberg simulations to full GWAS and polygenic-score pipelines. Weekly problem sets blend mathematical derivations with coding; a capstone project requires analyzing public whole-genome or single-cell data and presenting findings in a conference-style talk. By course end, participants can translate biological questions into formal statistical models, implement inference algorithms on high-dimensional data, control error rates in large-scale studies, and critically evaluate predictive models and their uncertainties.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "This graduate-level course builds a cohesive toolkit for analyzing complex genetic data, weaving together Mendelian and population-genetic principles, likelihood theory, and Bayesian inference with MCMC. Lectures progress from pedigree linkage and genome-wide association study (GWAS) design to population-structure correction, multiple-testing control, and genomic prediction using BLUP, penalized regressions, and non-parametric learners such as random forests and neural networks. Binary-trait modelling introduces logistic mixed models, AUC evaluation, and family-based tests, while hands-on R/Bioconductor labs guide students from Hardy–Weinberg simulations to full GWAS and polygenic-score pipelines. Weekly problem sets blend mathematical derivations with coding; a capstone project requires analyzing public whole-genome or single-cell data and presenting findings in a conference-style talk. By course end, participants can translate biological questions into formal statistical models, implement inference algorithms on high-dimensional data, control error rates in large-scale studies, and critically evaluate predictive models and their uncertainties.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-prerequisites",
    "href": "syllabus.html#course-prerequisites",
    "title": "Syllabus",
    "section": "Course Prerequisites",
    "text": "Course Prerequisites\n\nPUBH 6860: Principles of Bioinformatics",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-learning-objectives",
    "href": "syllabus.html#course-learning-objectives",
    "title": "Syllabus",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\n\nAnalyze Mendelian, population-genetic, and demographic models to quantify inheritance patterns, linkage, association, and population structure in diverse organisms.\nApply likelihood, Bayesian, and Markov-chain Monte Carlo techniques to estimate parameters and test hypotheses in genome-scale datasets.\nEvaluate genome-wide association studies and genomic-prediction pipelines, controlling false-discovery rates and assessing prediction bias, variance, and uncertainty.\nSynthesize multi-source genomic, phenotypic, and environmental data into reproducible R/Bioconductor workflows that meet FAIR and open-science standards.\nDesign and implement statistical learning models-shrinkage regressions, mixed models, and non-parametric methods-to predict complex traits and interpret model performance in a biological context.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#textbooks",
    "href": "syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\n\nRequired\n\nStatistical Learning in Genetics: An Introduction Using R, Daniel Sorensen, 2nd Edition (Available via the GWU Library)\n\n\n\nRecommended\n\nHandbook of Statistical Genomics, David J. Balding, Ida Moltke, John Marioni, 4th Edition (Available via the GWU Library)\nThe Fundamentals of Modern Statistical Genetics, Nan M. Laird and Christoph Lange, 1st Edition (Available via the GWU Library)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#technology-requirements",
    "href": "syllabus.html#technology-requirements",
    "title": "Syllabus",
    "section": "Technology Requirements",
    "text": "Technology Requirements\nStudents should have a desktop or laptop (Windows, macOS, or Linux) with at least 8 GB RAM, 20 GB free disk space, a reliable broadband connection (≥ 10 Mbps), and working webcam, microphone, and speakers or headphones for synchronous Zoom sessions. They must be able to navigate the university’s LMS through a modern web browser to download readings, submit assignments, and join discussion boards; install and update R (4.5 or newer), RStudio (or VS Code with the R extension), Bioconductor packages, Git, and Zoom; and use basic Git commands (clone, commit, push) to submit version-controlled lab work. We will use Zotero for file sharing and collaboration. Familiarity with screen sharing, breakout rooms, captioning, and recording in Zoom is expected. Optional but recommended tools include an SSH client or VPN for connecting to campus HPC resources and a PDF reader that supports annotation. All course materials follow WCAG 2.1 guidelines, and RStudio offers high-contrast and screen-reader modes; students who require further accommodations should contact Disability Services before the first week.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assignments-and-descriptions",
    "href": "syllabus.html#assignments-and-descriptions",
    "title": "Syllabus",
    "section": "Assignments and Descriptions",
    "text": "Assignments and Descriptions\n\n\n\nAssignment Type\n% of total grade\n\n\n\n\nProblem Sets\n60\n\n\nClass participation (defined below)\n20\n\n\nResearch Project (Final Exam)\n20\n\n\n\n\nStandard SPH Graduate Grading Scale\n\nA: 94-100%\nA-: 90-93%\nB+: 87-89%\nB: 84-86%\nB-: 80-83%\nC+: 77-79%\nC: 73-76%\nC-: 70-72%\nF: Below 70%\n\n\n\nProblem Sets\nProblem sets are key to exploring the concepts introduced both in class and in the textbook. These are meant to provide an opportunity to more deeply understand concepts and put them into practice and provide an opportunity for data manipulation. This is part of the ‘lab’ component of the course and you will be given time in class to work on assignments and collaborate. However, these problem sets will take substantial time outside of class to complete, so please plan accordingly.\n\n\nResearch Project\nStudents will choose a unique project to work on for the final third of the semester. At the end of the semester, students will submit a written project report in the form of a scientific research paper. Students are strongly encouraged to work in groups of up to three on their projects, however such groups will be expected to make proportionally more substantial contributions with clearly delineated responsibilities for each member’s contributions. Projects should be based on a research topic related to statistical genetics, but can be computational, methodological, or applied in nature. The Lab write-ups will follow the standard form of a scientific paper to gain experience in writing. Specifically, we will follow the format of the journal Genetics, the leading journal in the field. The paper MUST BE BASED ON THE PRIMARY LITERATURE IN THE APPROPRIATE REFEREED SCIENTIFIC JOURNALS, and it should adhere to the following format:\n\nBegin the paper with an original title, followed by your name, the course, and the date. All papers should be typed, single-spaced, and in 12 pt. font.\nThe paper should have the following sections:\n\nIntroduction – here you state the general problem or issue you are addressing.\nMaterials and Methods – describe the methods used to obtain data, analyze data, and test hypotheses associated with the data.\nResults – describe the results of the data analysis and hypothesis testing.\nDiscussion – here you draw conclusions about the problem you studied; this section should include a synthesis of ideas.\nLiterature Cited – List the relevant literature you have read and used to support your arguments/analyze your data. The literature cited should be in the format of the journal Genetics.\n\n\nAspects of the project will be required throughout the course with a final research project submitted in the form of a research paper during finals. See course outline for due dates for each part of project.\n\n\nWorkload\nIn this course, you will be expected to spend 5 or more hours per week in independent learning which can include reviewing assigned material, preparing for class discussions, working on assignments, and group work. In addition, 1.5 hours per week will be spent in class computational lab and 4.5 hours working on asynchronous materials provided online. The total workload for this course will be at least 112.5 hours.\n\n\nClass Policy: Statistical Genetics is Interdisciplinary and Quantitative\nThis course is highly interdisciplinary, quantitative, and technical. We will discuss algorithms, concepts, and methods from biology, computer science, and statistics. You will be asked to learn about and apply technical concepts in areas that you may have limited familiarity with. In our experience, to succeed in this class, you will have to commit to repeatedly engaging with concepts to build understanding and refine your understanding through class discussions, outside reading, and homework assignments. Willingness to think quantitatively/probabilistically is required to succeed in this course. There are a diversity of talent sets in this class and you each bring something unique to the table. Collaborate and find someone with the skills they might share if you are lacking in a particular area. Building effective teams with broad skillsets is a hallmark of effective statistical genetics.\n\n\nClass Policy: Participation and Discussion\nTeaching and learning require a team effort. We expect you to show up to synchronous sessions (on time) and be prepared for discussions. This means, you have gone through the asynchronous material and started on the homework assignment (problem sets) and come to the synchronous session with questions. You are strongly encouraged to ask questions during synchronous session to help complete your homework assignments and share thoughts and progress on a research project. Statistical genetics is an exciting and broad area with no shortage of ethical and societal implications. We welcome your points of view and respectful discussion. We also strongly encourage cooperation among students to help in each other’s understanding of the material, but homework assignments must be your own work. We would greatly appreciate any feedback on any aspects of this course, both positive and negative!\n20% of your grade is ‘Participation’. This is both a quantitative assessment of your responses to asynchronous materials as well as participation in the live synchronous sessions. Statistical Genetics is a demanding discipline that requires students to think critically and utilize high-level analytical skills regarding complex issues. The discipline requires such mastery not only in well-articulated written work, but also in thoughtful discussions between and among students and instructors. Receiving full points for participation is not simply a matter of showing up and turning work in on time. Outstanding participation grades require truly thoughtful, insightful, and well-argued contributions and leadership in class and in asynchronous prompts that demonstrate a high level of mastery of the course material.\n\n\nClass Policy: Late Work\nLate work will be accepted but with a 1% deduction per hour for the first 5 hours up to 5% deduction per day for unexcused late homework submission. All homework will be due at 11:59 pm on the designated due date unless otherwise specified. Homework assignments (Problem Sets) will typically be distributed via blackboard with a week to complete each assignment.\n\n\nClass Policy: Make-up Work/Make-up Exams\nAny student who experiences significant family or personal illness or emergency after the final withdrawal date and is unable to complete course work should ask the instructor for an incomplete for the course. Each case will be managed on an individual basis. The Incomplete Policy must be followed as outlined in the GWSPH Student Handbook.\n\n\nClass Policy: Generative Artificial Intelligence (GAI)\nStudents are permitted to use GAI tools to generate outlines of scripts and papers to help get started, but we expect heavy editing (and commenting in code) subsequently done by the student to verify functionality and understanding of code and implications of research results. Students are permitted to use GAI for coding assignments to get started but must further comment, edit, and validate their code. Any use of GAI must be acknowledged in each assignment with details on the distinction between the GAI material and the student contribution for any given assignment along with details of the prompts supplied to the GAI tool, acknowledgement of the GAI tool including tool name and version. Note also the GW University Policy on GAI.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Office hours are held on Friday’s from 10am-12pm. You can book a 30 minute slot using this link. If you need to meet outside of these hours, please email me at chiraaggohel@gwu.edu.",
    "crumbs": [
      "FAQ"
    ]
  },
  {
    "objectID": "faq.html#office-hours",
    "href": "faq.html#office-hours",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Office hours are held on Friday’s from 10am-12pm. You can book a 30 minute slot using this link. If you need to meet outside of these hours, please email me at chiraaggohel@gwu.edu.",
    "crumbs": [
      "FAQ"
    ]
  },
  {
    "objectID": "faq.html#course-location",
    "href": "faq.html#course-location",
    "title": "Frequently Asked Questions",
    "section": "Course Location",
    "text": "Course Location\nMilken SPH, 300C. The zoom link is available on Blackboard, and the Google Calendar invite.",
    "crumbs": [
      "FAQ"
    ]
  },
  {
    "objectID": "faq.html#the-required-paper-is-not-open-source-how-do-i-access-it",
    "href": "faq.html#the-required-paper-is-not-open-source-how-do-i-access-it",
    "title": "Frequently Asked Questions",
    "section": "The required paper is not open source, how do I access it?",
    "text": "The required paper is not open source, how do I access it?\nVia our shared Zotero library. If you don’t have access, please email chiraaggohel@gwu.edu to be added.",
    "crumbs": [
      "FAQ"
    ]
  },
  {
    "objectID": "assignments/assignment-02.html",
    "href": "assignments/assignment-02.html",
    "title": "Assignment 02",
    "section": "",
    "text": "Due before class on Wednesday, September 10th.\nRequirements:"
  },
  {
    "objectID": "assignments/assignment-02.html#problem-1-missing-heritability-and-rare-variants-40-pts",
    "href": "assignments/assignment-02.html#problem-1-missing-heritability-and-rare-variants-40-pts",
    "title": "Assignment 02",
    "section": "Problem 1: Missing Heritability and Rare Variants (40 pts)",
    "text": "Problem 1: Missing Heritability and Rare Variants (40 pts)\nRecall (Young 2019): “The first challenge is one of precision. The information used to estimate heritability from rare variants by GREML-WGS comes from the variation in sharing of rare variants among distantly related pairs of individuals. However, distantly related individuals typically do not share any particular rare variant, so the variation in rare variant sharing is low. This means that large samples with high quality WGS data are required to obtain precise estimates, and such samples are not common yet. Based on the only existing application of GREML-WGS, a sample size of ~40,000 would produce estimates precise enough to be statistically distinguished from other heritability estimates. It is likely that this challenge will be overcome shortly, since samples of similar magnitude already exist.”\n\nAssume the probability that two distantly related individuals share a rare variant is p=0.001. Assume a sample size of n=40,000 individuals.\n\nCalculate the expected number of pairs of individuals in this sample who share a rare variant. How many total pairs of individuals exist in the sample?\nIf we estimate that rare variants contribute h^2_{\\text{rare}}=0.10 to heritability, calculate the standard error of this estimate given the sample size. Use the formula \\text{SE}(h^2) \\approx \\frac{2}{\\sqrt{n_{\\text{eff}}}}, where n_{\\text{eff}} is the effective number of independent observations (approximately the number of pairs sharing rare variants).\nCalculate a 95% confidence interval for the heritability estimate. Does this confidence interval allow us to distinguish between h^2_{\\text{rare}}=0.10 and h^2_{\\text{common}}=0.25\n\nBriefly explain in 2-3 sentences how the “missing heritability” problem relates to rare variants, and why larger samples with whole-genome sequencing may be needed to resolve this question."
  },
  {
    "objectID": "assignments/assignment-02.html#problem-2-parentoffspring-regression-with-assortative-mating-40-pts",
    "href": "assignments/assignment-02.html#problem-2-parentoffspring-regression-with-assortative-mating-40-pts",
    "title": "Assignment 02",
    "section": "Problem 2: Parent–offspring regression with assortative mating (40 pts)",
    "text": "Problem 2: Parent–offspring regression with assortative mating (40 pts)\nLet Y=A+E with \\operatorname{Var}(A)=\\sigma_A^2, \\operatorname{Var}(E)=\\sigma_E^2, random environments, and phenotypic mate correlation \\operatorname{corr}(Y_{\\text{father}},Y_{\\text{mother}})=r_m.\n\nShow that \\operatorname{Var}(Y_{\\text{mid-parent}})=\\frac{1}{2}(1+r_m)\\operatorname{Var}(Y).\nGiven the regression slope \\beta=\\frac{\\operatorname{Cov}(Y_o,Y_{mp})}{\\operatorname{Var}(Y_{mp})}, show that \\beta \\;=\\; \\frac{\\sigma_A^2}{\\sigma_A^2+\\sigma_E^2}\\cdot\\frac{1}{1+r_m} \\;=\\; \\frac{h^2}{1+r_m}. Interpret the direction of bias in \\beta for r_m&gt;0.\nFor h^2=0.5, compute \\beta for r_m=0,0.1,0.3,0.5. Comment on the practical impact of assortative mating on parent–offspring regression."
  },
  {
    "objectID": "assignments/assignment-02.html#problem-3-derivation-of-falconers-formula-from-the-ace-model-20-pts",
    "href": "assignments/assignment-02.html#problem-3-derivation-of-falconers-formula-from-the-ace-model-20-pts",
    "title": "Assignment 02",
    "section": "Problem 3: Derivation of Falconer’s Formula from the ACE Model (20 pts)",
    "text": "Problem 3: Derivation of Falconer’s Formula from the ACE Model (20 pts)\nBackground: The ACE model is a foundational tool in quantitative genetics for partitioning phenotypic variance (V_P) into three components: additive genetic effects (A), common or shared environmental effects (C), and unique or non-shared environmental effects (E). Under this model, the total variance is given by V_P = V_A + V_C + V_E.\nThe intraclass correlations for a trait between monozygotic (MZ) and dizygotic (DZ) twins are given by:\nr_{\\text{MZ}} = \\frac{\\operatorname{Cov}(Y_1, Y_2 \\mid \\text{MZ})}{V_P}\nr_{\\text{DZ}} = \\frac{\\operatorname{Cov}(Y_1, Y_2 \\mid \\text{DZ})}{V_P}\nAssume that:\n\nMating is random (no assortative mating).\nThere are no gene-environment interactions or correlations.\nThe equal environments assumption holds (MZ and DZ twins experience their shared environments to a similar degree).\nGenetic effects are purely additive (no dominance or epistasis).\n\nAssuming the ACE model, demonstrate that the narrow-sense heritability (h^2 = V_A/V_P) can be estimated as twice the difference between the MZ and DZ twin correlations.\nShow that:\nh^2 = 2(r_{\\text{MZ}} - r_{\\text{DZ}})"
  },
  {
    "objectID": "assignments/assignment-04.html",
    "href": "assignments/assignment-04.html",
    "title": "Assignment 04",
    "section": "",
    "text": "Requirements\nA helpful vignette on using cmdstanr is available at https://mc-stan.org/cmdstanr/articles/cmdstanr.html."
  },
  {
    "objectID": "assignments/assignment-04.html#problem-1-population-substructure-and-allele-frequency-estimation-30-pts",
    "href": "assignments/assignment-04.html#problem-1-population-substructure-and-allele-frequency-estimation-30-pts",
    "title": "Assignment 04",
    "section": "Problem 1: Population Substructure and Allele Frequency Estimation (30 pts)",
    "text": "Problem 1: Population Substructure and Allele Frequency Estimation (30 pts)\nLet there be n diploid individuals sampled at random from a population made of subpopulations k=1,\\dots,K. In subpopulation k, the allele A frequency is p_k, and let w_k be the fraction of sampled individuals from subpopulation k (\\sum_k w_k=1). The “global” allele frequency we want to estimate is the mixture average \np \\;=\\; \\sum_k w_k p_k.\n\nLet n_{AA}, n_{Aa}, n_{aa} be the counts of genotypes AA, Aa, aa in the sample of size n. The standard estimator of the allele frequency is \n\\hat{p} \\;=\\; \\frac{2n_{AA} + n_{Aa}}{2n}.\n\n(a) Show that in the presence of population substructure, \\hat{p} is unbiased.\nHint: Let D_i be the A-dosage for individual i (D_i\\in\\{0,1,2\\}). Under HWE within k, D_i\\mid Z_i=k\\sim \\mathrm{Bin}(2,p_k). Use LOTUS: \\mathbb E[D_i]=\\sum_k w_k\\mathbb E[D_i\\mid Z_i=k].\n(b) What is \\mathrm{Var}(\\hat{p}) under random mixture sampling (i.i.d. individuals from the mixture)? Assume each subpopulation is in HWE. Compare to the case with no substructure (K=1).\nHint: Use the law of total variance on D_i:\n\\mathrm{Var}(D_i)=\\mathbb E\\{\\mathrm{Var}(D_i\\mid Z)\\}+\\mathrm{Var}\\{\\mathbb E(D_i\\mid Z)\\}, with \\mathrm{Var}(D_i\\mid Z=k)=2p_k(1-p_k) and \\mathbb E(D_i\\mid Z=k)=2p_k.\nDefine\n\\begin{gather}\n\\bar p=\\sum_k w_k p_k \\\\\n\\mathrm{Var}_w(p_k)=\\sum_k w_k(p_k-\\bar p)^2,\n\\end{gather}\nand note \\mathbb E[p_k(1-p_k)]=\\bar p(1-\\bar p)-\\mathrm{Var}_w(p_k).\n(c) Now consider a stratified sample: take exactly n_k individuals from subpopulation k (\\sum_k n_k=n; write w_k=n_k/n). Maintain HWE within subpopulations. Derive \\mathrm{Var}(\\hat p) under this design and compare it to your answer in (b). State which design yields the larger variance, and by how much, in terms of \\mathrm{Var}_w(p_k).\nHint: Write \\hat p=\\sum_k w_k \\hat p_k with \\hat p_k=\\frac{1}{2n_k}\\sum_{i:Z_i=k} D_i. Use independence across strata and \\mathrm{Var}(\\hat p_k)=\\frac{p_k(1-p_k)}{2n_k}."
  },
  {
    "objectID": "assignments/assignment-04.html#problem-2-population-substructure-ld-and-association-testing-40-pts",
    "href": "assignments/assignment-04.html#problem-2-population-substructure-ld-and-association-testing-40-pts",
    "title": "Assignment 04",
    "section": "Problem 2: Population Substructure, LD, and Association Testing (40 pts)",
    "text": "Problem 2: Population Substructure, LD, and Association Testing (40 pts)\nLet X be the genotype dosage (0/1/2 copies of the effect allele) at a tag SNP and X_c at a causal SNP. The causal SNP has effect size \\beta_c on quantitative trait Y. The observed effect from simple regression of Y on X is \\beta_{\\text{obs}}.\nConvenience (scaling): Work with standardized genotypes \n\\tilde X=\\frac{X-\\mathbb E[X]}{\\sqrt{\\mathrm{Var}(X)}},\\qquad\n\\tilde X_c=\\frac{X_c-\\mathbb E[X_c]}{\\sqrt{\\mathrm{Var}(X_c)}}.\n With this scaling, \\beta_{\\text{obs}}=r\\,\\beta_c exactly, where r=\\mathrm{Corr}(\\tilde X,\\tilde X_c)=\\mathrm{Corr}(X,X_c).\nAssume individuals are sampled i.i.d. from a mixture with \\Pr(Z=k)=w_k, \\sum_k w_k=1. Within each subpopulation k: - HWE holds at each locus, - LE (no within-k LD) holds between X and X_c.\nDefine\n\\begin{gather}\n\\bar p=\\sum_k w_k p_k,\\quad \\bar p_c=\\sum_k w_k p_{c,k} \\\\\n\\mathrm{Var}_w(p_k)=\\sum_k w_k(p_k-\\bar p)^2 \\\\\n\\mathrm{Cov}_w(p_k,p_{c,k})=\\sum_k w_k (p_k-\\bar p)(p_{c,k}-\\bar p_c)\n\\end{gather}\n\nPart A (10 pts): Correlation induced by population structure\nLet the allele frequencies at the tag and causal SNPs be p_k and p_{c,k} in subpopulation k. Show that \nr=\\frac{\\mathrm{Cov}(X,X_c)}{\\sqrt{\\mathrm{Var}(X)\\,\\mathrm{Var}(X_c)}},\n and express \\mathrm{Cov}(X,X_c), \\mathrm{Var}(X), and \\mathrm{Var}(X_c) in terms of \\{w_k,p_k,p_{c,k}\\}.\nHint: Law of total covariance:\n\\mathrm{Cov}(X,X_c)=\\mathbb E[\\mathrm{Cov}(X,X_c\\mid Z)]+\\mathrm{Cov}(\\mathbb E[X\\mid Z],\\mathbb E[X_c\\mid Z]). Under LE, the first term is 0. Use \\mathbb E[X\\mid Z=k]=2p_k.\n\n\nPart B (10 pts): Bias from ignoring structure in the trait\nSuppose population structure also affects the trait mean: \\mathbb E[Y\\mid Z=k]=\\mu_k. Consider the model Y=\\mu_Z+\\beta_c X_c+\\varepsilon with \\mathbb E[\\varepsilon]=0. Show that the naïve regression of Y on X (without structure covariates) is biased: \n\\hat\\beta_{\\text{naïve}}\n\\approx \\frac{\\beta_c\\,\\mathrm{Cov}(X,X_c)+\\mathrm{Cov}(X,\\mu_Z)}{\\mathrm{Var}(X)}\n= r\\,\\beta_c + \\underbrace{\\frac{\\mathrm{Cov}(X,\\mu_Z)}{\\mathrm{Var}(X)}}_{\\text{bias}}.\n Under the assumptions above, prove that \\mathrm{Cov}(X,\\mu_Z)=2\\,\\mathrm{Cov}_w(p_k,\\mu_k), and give the bias in terms of w_k,p_k,\\mu_k.\nHint: \\mathrm{Cov}(X,\\mu_Z)=\\mathrm{Cov}(\\mathbb E[X\\mid Z],\\mu_Z)=\\mathrm{Cov}(2p_Z,\\mu_Z).\n\n\nPart C (20 pts): Brief interpretation\nIn a few sentences each:\n\nExplain why r\\neq 0 can arise even if within each subpopulation there is no LD. What feature of the mixture induces it?\n\nGive a sign‑consistent example: if subpopulations with larger p_k also have larger \\mu_k, what is the expected direction of the naïve bias?\n\nName two standard strategies to mitigate both components of bias (structure‑induced r and trait mean differences) in practice."
  },
  {
    "objectID": "assignments/assignment-04.html#problem-3-bayesian-analysis-30-pts",
    "href": "assignments/assignment-04.html#problem-3-bayesian-analysis-30-pts",
    "title": "Assignment 04",
    "section": "Problem 3: Bayesian Analysis (30 pts)",
    "text": "Problem 3: Bayesian Analysis (30 pts)\n\nPart A (10 pts): Beta–Binomial conjugacy\n\nWith prior \\mathrm{Beta}(4,18) and data x=11 successes out of n=27, write the posterior distribution for p.\n\nCompute the posterior mean and a central 95% credible interval in R using qbeta. Compare to the MLE \\hat p=11/27. Briefly interpret the shrinkage.\n\nSensitivity: repeat with priors \\mathrm{Beta}(1,1) and \\mathrm{Beta}(8,32). Summarize how the posterior mean and width change across priors, and why.\n\n\n\nPart B (10 pts): Beta–Binomial in Stan\n\nWrite a Stan model to estimate the allele frequency p from Binomial data with a \\mathrm{Beta}(4,18) prior. Use x=11, n=27.\n\nRun the model in R using cmdstanr. Check convergence and effective sample size; report \\hat R and bulk ESS for p.\n\nSummarize the posterior mean and a central 95% credible interval. Compare to your analytical result from Part A.\n\nYou will need to install cmdstanr and cmdstan if you haven’t already. Please follow installation instructions at https://mc-stan.org/cmdstanr/\nBoilerplate is provided below. You will need to set eval to TRUE to run the code when knitting the final document:\n\nlibrary(cmdstanr)\n\nstan_beta_binomial &lt;- \"\ndata {\n  int&lt;lower=0&gt; n;\n  int&lt;lower=0, upper=n&gt; x;\n  real&lt;lower=0&gt; a;\n  real&lt;lower=0&gt; b;\n}\nparameters {\n  real&lt;lower=0, upper=1&gt; p;\n}\nmodel {\n  // TODO: prior on p\n  // Example: p ~ beta(a, b);\n  // TODO: likelihood\n  // Example: x ~ binomial(n, p);\n}\ngenerated quantities {\n  real logit_p = logit(p);\n  int x_rep = binomial_rng(n, p);\n}\n\"\n\nlibrary(cmdstanr)\nset.seed(8878)\n\nwriteLines(stan_beta_binomial, con = \"beta_binomial.stan\")\nmod_bb &lt;- cmdstan_model(\"beta_binomial.stan\")\nfit_bb &lt;- mod_bb$sample(\n    data = list(n = 27, x = 11, a = 4, b = 18),\n    seed = 8878, chains = 4, parallel_chains = 4,\n    iter_warmup = 1000, iter_sampling = 1000\n)\n\n# TODO: check convergence and summarize posterior\n\n\n\nPart C (10 pts): ABO blood group frequencies in Stan (missing AB phenotype)\nWe observe phenotype counts in a population sample where AB individuals are not sampled:\n\nn_A = 725\n\nn_B = 258\n\nn_O = 1073\n\nUnder HWE with allele frequencies \\mathbf p=(p_A,p_B,p_O), the unconditional phenotype probabilities are \n\\Pr(A)=p_A^2 + 2p_A p_O,\\quad\n\\Pr(B)=p_B^2 + 2p_B p_O,\\quad\n\\Pr(AB)=2p_A p_B,\\quad\n\\Pr(O)=p_O^2.\n\nBecause AB is missing, the observed category probabilities are the renormalized values \nq_A=\\frac{\\Pr(A)}{1-\\Pr(AB)},\\quad\nq_B=\\frac{\\Pr(B)}{1-\\Pr(AB)},\\quad\nq_O=\\frac{\\Pr(O)}{1-\\Pr(AB)}.\n\n\nWrite a Stan model that estimates (p_A,p_B,p_O) with prior \\mathrm{Dirichlet}(1,1,1) and a Multinomial likelihood on (n_A,n_B,n_O) using (q_A,q_B,q_O).\n\nRun the model and check convergence (report \\hat R and ESS).\n\nPrior sensitivity: re‑run with \\boldsymbol\\alpha = k\\,(0.26,0.09,0.65) for k\\in\\{1,10,100\\}. Summarize how posterior means and credible intervals change with k, and why.\n\nBoilerplate is provided below:\n\nstan_abo_missing_ab &lt;- \"\ndata {\n  int&lt;lower=0&gt; n_A;\n  int&lt;lower=0&gt; n_B;\n  int&lt;lower=0&gt; n_O;\n  vector&lt;lower=0&gt;[3] alpha;\n}\ntransformed data {\n  int N = n_A + n_B + n_O;\n  int y[3] = { n_A, n_B, n_O };\n}\nparameters {\n  simplex[3] p;\n}\ntransformed parameters {\n  // Unconditional phenotype probabilities under HWE:\n  real PrA  = square(p[1]) + 2 * p[1] * p[3];\n  real PrB  = square(p[2]) + 2 * p[2] * p[3];\n  real PrAB = 2 * p[1] * p[2];\n  real PrO  = square(p[3]);\n\n  // Observed (AB excluded): renormalize by (1 - PrAB)\n  simplex[3] q;\n  {\n    real denom = 1 - PrAB;\n    q[1] = PrA / denom;\n    q[2] = PrB / denom;\n    q[3] = PrO / denom;\n  }\n}\nmodel {\n  // TODO: prior on allele frequencies\n  // Example: p ~ dirichlet(alpha);\n  // TODO: likelihood for observed counts\n  // Example: y ~ multinomial(q);\n}\ngenerated quantities {\n  // Unconditional phenotype probabilities (optional checks)\n  vector[4] phen_prob = [PrA, PrB, PrAB, PrO]';\n}\n\"\n\nset.seed(8878)\nwriteLines(stan_abo_missing_ab, con = \"abo_missing_ab.stan\")\nmod_abo &lt;- cmdstan_model(\"abo_missing_ab.stan\")\nfit_abo &lt;- mod_abo$sample(\n    data = list(n_A = 725, n_B = 258, n_O = 1073, alpha = c(1, 1, 1)),\n    seed = 8878, chains = 4, parallel_chains = 4,\n    iter_warmup = 1000, iter_sampling = 1000\n)"
  },
  {
    "objectID": "assignments/assignment-03.html",
    "href": "assignments/assignment-03.html",
    "title": "Assignment 03",
    "section": "",
    "text": "Requirements:"
  },
  {
    "objectID": "assignments/assignment-03.html#problem-1-em-for-abo-gene-frequencies-derivation-inference-and-sensitivity-25-pts",
    "href": "assignments/assignment-03.html#problem-1-em-for-abo-gene-frequencies-derivation-inference-and-sensitivity-25-pts",
    "title": "Assignment 03",
    "section": "Problem 1: EM for ABO gene frequencies; derivation, inference, and sensitivity (25 pts)",
    "text": "Problem 1: EM for ABO gene frequencies; derivation, inference, and sensitivity (25 pts)\n\nPart A (15 pts)\nConsider phenotype counts from the ABO system under Hardy–Weinberg equilibrium (HWE): n_A, n_{AB}, n_B, n_O with total N. Let genotype frequencies be p_A^2, 2p_Ap_O, 2p_Ap_B, p_B^2, 2p_Bp_O, p_O^2 and p_O = 1 - p_A - p_B.\nDerive the EM updates shown in lecture. Specifically, show that the E‑step allocations for A and B phenotypes are\n\n\\tilde n_{AA} = n_A \\dfrac{p_A^2}{p_A^2 + 2p_Ap_O}\n\n\n\\tilde n_{AO} = n_A \\dfrac{2p_Ap_O}{p_A^2 + 2p_Ap_O}\n\nand similarly for BB, BO, and that the M‑step is \np_A^{(t+1)}=\\frac{2\\tilde n_{AA}+\\tilde n_{AO}+n_{AB}}{2N}\n \np_B^{(t+1)}=\\frac{2\\tilde n_{BB}+\\tilde n_{BO}+n_{AB}}{2N}\n \np_O^{(t+1)}=1-p_A^{(t+1)}-p_B^{(t+1)}\n\nHint (how to derive EM generally):\n\nChoose latent variables so the complete data are simple. Here, treat latent genotype counts, \n(n_{AA}, n_{AO}, n_{AB}, n_{BB}, n_{BO}, n_{OO})\n\n\nas missing, with only phenotype totals observed.\n\nWrite the complete‑data log‑likelihood\n\n\\begin{align*}\n\\ell_c(p_A,p_B) =\n& n_{AA}\\log p_A^2 + n_{AO}\\log(2p_Ap_O) + n_{AB}\\log(2p_Ap_B) + \\\\\n& n_{BB}\\log p_B^2 + n_{BO}\\log(2p_Bp_O) + n_{OO}\\log p_O^2\n\\end{align*}\nusing p_O=1-p_A-p_B.\n\nE‑step: replace latent counts by their conditional expectations given the observed phenotypes under current parameters, e.g., for phenotype A, and form Q(p\\,|\\,p^{(t)}) = \\mathbb E[\\ell_c(p)\\mid\\text{data}, p^{(t)}].\nM‑step: M-step: Maximize Q(p|p^{(t)}) subject to p_A + p_B + p_O = 1. Hint: Consider rewriting the objective in terms of allele counts rather than genotype counts.\n\n\n\nPart B (10 pts)\nConsider two biallelic SNPs with haplotypes \\{ab, aB, Ab, AB\\} under HWE and unphased genotypes (g_1,g_2)\\in\\{0,1,2\\}^2. Note that (ab, ab) yields (0,0), (ab, aB) or (ab, Ab) yields (0,1), (AB, AB) yields (2,2), etc.\nShow that if every observed genotype is (1,1), then P\\{(1,1)\\}=2\\,\\big(p_{ab}p_{AB}+p_{aB}p_{Ab}\\big) and the likelihood depends only on the cross‑sum S=p_{ab}p_{AB}+p_{aB}p_{Ab} (a ridge; parameters not identifiable)."
  },
  {
    "objectID": "assignments/assignment-03.html#problem-2-twopoint-linkage-lod-and-support-intervals-25-pts",
    "href": "assignments/assignment-03.html#problem-2-twopoint-linkage-lod-and-support-intervals-25-pts",
    "title": "Assignment 03",
    "section": "Problem 2: Two‑point linkage — LOD and support intervals (25 pts)",
    "text": "Problem 2: Two‑point linkage — LOD and support intervals (25 pts)\nSuppose one heterozygous transmitting parent (A/a and B/b) is crossed to an aabb mate, yielding child haplotype counts (n_{AB}, n_{Ab}, n_{aB}, n_{ab}). Let n_{\\text{NR}}=n_{AB}+n_{ab} and n_{\\text{R}}=n_{Ab}+n_{aB}.\n\nPart A: LOD from counts (10 pts).\nShow that for a given recombination fraction \\theta\\in(0,0.5) the two‑point LOD relative to independence (\\theta=0.5) is \n\\mathrm{LOD}(\\theta)\n= \\log_{10}\\!\\left\\{ \\frac{\\theta^{\\,n_{\\text{R}}} (1-\\theta)^{\\,n_{\\text{NR}}}}{0.5^{\\,n_{\\text{R}}+n_{\\text{NR}}}} \\right\\}.\n\nDerive the MLE \\hat\\theta and show it equals n_{\\text{R}}/(n_{\\text{R}}+n_{\\text{NR}}) when 0&lt;\\hat\\theta&lt;0.5.\n\n\nPart B: Unknown phase and LD‑informed LOD (15 pts)\nA heterozygous transmitting parent (A/a,\\;B/b) has unknown phase: either coupling (AB/ab) or repulsion (Ab/aB). Let \nn_{\\mathrm{NR}} = n_{AB}+n_{ab},\\qquad\nn_{\\mathrm{R}}  = n_{Ab}+n_{aB},\\qquad\nN=n_{\\mathrm{NR}}+n_{\\mathrm{R}}.\n\nLet w=\\Pr\\{\\text{coupling }(AB/ab)\\} and 1-w=\\Pr\\{\\text{repulsion }(Ab/aB)\\}.\n(i) Mixture likelihood (5 pts).\nShow that with unknown phase the observed‑data likelihood is a mixture of the two phase‑specific binomial likelihoods: \nL(\\theta; w)\n= w\\,(1-\\theta)^{\\,n_{\\mathrm{NR}}}\\,\\theta^{\\,n_{\\mathrm{R}}}\n\\;+\\;\n(1-w)\\,(1-\\theta)^{\\,n_{\\mathrm{R}}}\\,\\theta^{\\,n_{\\mathrm{NR}}}.\n Hence the two‑point LOD relative to independence (\\theta=0.5) is \n\\mathrm{LOD}(\\theta; w)\n= \\log_{10}\\!\\left\\{\n\\frac{w\\,(1-\\theta)^{\\,n_{\\mathrm{NR}}}\\theta^{\\,n_{\\mathrm{R}}}\n+(1-w)\\,(1-\\theta)^{\\,n_{\\mathrm{R}}}\\theta^{\\,n_{\\mathrm{NR}}}}\n{0.5^{\\,N}}\n\\right\\}.\n\n(ii) Linking LD to w (5 pts).\nLet population haplotype frequencies be p=(p_{ab}, p_{aB}, p_{Ab}, p_{AB}) (sum to 1).\nCondition on the parent being the double heterozygote (g_1,g_2)=(1,1). Use Bayes’ rule to show \nw\n= \\Pr\\{(ab,AB)\\mid(1,1)\\}\n= \\frac{p_{ab}\\,p_{AB}}{p_{ab}\\,p_{AB}+p_{aB}\\,p_{Ab}},\n\n\n1-w\n= \\frac{p_{aB}\\,p_{Ab}}{p_{ab}\\,p_{AB}+p_{aB}\\,p_{Ab}}.\n\nDefine D = p_{ab}p_{AB}-p_{aB}p_{Ab} and note that \\operatorname{sign}(D) indicates whether coupling (D&gt;0) or repulsion (D&lt;0) phase is a priori more likely.\n(iii) Quick numerical check (5 pts).\nTake p^\\star=(0.40,\\,0.10,\\,0.25,\\,0.25).\nCompute w and 1-w. Then, using the example counts (n_{AB}, n_{Ab}, n_{aB}, n_{ab})=(18, 5, 4, 17) (so N=44, n_{\\mathrm{R}}=9, n_{\\mathrm{NR}}=35), evaluate and compare \\mathrm{LOD}(\\hat\\theta; w=\\tfrac12) versus \\mathrm{LOD}(\\hat\\theta; w) at \\hat\\theta = n_{\\mathrm{R}}/N.\nBriefly explain (one sentence) how LD information (w\\neq\\tfrac12) can increase or decrease the peak LOD when phase is unknown."
  },
  {
    "objectID": "assignments/assignment-03.html#problem-3-twosnp-haplotype-em-and-ld-measures-25-pts",
    "href": "assignments/assignment-03.html#problem-3-twosnp-haplotype-em-and-ld-measures-25-pts",
    "title": "Assignment 03",
    "section": "Problem 3: Two‑SNP haplotype EM and LD measures (25 pts)",
    "text": "Problem 3: Two‑SNP haplotype EM and LD measures (25 pts)\n\nPart A (10 pts)\nFor two biallelic SNPs with haplotypes \\{ab, aB, Ab, AB\\} at frequencies \\{p_{ab},p_{aB},p_{Ab},p_{AB}\\} (summing to 1), enumerate the possible haplotype pairs consistent with each unphased genotype (g_1,g_2)\\in\\{0,1,2\\}^2.\nShow that only (g_1,g_2)=(1,1) is ambiguous with two possible pairs: (ab,AB) and (aB,Ab).\n\n\nPart B (10 pts)\nSimulate N=1000 individuals from true haplotype frequencies p^{\\star}=(0.40,0.10,0.25,0.25) and estimate \\hat p via EM from a uniform start. Report \\hat p and absolute errors |\\hat p - p^{\\star}|.\nNote that the E‑step weights for (1,1) are proportional to p_{ab}p_{AB} and p_{aB}p_{Ab}, and the M‑step update is p^{(t+1)}=\\text{(expected hap counts)}/(2N).\nHere is a sample R code snippet to get you started:\n\nset.seed(8878)\n\nN &lt;- 1000\np_true &lt;- c(ab = 0.40, aB = 0.10, Ab = 0.25, AB = 0.25)\n\n# helper: draw N unordered haplotype pairs, then make unphased genotypes (g1,g2)\ndraw_genotypes &lt;- function(N, p) {\n    # code haplotypes to allele counts (B allele) at SNP1, SNP2\n    H &lt;- rbind(ab = c(0, 0), aB = c(0, 1), Ab = c(1, 0), AB = c(1, 1))\n    hap1 &lt;- sample(rownames(H), size = N, replace = TRUE, prob = p)\n    hap2 &lt;- sample(rownames(H), size = N, replace = TRUE, prob = p)\n    G &lt;- H[hap1, ] + H[hap2, ] # N x 2 matrix with entries in {0,1,2}\n    as.data.frame(G) |&gt;\n        setNames(c(\"g1\", \"g2\"))\n}\n\n# simulate data\ndat &lt;- draw_genotypes(N, p_true)\n\n\n\nPart C (5 pts)\nCompute D = p_{11} - p_{B1}p_{B2} with p_{11}=p_{AB}, p_{B1}=p_{Ab}+p_{AB}, p_{B2}=p_{aB}+p_{AB}. Report D and r^2 = D^2/(p_{B1}(1-p_{B1})p_{B2}(1-p_{B2})). Comment on how LD would affect single‑marker association at either SNP."
  },
  {
    "objectID": "assignments/assignment-03.html#problem-4-singlemarker-association-with-qc-and-ld-attenuation-25-pts",
    "href": "assignments/assignment-03.html#problem-4-singlemarker-association-with-qc-and-ld-attenuation-25-pts",
    "title": "Assignment 03",
    "section": "Problem 4: Single‑marker association with QC and LD attenuation (25 pts)",
    "text": "Problem 4: Single‑marker association with QC and LD attenuation (25 pts)\nSimulate n=2000 unrelated individuals. Let a causal biallelic SNP C have MAF 0.30 and generate a quantitative trait Y with additive effect size \\beta_C=0.50 (per allele) and noise \\epsilon\\sim\\mathcal N(0,1). Let a tag SNP T be in LD with C such that r=\\operatorname{corr}(G_C,G_T)=0.8 and both are in HWE. Let T have MAF 0.30.\n\nPart A (15 pts)\nGenerate (G_C,G_T,Y) by first simulating haplotypes for (C,T) with a chosen LD structure that yields r\\approx 0.8, then form genotypes and Y=\\beta_C G_C + \\epsilon. Fit simple linear models Y\\sim G_C and Y\\sim G_T and report \\hat\\beta_C and \\hat\\beta_T, alongside their 95\\% confidence intervals.\n\n\nPart B (10 pts)\nIntroduce a basic QC step: test HWE in the controls of a case–control subsample formed by thresholding Y at its 80th percentile to define cases (cases are the top 20% of Y). Compute an exact or \\chi^2 HWE p‑value in controls for T; state whether you would flag T using a threshold of 10^{-6} and why QC is typically done in controls only."
  },
  {
    "objectID": "assignments/assignment-01.html",
    "href": "assignments/assignment-01.html",
    "title": "Assignment 01",
    "section": "",
    "text": "Make a Zotero account using the guide here. Make sure you use your GW email address, as this will provide unlimited cloud storage for PDFs. Once you have created your account, email chiraaggohel@gwu.edu your username.\n(Laird, 2.4) How many genotypes are possible with a 3-allele marker? With K alleles?\n(Laird, 2.6) Consider a recessive Mendelian disease, where in the population, P(\\text{an individual has 2 disease variants}) = 0.000001.\n\nWhat is the probability that a randomly selected person is affected? Suppose that the randomly selected person is affected. What does that imply about the probability that their sibling is also affected (you can assume that having either one or two parents with two variants is so rare that you can ignore them)?\nNow answer both of these questions assuming the penetrance is only \\frac{1}{2}, i.e., P(\\text{disease} | 2 \\text{ variants}) = \\frac{1}{2}, but the phenocopy rate is still zero.\n\nConsider a sample size of n of unrelated haploid individuals is obtained from some population with the objective of estimating allele frequency at a biallelic locus. The sample contains x copies of A, and n-x copies of a.\n\nPlot the probability distribution of X given n = 30, and \\theta = .1. Plot the probability distribution of X given n = 1000, and \\theta = .1.\nLets say we observed 30 samples, with 10 copies of allele A. Plot the likelihood function for \\theta\nWhat is the MLE of \\theta?\nLet’s say n = 1000, and x = 100. What is the sampling variance of \\hat{\\theta}?\nLet’s say n = 100, and x = 10. What is the sampling variance of \\hat{\\theta}? Why is this different than the result above?\n\nRefer to equations (1.3) and (1.5) in Sorensen. Say you observe 8 individuals, and 1 copies of genotype X. Assume that X \\sim \\textsf{Binom}(n, \\theta).\n\n\nCompute \\hat{\\theta}\nCompute \\hat{\\text{Var}}(\\hat{\\theta})\nProvide a 95% Wald confidence interval for \\theta\nWrite an interpretation of this confidence interval. What problem does this reveal about the Wald confidence interval?\nCompute a 95% Wilson confidence interval for \\theta. Documentation for this can be found here. Hint: you will need the fastR2 package.\n\n\nRefer to slides 13 and 14 in lecture 1. Write a one to two sentence answer for how a researcher would try to answer each question.\nWhat is your math background? What is your programming background?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Schedule",
    "section": "",
    "text": "Article links direct to files hosted on the Zotero group library\n\n\n\nWeek\nLecture\nReadings\nAssignment\n\n\n\n\n1\nFoundations: Mendelian genetics & statistical basics\nMendel’s laws, Hardy–Weinberg equilibrium, \\(\\chi^2\\)goodness‑of‑fit. Sampling distributions; linking population parameters to sample estimates. One‑locus likelihood: building and interpreting likelihood functions.\n\nSorensen Chapter 1\nEdwards, A. W. F. (2008), “G. H. Hardy (1908) and Hardy–Weinberg Equilibrium,” Genetics.\nIntroduction to Probability Theory\n\nProblem Set 01\n\n\n2\nHeritability, segregation, and the gene-mapping toolkit\nNarrow and broad sense heritability; variance-component interpretation. Segregation analysis and modelling genetic inheritance without marker data.\n\nSorensen Chapter 2.1-2.2, 2.4, 2.8\nVisscher, P. M., et al., (2008), “Heritability in the genomics era — concepts and misconceptions,” Nature Reviews Genetics.\n\nProblem Set 02\n\n\n3\nLikelihood algorithms & practical gene mapping\nNewton-Raphson, EM, and stochastic gradient algorithms for complex likelihoods. Pedigree linkage analysis, LOD-score calculation, and missing-data EM steps. Single-marker and haplotype association tests with basic quality control\n\nSorensen, Chapter 3\nLander, E. S., and Green, P. (1987), “Construction of multilocus genetic linkage maps in humans.,” Proceedings of the National Academy of Sciences of the United States of America.\n\nProblem Set 03\n\n\n4\nPopulation structure & Bayesian fundamentals\nDetecting and correcting for population stratification and admixture confounding. Priors, posteriors, and the Bayes-frequentist debate in genetic inference. Bayesian admixture/STRUCTURE-style modelling implemented in Stan.\n\nSorensen 4.1-4.5, 4.7-4.8, 5.1\nPorras-Hurtado, L. et al., (2013), “An overview of STRUCTURE: applications, parameter settings, and supporting software,” Frontiers in Genetics.\nLawson, D. J. et al., (2018), “A tutorial on how not to over-interpret STRUCTURE and ADMIXTURE bar plots,” Nature Communications.\n\nProblem Set 04\n\n\n5\nGWAS at Scale\nEnd to end GWAS workflow: sample QC, variant QC, power, pitfalls. Linear mixed models (BOLT LMM/REML concepts) and SAIGE for imbalance/relatedness. Fixed/random effects meta-analysis; genomic inflation and calibration.\n\nLoh, P.-R. et al., (2015), “Efficient Bayesian mixed model analysis increases association power in large cohorts,” Nature Genetics.\n\n\n\n\n6\nPrediction models in genetics\n\nSorensen Chapters 6, 7.1-7.2, 10.1, 10.5, 11.3-11.5\nWu, T. T., Chen, Y. F., Hastie, T., Sobel, E., and Lange, K. (2009), “Genome-wide association analysis by lasso penalized logistic regression,” Bioinformatics, 25, 714–721. https://doi.org/10.1093/bioinformatics/btp041.\n\n\n\n\n7\nMultiple testing & false-discovery control\n\nSorensen Chapter 8\nOtani, T., Noma, H., Nishino, J., and Matsui, S. (2018), “Re-assessment of multiple testing strategies for more efficient genome-wide association studies,” European Journal of Human Genetics, Nature Publishing Group, 26, 1038–1048. https://doi.org/10.1038/s41431-018-0125-3.\n\n\n\n\n8\nBinary traits and family-based association tests\n\nSorensen Chapter 9\nZhou, W., Bi, W., Zhao, Z., Dey, K. K., Jagadeesh, K. A., Karczewski, K. J., Daly, M. J., Neale, B. M., and Lee, S. (2022), “SAIGE-GENE+ improves the efficiency and accuracy of set-based rare variant association tests,” Nature Genetics, Nature Publishing Group, 54, 1466–1469. https://doi.org/10.1038/s41588-022-01178-w.\n\n\n\n\n9\nCausal inference & functional integration (Mendelian randomization)\n\nSanderson, E., Glymour, M. M., Holmes, M. V., Kang, H., Morrison, J., Munafò, M. R., Palmer, T., Schooling, C. M., Wallace, C., Zhao, Q., and Davey Smith, G. (2022), “Mendelian randomization,” Nature Reviews Methods Primers, 2, 1–21. https://doi.org/10.1038/s43586-021-00092-5.\n\n\n\n\n10\nAdvanced AI Topics in Statistical Genetics: Language Models for Genomics\nRecommended:\n\nJi et al., 2021\nAvsec et al., 2021\nCheng et al., 2023\nBaghbanzadeh et al., 2025\nMollerus et al., 2025",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Helpful Links",
    "section": "",
    "text": "Linear Algebra Primer\nProbability Primer\nStatistical Inference Primer\nStatistics Cheatsheet\nEM Algorithm Video",
    "crumbs": [
      "Useful Links"
    ]
  },
  {
    "objectID": "links.html#mathematical-resources",
    "href": "links.html#mathematical-resources",
    "title": "Helpful Links",
    "section": "",
    "text": "Linear Algebra Primer\nProbability Primer\nStatistical Inference Primer\nStatistics Cheatsheet\nEM Algorithm Video",
    "crumbs": [
      "Useful Links"
    ]
  },
  {
    "objectID": "lectures/lecture-05.html",
    "href": "lectures/lecture-05.html",
    "title": "Lecture 05: GWAS in R",
    "section": "",
    "text": "This session walks through a minimal GWAS workflow in R using SNPRelate and GENESIS. There exist many tools for GWAS, including PLINK, BOLT-LMM, REGENIE, SAIGE, and others. Here, we focus on GENESIS to illustrate the key steps.\nWe work with a small example GDS bundled with GENESIS to keep runtime short while illustrating the full pipeline:\n\nQuality control (variant- and sample‑level)\nLD pruning for structure/relatedness tasks\nRelatedness (KING‑robust) and population structure (PC‑AiR)\nNull model fitting (linear mixed model)\nSingle‑variant association and diagnostics (QQ, λGC)"
  },
  {
    "objectID": "lectures/lecture-05.html#overview",
    "href": "lectures/lecture-05.html#overview",
    "title": "Lecture 05: GWAS in R",
    "section": "",
    "text": "This session walks through a minimal GWAS workflow in R using SNPRelate and GENESIS. There exist many tools for GWAS, including PLINK, BOLT-LMM, REGENIE, SAIGE, and others. Here, we focus on GENESIS to illustrate the key steps.\nWe work with a small example GDS bundled with GENESIS to keep runtime short while illustrating the full pipeline:\n\nQuality control (variant- and sample‑level)\nLD pruning for structure/relatedness tasks\nRelatedness (KING‑robust) and population structure (PC‑AiR)\nNull model fitting (linear mixed model)\nSingle‑variant association and diagnostics (QQ, λGC)"
  },
  {
    "objectID": "lectures/lecture-05.html#setup",
    "href": "lectures/lecture-05.html#setup",
    "title": "Lecture 05: GWAS in R",
    "section": "Setup",
    "text": "Setup\n\n1library(SNPRelate)\n2library(GENESIS)\n3library(GWASTools)\n4library(qqman)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nset.seed(8878)\n\n\n1\n\nSNPRelate (Zheng et al., 2012) for GDS file handling and genotype data management (Zheng et al., 2012)\n\n2\n\nGENESIS for genome-wide association analysis and relatedness estimation (Manichaikul et al., 2010; Conomos et al., 2015)\n\n3\n\nGWASTools for genotype data management and quality control\n\n4\n\nqqman for creating Q-Q and Manhattan plots"
  },
  {
    "objectID": "lectures/lecture-05.html#step-1-accessing-and-loading-data",
    "href": "lectures/lecture-05.html#step-1-accessing-and-loading-data",
    "title": "Lecture 05: GWAS in R",
    "section": "Step 1: Accessing and Loading Data",
    "text": "Step 1: Accessing and Loading Data\n\nHapMap 3\n\nHapMap3 is an integrated reference of common and low-frequency alleles. The project genotyped 1.6 million common SNPs in 1,184 reference individuals from 11 global populations. (The International HapMap 3 Consortium, 2010)\nThe project mapped linkage disequilibrium (LD) patterns used for tag‑SNP selection and served as an early imputation reference. (The International HapMap 3 Consortium, 2010; The International HapMap Consortium, 2005)\nHere, we will use a small subset from ASW (African ancestry in the Southwest USA) and MXL (Mexican ancestry in Los Angeles) samples\n\n\n1gdsfile &lt;- system.file(\"extdata\", \"HapMap_ASW_MXL_geno.gds\", package = \"GENESIS\")\n\n2g &lt;- SNPRelate::snpgdsOpen(gdsfile)\n\n3samp.id &lt;- read.gdsn(index.gdsn(g, \"sample.id\"))\nsnp.id &lt;- read.gdsn(index.gdsn(g, \"snp.id\"))\nchr &lt;- read.gdsn(index.gdsn(g, \"snp.chromosome\"))\npos &lt;- read.gdsn(index.gdsn(g, \"snp.position\"))\n\n\n1\n\nLocate the example GDS file included with the GENESIS package\n\n2\n\nOpen the GDS file for random access to genotype data\n\n3\n\nRead sample IDs, SNP IDs, chromosome numbers, and positions from the GDS file\n\n\n\n\nWe can also download the associated metadata\n\nmetadata &lt;- readr::read_tsv(\"https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/hapmap3_r3/relationships_w_pops_041510.txt\", show_col_types = FALSE)\nhead(metadata)\n\n# A tibble: 6 × 7\n  FID   IID     dad     mom       sex pheno population\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     \n1 2427  NA19919 NA19908 NA19909     1     0 ASW       \n2 2431  NA19916 0       0           1     0 ASW       \n3 2424  NA19835 0       0           2     0 ASW       \n4 2469  NA20282 0       0           2     0 ASW       \n5 2368  NA19703 0       0           1     0 ASW       \n6 2425  NA19902 NA19900 NA19901     2     0 ASW       \n\n\n\nmetadata |&gt;\n    mutate(population = case_when(population == \"MEX\" ~ \"MXL\", \n    TRUE ~ population))\n\n# A tibble: 1,397 × 7\n   FID   IID     dad     mom       sex pheno population\n   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     \n 1 2427  NA19919 NA19908 NA19909     1     0 ASW       \n 2 2431  NA19916 0       0           1     0 ASW       \n 3 2424  NA19835 0       0           2     0 ASW       \n 4 2469  NA20282 0       0           2     0 ASW       \n 5 2368  NA19703 0       0           1     0 ASW       \n 6 2425  NA19902 NA19900 NA19901     2     0 ASW       \n 7 2425  NA19901 0       0           2     0 ASW       \n 8 2427  NA19908 0       0           1     0 ASW       \n 9 2430  NA19914 0       0           2     0 ASW       \n10 2470  NA20287 0       0           2     0 ASW       \n# ℹ 1,387 more rows"
  },
  {
    "objectID": "lectures/lecture-05.html#step-2-pre-modeling-qc",
    "href": "lectures/lecture-05.html#step-2-pre-modeling-qc",
    "title": "Lecture 05: GWAS in R",
    "section": "Step 2: Pre-Modeling QC",
    "text": "Step 2: Pre-Modeling QC\n\nGenotype quality control\n\nFiltering for minor allele frequency (MAF)\n\nSNP-chips often determine genotypes based on intensities for each allele\nWhen MAF is low, genotype clusters can overlap, leading to genotyping errors\n\n\n\n\n\n\n\n\n\n\nWe can compute various statistics regarding allele frequency using snpgdsSNPRateFreq\n\naf &lt;- snpgdsSNPRateFreq(g)\nstr(af)\nmaf &lt;- af$MinorFreq\n\nList of 3\n $ AlleleFreq : num [1:20000] 0.39 0.494 0.101 0.486 0.445 ...\n $ MinorFreq  : num [1:20000] 0.39 0.494 0.101 0.486 0.445 ...\n $ MissingRate: num [1:20000] 0 0 0 0 0.00578 ...\n\n\nWe can see that af contains both minor allele frequency and missing rate per SNP.\n\ndata.frame(maf = maf) |&gt;\n    mutate(maf_le05 = maf &lt; 0.05) |&gt;\n    ggplot(aes(x = maf, fill = maf_le05)) +\n    geom_histogram(binwidth = 0.01, color = \"black\") +\n    geom_vline(xintercept = c(0.05), linetype = \"dashed\", color = \"red\") +\n    theme_minimal() +\n    viridis::scale_fill_viridis(discrete = TRUE, end = .9, begin = .2) +\n    labs(fill = \"MAF &lt; .05\")\n\n\n\n\n\n\n\n\n\n\nVariant missingness\n\nMissingness can indicate genotyping errors or batch effects\nMissingness can be non-random with respect to phenotype or ancestry\nTypically filter variants with &gt; 2% missingness\nWe can use the missingness results from our af object\nAdditionally, we can retrieve the genotype matrix itself, and manually compute the rate per SNP\n\n\nG &lt;- snpgdsGetGeno(g, with.id = TRUE)\n\nGenotype matrix: 20000 SNPs X 173 samples\n\ngeno_mat &lt;- G$genotype\ndim(geno_mat)\n\n[1] 20000   173\n\n\nIndexing in R goes rows-by-columns. So, For geno_mat, the SNPs are rows and the samples are columns. We can inspect subset of the matrix:\n\ngeno_mat[10:20, 1:10]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    2    0    0    1    0    1    0    1    2     0\n [2,]    0    1    1    1    1    0    0    0    2     0\n [3,]    1    1    1    1    0    0    1    0    2     1\n [4,]    0    0    0    0    0    0    0    0    0     1\n [5,]    0    0    0    0    0    0    0    0    0     0\n [6,]    1    0    1    1    0    0    0    2    1     0\n [7,]    2    1    1    1    0    0    0    2    0     1\n [8,]    0    0    0    0    1    1    0    0    2     0\n [9,]    1    0    0   NA    0    1    0    2    0     0\n[10,]    1    1    0    1    1    1    1    0    0     0\n[11,]    1    0    1    0    1    1    0    2    1     0\n\n\nSNPRelate allows for four possible values stored in the genotype matrix: 0, 1, 2, 3. Consider a bi-allelic SNP site with possible alleles A and B.\n\n0 indicates BB\n1 indicates AB\n2 indicates AA\n3 indicates a missing genotype\n\nFor multi-allelic sites, it is a count of the reference allele.\n\nmiss_var_manual &lt;- rowMeans(is.na(geno_mat))\nmiss_var &lt;- af$MissingRate \nidentical(miss_var, miss_var_manual)\n\n[1] TRUE\n\n\n\ndata.frame(miss_var = miss_var) |&gt;\n    mutate(miss_var_gt02 = miss_var &gt; 0.02) |&gt;\n    ggplot(aes(x = miss_var, fill = miss_var_gt02)) +\n    geom_histogram(binwidth = 0.001, color = \"black\") +\n    geom_vline(xintercept = c(0.02), linetype = \"dashed\", color = \"red\") +\n    theme_minimal() +\n    viridis::scale_fill_viridis(discrete = TRUE, end = .9, begin = .2) +\n    labs(fill = \"Missing &gt; .02\")\n\n\n\n\n\n\n\n\n\n\n\nSample-level missingness & heterozygosity\n\nSamples with high missingness may have poor DNA quality or technical issues\nSamples with extreme heterozygosity rates may be contaminated or mislabelled\n\n\nsamp_miss &lt;- snpgdsSampMissRate(g)\n\ndata.frame(samp_miss = samp_miss) |&gt;\n    mutate(samp_outlier = samp_miss &gt; 0.02) |&gt;\n    ggplot(aes(x = samp_miss, fill = samp_outlier)) +\n    geom_histogram(binwidth = 0.01, color = \"black\") +\n    geom_vline(xintercept = c(0.02), linetype = \"dashed\", color = \"red\") +\n    theme_minimal() +\n    labs(title = \"Sample missingness\", x = \"Missing rate\")\n\n\n\n\n\n\n\n\n\nhet_rate &lt;- colMeans(geno_mat == 1, na.rm = TRUE)\n\ndata.frame(het_rate = het_rate) |&gt;\n    mutate(het_outlier = abs(scale(het_rate)) &gt; 3) |&gt;\n    ggplot(aes(x = het_rate, fill = het_outlier)) +\n    geom_histogram(binwidth = 0.01, color = \"black\") +\n    geom_vline(xintercept = c(mean(het_rate) + 3 * sd(het_rate),\n                              mean(het_rate) - 3 * sd(het_rate)),\n               linetype = \"dashed\", color = \"red\") +\n    theme_minimal() +\n    labs(title = \"Heterozygosity rate\", x = \"Heterozygosity rate\")\n\n\n\n\n\n\n\n\n\n\nThresholding\nPutting it together: below we (i) filter samples on missingness/heterozygosity, (ii) apply preliminary variant filters (missingness + MAF), (iii) LD‑prune and estimate relatedness (KING) on that prelim set, and later (iv) compute HWE within ancestry‑homogeneous unrelated subsets using the metadata to finalize variant inclusion. This order avoids false HWE failures from population mixture/relatedness. (Anderson et al., 2010)\n\nthr_samp_miss &lt;- 0.02\nthr_var_miss &lt;- 0.02\nthr_maf &lt;- 0.05\nthr_het_z &lt;- 3\n\nkeep_sample &lt;- which(samp_miss &lt;= thr_samp_miss & abs(scale(het_rate)) &lt; thr_het_z)\nsamp.keep.id &lt;- samp.id[keep_sample]\n\nkeep_snp_prelim &lt;- which(miss_var &lt;= thr_var_miss & maf &gt;= thr_maf)\nsnp.prelim.id &lt;- snp.id[keep_snp_prelim]\nlength(snp.prelim.id)\n\n[1] 18960\n\n\n\n\nSNP correlation structure\n\nmax_plot_snps &lt;- 100\nld_snp_ids &lt;- snp.prelim.id[seq_len(max_plot_snps)]\n\nld_mat &lt;- snpgdsLDMat(\n    g,\n    sample.id = samp.keep.id,\n    snp.id = ld_snp_ids,\n    slide = -1,\n    method = \"corr\"\n)\n\nLinkage Disequilibrium (LD) estimation on genotypes:\n    # of samples: 167\n    # of SNPs: 100\n    using 1 thread\n    method: correlation\nLD matrix:    the sum of all selected genotypes (0,1,2) = 9659\n\n\n\n\n\n\n\n\n\n\n\n\n\nLD pruning\nLD pruning removes variants in high LD to yield a subset of approximately independent variants. This is useful for population structure and relatedness estimation, which can be distorted by large blocks of correlated SNPs.\nThe algorithm works as follows (Zheng et al., 2012):\n\nRandomly select a starting position i (start.pos=\"random\"), i=1 if start.pos=\"first\", or i=last if start.pos=\"last\"; and let the current SNP set S={ i };\nFor each right position j from i+1 to n: if any LD between j and k is greater than ld.threshold, where k belongs to S, and both of j and k are in the sliding window, then skip j; otherwise, let S be S + { j };\nFor each left position j from i-1 to 1: if any LD between j and k is greater than ld.threshold, where k belongs to S, and both of j and k are in the sliding window, then skip j; otherwise, let S be S + { j };\nOutput S, the final selection of SNPs.\n\n\nset.seed(1)\nld_prune &lt;- snpgdsLDpruning(g,\n    sample.id = samp.keep.id, \n    snp.id = snp.prelim.id,\n    autosome.only = TRUE,\n    method = \"r\",\n    slide.max.bp = 500000,\n    ld.threshold = sqrt(.1)\n)\n\nSNP pruning based on LD:\nExcluding 1,040 SNPs (non-autosomes or non-selection)\nExcluding 0 SNP (monomorphic: TRUE, MAF: 0.005, missing rate: 0.05)\n    # of samples: 167\n    # of SNPs: 18,960\n    using 1 thread\n    sliding window: 500,000 basepairs, Inf SNPs\n    |LD| threshold: 0.316228\n    method: R\nChrom 1: |====================|====================|\n    21.27%, 4,255 / 20,000 (Fri Oct  3 16:11:37 2025)\n4,255 markers are selected in total.\n\nsnps_pruned &lt;- unlist(ld_prune, use.names = FALSE)\n\n\n\nRelatedness\nWe need to account for relatedness to avoid confounding in PCA and association testing. KING (Kinship‑based INference for GWAS) estimates pairwise kinship robust to population structure (Manichaikul et al., 2010).\n\n1king &lt;- snpgdsIBDKING(g,\n                      sample.id = samp.keep.id,\n                      snp.id = snps_pruned,\n                      type = \"KING-robust\",\n2                      family.id = metadata$FID[match(samp.keep.id, metadata$IID)])\n\n3KINGmat &lt;- GENESIS::kingToMatrix(king)\n\n\n1\n\nRun KING on our dataset with the filtered samples and LD‑pruned SNPs.\n\n2\n\nModel is robust to misspecifications in population structure.\n\n3\n\nConvert into a kinship matrix we can use for downstream tasks.\n\n\n\n\nUsing 167 samples provided\n\n\nIdentifying clusters of relatives...\n\n\n    154 relatives in 29 clusters; largest cluster = 49\n\n\nCreating block matrices for clusters...\n\n\n13 samples with no relatives included\n\n\nPutting all samples together into one block diagonal matrix\n\n\nIBD analysis (KING method of moment) on genotypes:\nExcluding 15,745 SNPs (non-autosomes or non-selection)\nExcluding 0 SNP (monomorphic: TRUE, MAF: NaN, missing rate: NaN)\n    # of samples: 167\n    # of SNPs: 4,255\n    using 1 thread\n# of families: 73, and within- and between-family relationship are estimated differently.\nRelationship inference in the presence of population stratification.\nKING IBD:    the sum of all selected genotypes (0,1,2) = 365234\nCPU capabilities:\nFri Oct  3 16:11:37 2025    (internal increment: 65536)\n\n[..................................................]  0%, ETC: ---        \n[==================================================] 100%, completed, 0s\nFri Oct  3 16:11:37 2025    Done.\n\n\n\n\nHWE within ancestry groups (metadata + unrelateds)\nUnder random mating and no technical artifacts, genotype frequencies should follow the familiar proportions under HWE for allele frequency p. A simple check is whether observed counts depart more than sampling noise would predict. In GWAS array data, notable departures often flag lab/array problems rather than biology. Common culprits are poor clustering for one allele, batch effects, or plate swaps (all of which distort heterozygote vs. homozygote balance).\nFor case-control studies, we compute HWE in controls. True disease associations can produce real departures in cases (e.g., risk allele homozygotes enriched), so filtering on cases risks discarding biological signal. (Anderson et al., 2010)\nWe will use an exact test for bi‑allelic SNPs. Asymptotic chi‑square tests can mis‑calibrate at low genotype counts or low MAF, while exact tests keep Type I error in check across MAF ranges. (Wigginton et al., 2005; Guo and Thompson, 1992)\nRegarding a specific threshold, large studies typically use $p &lt; 10^{−6} as a conservative filter. Smaller studies sometimes use p &lt; 10^{-4}. Because power to detect departures depends on MAF we can consider MAF‑stratified thresholds to avoid over‑penalizing rare variants. (Anderson et al., 2010)\nX/sex chromosomes require specific sex‑aware tests (not used here; single autosome). (Anderson et al., 2010)\nTo avoid the Wahlund effect (heterozygote deficit from pooling distinct ancestries) and non‑independence from relatives, we compute HWE within ancestry‑homogeneous, unrelated subsets. We use KING to define unrelateds and the metadata population labels (ASW, MXL) for grouping.\n\n# Identify unrelateds (3rd‑degree or closer considered related)\nkm &lt;- KINGmat |&gt; as.matrix()\ndiag(km) &lt;- 0 \n1unrel_ids &lt;- rownames(km)[rowSums(km &gt;= 0.0442, na.rm = TRUE) == 0]\nunrel_ids &lt;- intersect(unrel_ids, samp.keep.id)\n\n2grp &lt;- split(unrel_ids, metadata$population[match(unrel_ids, metadata$IID)])\n\n# Exact HWE p-values per group on prelim variants\n3p_by_grp &lt;- lapply(grp, function(ids) {\n  snpgdsHWE(g, snp.id = snp.prelim.id, sample.id = ids)\n})\np_mat &lt;- do.call(cbind, p_by_grp)\n\n4thr_hwe &lt;- 1e-6\n\n# Conservative rule: pass only if all groups pass\n5pass_all &lt;- apply(p_mat, 1, function(p) all(is.na(p) | p &gt; thr_hwe))\n\n# Final variant set after HWE\nkeep_snp &lt;- which(pass_all)\nsnp.keep.id &lt;- snp.prelim.id[keep_snp]\nlength(snp.prelim.id)\nlength(snp.keep.id)\n\n\n1\n\nDefine unrelateds as those with kinship &lt; 0.0442 (3rd-degree or closer) to any other sample.\n\n2\n\nSplit unrelateds by population using metadata\n\n3\n\nCompute exact HWE p-values for each group on the preliminary variant set\n\n4\n\nSet a stringent HWE p-value threshold\n\n5\n\nRetain variants that pass HWE in all groups\n\n\n\n\n[1] 18960\n[1] 18960\n\n\nNote that we end up not filtering any variants with this dataset.\n\n\nAncestry PCs\nWith a kinship matrix in hand, we estimate ancestry PCs to use as covariates. PC‑AiR computes PCs on a subset of unrelated individuals and projects relateds onto that space, providing robust structure estimates in the presence of relatedness (Conomos et al., 2015).\n\n1snpgdsClose(g)\n2geno_reader &lt;- GWASTools::GdsGenotypeReader(gdsfile)\n3geno_data &lt;- GenotypeData(geno_reader)\n4pcair_out &lt;- pcair(geno_data,\n                   kinobj = KINGmat,\n                   divobj = KINGmat,\n                   snp.include = snps_pruned,\n                   sample.include = samp.keep.id)\n\n\n1\n\nWe need to close the SNPRelate connection before opening with GWASTools\n\n2\n\nOpen the GDS file with GWASTools\n\n3\n\nCreate a GenotypeData object for analysis\n\n4\n\nRun PC-AiR using the KING kinship matrix to identify unrelateds and compute PCs\n\n\n\n\nUsing kinobj and divobj to partition samples into unrelated and related sets\n\n\nWorking with 167 samples\n\n\nIdentifying relatives for each sample using kinship threshold 0.0220970869120796\n\n\nIdentifying pairs of divergent samples using divergence threshold -0.0220970869120796\n\n\nPartitioning samples into unrelated and related sets...\n\n\nUnrelated Set: 92 Samples \nRelated Set: 75 Samples\n\n\nPerforming PCA on the Unrelated Set...\n\n\nPredicting PC Values for the Related Set...\n\n\nPrincipal Component Analysis (PCA) on genotypes:\nExcluding 15,745 SNPs (non-autosomes or non-selection)\nExcluding 0 SNP (monomorphic: TRUE, MAF: NaN, missing rate: NaN)\n    # of samples: 92\n    # of SNPs: 4,255\n    using 1 thread\n    # of principal components: 32\nPCA:    the sum of all selected genotypes (0,1,2) = 201672\nCPU capabilities:\nFri Oct  3 16:11:37 2025    (internal increment: 5340)\n\n[..................................................]  0%, ETC: ---        \n[==================================================] 100%, completed, 0s\nFri Oct  3 16:11:37 2025    Begin (eigenvalues and eigenvectors)\nFri Oct  3 16:11:37 2025    Done.\nSNP Loading:\n    # of samples: 92\n    # of SNPs: 4,255\n    using 1 thread\n    using the top 32 eigenvectors\nSNP Loading:    the sum of all selected genotypes (0,1,2) = 201672\nFri Oct  3 16:11:37 2025    (internal increment: 42740)\n\n[..................................................]  0%, ETC: ---        \n[==================================================] 100%, completed, 0s\nFri Oct  3 16:11:37 2025    Done.\nSample Loading:\n    # of samples: 75\n    # of SNPs: 4,255\n    using 1 thread\n    using the top 32 eigenvectors\nSample Loading:    the sum of all selected genotypes (0,1,2) = 163562\nFri Oct  3 16:11:37 2025    (internal increment: 52428)\n\n[..................................................]  0%, ETC: ---        \n[==================================================] 100%, completed, 0s\nFri Oct  3 16:11:37 2025    Done.\n\n\n\npcair_tb &lt;- pcair_out$vectors |&gt;\n    as_tibble() |&gt;\n    bind_cols(\"IID\" = pcair_out$sample.id) |&gt;\n    left_join(metadata, by = \"IID\")\n\npcair_tb |&gt;\n    ggplot(aes(V1, V2, col = population)) +\n    geom_point()\n\n\n\n\n\n\n\n\nWe can use a screeplot to visualize the variance explained by each PC. Depending on where we see an “elbow” in the screeplot, we might choose to include the corresponding PCs in our model.\n\npcair_out$values |&gt;\n    as_tibble() |&gt;\n    mutate(PC = row_number()) |&gt;\n    ggplot(aes(PC, value)) +\n    geom_point() +\n    geom_line() +\n    scale_x_continuous(breaks = seq(1, 10, by = 1)) +\n    labs(title = \"Screeplot of PC-AiR eigenvalues\", x = \"Principal Component\", y = \"Eigenvalue\")\n\n\n\n\n\n\n\n\n\n\nAncestry‑adjusted relatedness\nNow that we have ancestry PCs, we estimate relatedness adjusted for ancestry using PC‑Relate and convert it to a GRM suitable for the null model. Training on the unrelated set stabilizes estimates. (Conomos et al., 2016)\n\ngenodata_iter &lt;- GenotypeBlockIterator(geno_data, snpInclude=snps_pruned)\n\npcrel &lt;- GENESIS::pcrelate(\n  gdsobj = genodata_iter,\n  pcs = pcair_out$vectors[,1:3],\n  training.set = intersect(unrel_ids, pcair_out$sample.id),\n  sample.include = samp.keep.id,\n)\n\nUsing 6 CPU cores\n\n\n167 samples to be included in the analysis...\n\n\nBetas for 3 PC(s) will be calculated using 20 samples in training.set...\n\n\nRunning PC-Relate analysis for 167 samples using 4255 SNPs in 1 blocks...\n\n\nPerforming Small Sample Correction...\n\npcrelMat &lt;- GENESIS::pcrelateToMatrix(pcrel, scaleKin = 2)\n\nUsing 167 samples provided\n\n\nIdentifying clusters of relatives...\n\n\n    167 relatives in 1 clusters; largest cluster = 167\n\n\nCreating block matrices for clusters...\n\n\n0 samples with no relatives included\n\n\n\n\nConstruct phenotype and merge covariates\nFor the sake of this demo, we will simulate a quantitative trait with a known genetic effect\n\nset.seed(99)\nN &lt;- length(samp.keep.id)\n# Use recorded sex from metadata for covariates (single chromosome: no sex-chr analysis needed)\nsex &lt;- metadata |&gt;\n    filter(IID %in% samp.keep.id) |&gt;\n    mutate(sex = case_when(sex == 1 ~ \"M\",\n                           TRUE ~ \"F\")) |&gt;\n    pull(sex)\n\n# Choose one moderately frequent SNP and simulate a quantitative trait with a planted effect\nmaf_keep &lt;- maf[snp.keep.id]\nix &lt;- which(!is.na(maf_keep) & maf_keep &gt; 0.2 & maf_keep &lt; 0.3)[1]\n\nG1 &lt;- GWASTools::getGenotypeSelection(geno_data, snpID = snp.keep.id[ix], scanID = samp.keep.id)\nG1 &lt;- as.numeric(G1) \ny &lt;- as.numeric(0.4* scale(G1) + 0.05 * (sex == \"M\") + rnorm(N))\n\nWe now have a phenotype vector y and covariates. We need to construct a ScanAnnotationDataFrame object for the null model fitting.\n\npcs_df &lt;- data.frame(\n  scanID = pcair_out$sample.id,\n  PC1 = pcair_out$vectors[, 1],\n  PC2 = pcair_out$vectors[, 2],\n  PC3 = pcair_out$vectors[, 3]\n)\n\nscanDF &lt;- dplyr::left_join(\n  data.frame(scanID = samp.keep.id, y = y, sex = sex),\n  pcs_df,\n  by = \"scanID\"\n)\n\nscanAnnot2 &lt;- GWASTools::ScanAnnotationDataFrame(scanDF)"
  },
  {
    "objectID": "lectures/lecture-05.html#step-3-modeling",
    "href": "lectures/lecture-05.html#step-3-modeling",
    "title": "Lecture 05: GWAS in R",
    "section": "Step 3: Modeling",
    "text": "Step 3: Modeling\n\nNull model\nWe first fit the mixed model under the null hypothesis that each SNP has no effect. The null includes covariates (age, sex, ancestry PCs) and a random genetic effect capturing relatedness; it does not include per‑SNP fixed effects.\nThe model:\n\ny = X\\beta + g + \\epsilon\n\nWhere y is the phenotype, X\\beta are fixed‑effect covariates, g \\sim \\mathcal{N}(0, \\sigma^2_g K) is the random genetic effect with relatedness matrix K, and \\epsilon \\sim \\mathcal{N}(0, \\sigma^2_e I) is residual error.\n\nNotes on estimation\n\nCovariance under the null: \\operatorname{Var}(y) = V = \\sigma^2_e I + \\sigma^2_g K.\nWe pass an ancestry‑adjusted GRM from PC‑Relate\n\nThis yields diagonals near 1 (via pcrelateToMatrix(scaleKin=2)) and controls for population structure in kinship estimation\n\nFixed effects (GLS) estimator given variance components: \\hat{\\beta} = (X^\\top V^{-1} X)^{-1} X^\\top V^{-1} y.\nVariance components for Gaussian outcomes are estimated by REML.\nHeritability estimate: h^2 = \\sigma^2_g/(\\sigma^2_g + \\sigma^2_e).\n\n\nnullmod &lt;- GENESIS::fitNullModel(\n    scanAnnot2,\n    outcome = \"y\",\n    covars = c(\"sex\", \"PC1\", \"PC2\", \"PC3\"),\n    cov.mat = pcrelMat, family = \"gaussian\"\n)\n\nComputing Variance Component Estimates...\n\n\nSigma^2_A     log-lik     RSS\n\n\n[1]    0.5717426    0.5717426 -243.7704375    1.0182557\n[1]    0.1884426    0.8753057 -242.1753414    1.0461782\n[1]    0.2420265    0.8685482 -242.1442635    1.0026855\n[1]    0.2408589    0.8726307 -242.1442236    1.0000083\n[1]    0.2411044    0.8724022 -242.1442228    1.0000000\n\nnullmod$varComp\n\n        V_A V_resid.var \n  0.2411044   0.8724022 \n\n\nvarComp reports the variance components from the mixed model. The entry V_A is the additive‑genetic variance captured by our GRM (PC‑Relate), and V_resid.var is the residual variance after adjusting for age, sex, and ancestry PCs. A convenient summary is the mixed‑model heritability on this working scale, h^2 = \\frac{V_A}{V_A + V_{resid}}\n\nvarc &lt;- nullmod$varComp\nh2 &lt;- unname(varc[\"V_A\"] / sum(varc))\n\ntibble::tibble(\n  component = c(\"V_A\", \"V_resid\", \"h2_mixed_model\"),\n  value = c(varc[\"V_A\"], varc[\"V_resid.var\"], h2)\n)\n\n# A tibble: 3 × 2\n  component      value\n  &lt;chr&gt;          &lt;dbl&gt;\n1 V_A            0.241\n2 V_resid        0.872\n3 h2_mixed_model 0.217\n\n\n\n\n\nAssociation testing\nWe now test each SNP for association with the phenotype using a score test. The score test is computationally efficient because it only requires fitting the null model once, rather than refitting for each SNP as in a Wald or likelihood ratio test.\n\n# Genotype iterator (block-wise scan of all SNPs kept by QC)\nit &lt;- GWASTools::GenotypeBlockIterator(\n    geno_data, \n    snpInclude = snp.keep.id, \n    snpBlock = 5000\n)\nassoc &lt;- GENESIS::assocTestSingle(it, null.model = nullmod, test = \"Score\")\n\nUsing 6 CPU cores\n\nres &lt;- data.frame(\n    SNP = assoc$variant.id,\n    CHR = as.numeric(assoc$chr),\n    BP = assoc$pos,\n    P = assoc$Score.pval,\n    BETA = assoc$Est,\n    SE = assoc$Est.SE\n)"
  },
  {
    "objectID": "lectures/lecture-05.html#step-4-postgwas",
    "href": "lectures/lecture-05.html#step-4-postgwas",
    "title": "Lecture 05: GWAS in R",
    "section": "Step 4: Post‑GWAS",
    "text": "Step 4: Post‑GWAS\n\nExtracting results\n\n\n        SNP    BP       BETA        SE            P\n6         6     6  0.8396265 0.1568205 8.600221e-08\n4628   4892  4892 -0.5765968 0.1501810 1.233630e-04\n9363   9895  9895 -0.5772324 0.1532537 1.655421e-04\n1796   1878  1878 -0.5223076 0.1415787 2.249887e-04\n17050 17968 17968  0.4745057 0.1312115 2.987953e-04\n\n\n[1] 6\n\n\nNote that our top SNP is the one we simulated to have an effect.\n\n\nManhattan plot\nWe visualize the genome‑wide p-values using a Manhattan plot. The blue line indicates a p-value threshold of 1 \\times 10^{-5}. Note that this is equivalent to a Bonferroni correction for 5,000 independent tests. The red line at 5 \\times 10^{-8} is a common genome‑wide significance threshold for GWAS, though it is conservative for smaller studies or those with fewer variants.\n\n# qqman expects columns named: CHR, BP, SNP, P\nqqman::manhattan(\n  res,\n  chr = \"CHR\", bp = \"BP\", snp = \"SNP\", p = \"P\",\n  suggestiveline = -log10(1e-5)\n)\n\n\n\n\n\n\n\n\n\n\nQQ Plot\nWe compare observed p‑values to the null expectation to assess calibration. Few true signals should deviate strongly.\n\nqqman::qq(res$P)\n\n\n\n\n\n\n\n\n\n\nGenomic-control inflation factor\nThe genomic-control inflation factor is a single-number summary (\\lambda_{GC}) of how inflated or conservative the genome‑wide test statistics are relative to the null. Values near 1 indicate well‑calibrated tests, &gt;1 suggests inflation (e.g., population structure, relatedness, batch), &lt;1 suggests conservative/underpowered tests\nFor 1‑df tests, the null statistic follows \\chi^2_1, whose median is qchisq(0.5, 1) \\approx 0.456. We will convert our p‑values to \\chi^2 statistics T_i = \\texttt{qchisq}(1 − p_i, df = 1). Then \\lambda_{GC} = \\text{median}(T_i) / m_0.\n\nlambda_gc &lt;- median(stats::qchisq(1 - res$P, df = 1), na.rm = TRUE) / 0.456\nlambda_gc\n\n[1] 0.9851403\n\n\n\n\nCleanup\nWhen done, close the GDS connection\n\nGWASTools::close(geno_reader)"
  },
  {
    "objectID": "lectures/lecture-05.html#when-to-use-a-different-tool",
    "href": "lectures/lecture-05.html#when-to-use-a-different-tool",
    "title": "Lecture 05: GWAS in R",
    "section": "When to use a different tool?",
    "text": "When to use a different tool?\n\nFor very large datasets (e.g., UK Biobank), specialized tools like BOLT-LMM, REGENIE, or SAIGE may be more efficient\nFor binary traits, consider tools that handle case-control imbalance and relatedness effectively, such as SAIGE\nFor rare variant analysis, consider burden tests or SKAT implemented in packages like SKAT or seqMeta"
  },
  {
    "objectID": "lectures/lecture-05.html#further-reading",
    "href": "lectures/lecture-05.html#further-reading",
    "title": "Lecture 05: GWAS in R",
    "section": "Further reading",
    "text": "Further reading\n\nPopulation Structure and Relatedness Inference using the GENESIS Package\nGenetic Association Testing using the GENESIS Package\nUsing PLINK for GWAS"
  },
  {
    "objectID": "lectures/lecture-05.html#session-info",
    "href": "lectures/lecture-05.html#session-info",
    "title": "Lecture 05: GWAS in R",
    "section": "Session Info",
    "text": "Session Info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Tahoe 26.0.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] C.UTF-8/C.UTF-8/C.UTF-8/C/C.UTF-8/C.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] readr_2.1.5         dplyr_1.1.4         ggplot2_3.5.2      \n [4] qqman_0.1.9         GWASTools_1.54.0    Biobase_2.68.0     \n [7] BiocGenerics_0.54.0 generics_0.1.4      GENESIS_2.38.0     \n[10] SNPRelate_1.42.0    gdsfmt_1.44.1      \n\nloaded via a namespace (and not attached):\n  [1] Rdpack_2.6.4            DBI_1.2.3               gridExtra_2.3          \n  [4] sandwich_3.1-1          rlang_1.1.6             magrittr_2.0.3         \n  [7] SeqVarTools_1.46.0      compiler_4.5.1          RSQLite_2.4.3          \n [10] mgcv_1.9-3              reshape2_1.4.4          vctrs_0.6.5            \n [13] stringr_1.5.1           quantreg_6.1            pkgconfig_2.0.3        \n [16] shape_1.4.6.1           crayon_1.5.3            fastmap_1.2.0          \n [19] backports_1.5.0         XVector_0.48.0          labeling_0.4.3         \n [22] utf8_1.2.6              rmarkdown_2.29          tzdb_0.5.0             \n [25] UCSC.utils_1.4.0        nloptr_2.2.1            MatrixModels_0.5-4     \n [28] purrr_1.0.4             bit_4.6.0               xfun_0.52              \n [31] glmnet_4.1-10           jomo_2.7-6              logistf_1.26.1         \n [34] cachem_1.1.0            GenomeInfoDb_1.44.3     jsonlite_2.0.0         \n [37] blob_1.2.4              pan_1.9                 BiocParallel_1.42.2    \n [40] broom_1.0.8             parallel_4.5.1          R6_2.6.1               \n [43] stringi_1.8.7           RColorBrewer_1.1-3      boot_1.3-31            \n [46] DNAcopy_1.82.0          rpart_4.1.24            lmtest_0.9-40          \n [49] GenomicRanges_1.60.0    Rcpp_1.1.0              iterators_1.0.14       \n [52] knitr_1.50              zoo_1.8-14              IRanges_2.42.0         \n [55] igraph_2.1.4            Matrix_1.7-3            splines_4.5.1          \n [58] nnet_7.3-20             tidyselect_1.2.1        viridis_0.6.5          \n [61] yaml_2.3.10             codetools_0.2-20        curl_6.2.3             \n [64] plyr_1.8.9              lattice_0.22-7          tibble_3.3.0           \n [67] quantsmooth_1.74.0      withr_3.0.2             evaluate_1.0.3         \n [70] survival_3.8-3          Biostrings_2.76.0       pillar_1.11.0          \n [73] BiocManager_1.30.26     mice_3.18.0             renv_1.1.5             \n [76] foreach_1.5.2           stats4_4.5.1            reformulas_0.4.1       \n [79] vroom_1.6.5             S4Vectors_0.46.0        hms_1.1.3              \n [82] scales_1.4.0            minqa_1.2.8             calibrate_1.7.7        \n [85] GWASExactHW_1.2         glue_1.8.0              tools_4.5.1            \n [88] data.table_1.17.4       lme4_1.1-37             SparseM_1.84-2         \n [91] cowplot_1.2.0           grid_4.5.1              tidyr_1.3.1            \n [94] rbibutils_2.3           nlme_3.1-168            formula.tools_1.7.1    \n [97] GenomeInfoDbData_1.2.14 cli_3.6.5               SeqArray_1.48.3        \n[100] viridisLite_0.4.2       gtable_0.3.6            digest_0.6.37          \n[103] operator.tools_1.6.3    farver_2.1.2            memoise_2.0.1          \n[106] htmltools_0.5.8.1       lifecycle_1.0.4         httr_1.4.7             \n[109] mitml_0.4-5             bit64_4.6.0-1           MASS_7.3-65"
  },
  {
    "objectID": "lectures/lecture-06.html#learning-objectives",
    "href": "lectures/lecture-06.html#learning-objectives",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDistinguish association, prediction, and causation in statistical genetics and state the optimization target for each.\nFormalize genomic prediction for quantitative and binary traits; select appropriate metrics and calibration checks.\nCompare ridge/lasso/elastic‑net, BLUP/GBLUP, and tree/NN methods; understand when and why each is advantageous.\nDesign robust validation: nested CV, external validation, leakage avoidance with structure/kinship.\nAssess portability across ancestries and communicate limits and remedies."
  },
  {
    "objectID": "lectures/lecture-06.html#where-we-are-lectures-0105-06",
    "href": "lectures/lecture-06.html#where-we-are-lectures-0105-06",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Where We Are (Lectures 01–05 → 06)",
    "text": "Where We Are (Lectures 01–05 → 06)\n\nL01–L04: likelihoods, variance components, population structure, Bayesian thinking.\nL05: GWAS with mixed models—great for locus discovery, not optimized for individual‑level prediction.\nToday: risk prediction (polygenic modeling) and evaluation beyond p‑values."
  },
  {
    "objectID": "lectures/lecture-06.html#why-prediction-vs-association-vs-causation",
    "href": "lectures/lecture-06.html#why-prediction-vs-association-vs-causation",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Why Prediction (vs Association vs Causation)?",
    "text": "Why Prediction (vs Association vs Causation)?\n\nAssociation\n\nGoal: identify variants related to trait.\nTarget: low FDR/valid inference; effect estimates with SEs.\nTools: single‑variant tests, LMMs, fine‑mapping.\n\nPrediction\n\nGoal: accurate out‑of‑sample trait/risk estimates.\nTarget: minimize expected loss (MSE, log loss) and calibrate probabilities.\nTools: penalized GLMs, BLUP/GBLUP, ensembles, NNs.\n\n\n\n\n\n\n\n\nNote\n\n\nCausation: identify effects under interventions; target parameters and designs differ (e.g., MR, RCTs). A great predictor need not be causal, and a causal effect is not necessarily predictive."
  },
  {
    "objectID": "lectures/lecture-06.html#when-to-do-prediction",
    "href": "lectures/lecture-06.html#when-to-do-prediction",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "When To Do Prediction",
    "text": "When To Do Prediction\n\nYou have a clear decision/use‑case: screening, ranking, risk stratification, or prioritization.\nYou can evaluate externally (cohort/time/ancestry) and report calibration, not just discrimination.\nYou can incorporate non‑genetic covariates (age/sex/PCs) fairly and transparently."
  },
  {
    "objectID": "lectures/lecture-06.html#when-not-to-or-not-yet",
    "href": "lectures/lecture-06.html#when-not-to-or-not-yet",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "When Not To (or Not Yet)",
    "text": "When Not To (or Not Yet)\n\nNo external cohort; data leakage risks (relateds across folds, PCs fit on all data).\nLarge ancestry mismatch between training and target with no mitigation plan.\nClinical thresholds or utilities undefined (can’t translate scores to action)."
  },
  {
    "objectID": "lectures/lecture-06.html#prediction-setup",
    "href": "lectures/lecture-06.html#prediction-setup",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Prediction Setup",
    "text": "Prediction Setup\n\nData: response y, genotype matrix G (N×P), covariates C (PCs/age/sex).\nObjective: learn f(G, C) minimizing expected loss.\nQuantitative: MSE, R^2 (out‑of‑sample). Binary: AUC/PR, Brier score, calibration slope/intercept."
  },
  {
    "objectID": "lectures/lecture-06.html#penalized-glms-ridge-lasso-elasticnet",
    "href": "lectures/lecture-06.html#penalized-glms-ridge-lasso-elasticnet",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Penalized GLMs: Ridge / Lasso / Elastic‑Net",
    "text": "Penalized GLMs: Ridge / Lasso / Elastic‑Net\nLet standardized SNPs X and response y.\n\nRidge: \\hat\\beta = argmin ||y - X\\beta||^2 + \\lambda ||\\beta||_2^2 (dense, shrinkage; LD‑stable).\nLasso: \\hat\\beta = argmin ||y - X\\beta||^2 + \\lambda ||\\beta||_1 (sparse; feature selection; correlated SNPs → group instability).\nElastic‑Net: mix of L1/L2 to balance sparsity and LD stability.\nTuning: nested CV for \\lambda (and \\alpha for EN). Standardize predictors; stratify folds by ancestry/family."
  },
  {
    "objectID": "lectures/lecture-06.html#mixed-models-and-blupgblup",
    "href": "lectures/lecture-06.html#mixed-models-and-blupgblup",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Mixed Models And BLUP/GBLUP",
    "text": "Mixed Models And BLUP/GBLUP\n\nLMM: y = X\\beta + g + \\epsilon, g ~ N(0, K\\sigma_g^2) with GRM K.\nDuality: BLUP of SNP effects \\approx ridge with LD‑aware penalty via K.\nWhen: highly polygenic traits, p &gt;&gt; n, strong LD, fast training with big P."
  },
  {
    "objectID": "lectures/lecture-06.html#prs-workflows-individual-vs-summary-stats",
    "href": "lectures/lecture-06.html#prs-workflows-individual-vs-summary-stats",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "PRS Workflows (Individual vs Summary Stats)",
    "text": "PRS Workflows (Individual vs Summary Stats)\n\nIndividual‑level: penalized GLM/GBLUP on X with PCs; careful CV.\nSummary‑stats: clumping+thresholding (C+T) vs LD‑aware Bayesian shrinkage; use when individual genotypes unavailable.\nKey choices: LD reference, ancestry matching, sample overlap checks."
  },
  {
    "objectID": "lectures/lecture-06.html#nonlinear-models-rfgbmdnn",
    "href": "lectures/lecture-06.html#nonlinear-models-rfgbmdnn",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Nonlinear Models: RF/GBM/DNN",
    "text": "Nonlinear Models: RF/GBM/DNN\n\nCapture interactions and nonlinearities; risk of overfitting in p ≫ n with correlated SNPs.\nPros: robustness to monotone transforms (trees), potential epistasis capture.\nCons: compute, calibration, interpretability; gains often modest unless strong interactions."
  },
  {
    "objectID": "lectures/lecture-06.html#validation-strategy",
    "href": "lectures/lecture-06.html#validation-strategy",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Validation Strategy",
    "text": "Validation Strategy\n\nSplit: train/validation/test or nested CV; keep relateds within folds; compute PCs on training only and project.\nReport: mean ± SE across folds; external validation on new cohort and ancestry.\nCalibration: reliability plots, Brier score, Platt/Isotonic if needed; decision thresholds derived from utility."
  },
  {
    "objectID": "lectures/lecture-06.html#portability-fairness",
    "href": "lectures/lecture-06.html#portability-fairness",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Portability & Fairness",
    "text": "Portability & Fairness\n\nPerformance gaps across ancestries from LD/MAF differences and environment.\nRemedies: multi‑ancestry training, group‑aware reweighting, ancestry‑specific recalibration, transfer learning.\nReport per‑group metrics (R²/AUC, calibration slope/intercept), not just overall."
  },
  {
    "objectID": "lectures/lecture-06.html#from-association-to-prediction-mindset-shift",
    "href": "lectures/lecture-06.html#from-association-to-prediction-mindset-shift",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "From Association To Prediction: Mindset Shift",
    "text": "From Association To Prediction: Mindset Shift\n\nHypothesis testing → risk minimization.\nMultiple testing/FDR → CV/hold‑out and external replication.\nWinner’s curse/sample overlap → unbiased evaluation and strict data hygiene."
  },
  {
    "objectID": "lectures/lecture-06.html#case-study-lassopenalized-logistic-regression-in-gwas",
    "href": "lectures/lecture-06.html#case-study-lassopenalized-logistic-regression-in-gwas",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Case Study: Lasso‑Penalized Logistic Regression In GWAS",
    "text": "Case Study: Lasso‑Penalized Logistic Regression In GWAS\n\nAnchor reading: Wu et al. (2009) “Genome‑wide association analysis by lasso‑penalized logistic regression”.\nIdea: coordinate descent for logistic loss + L1 penalty; scalable screening; interaction search extension; applied to coeliac disease.\nTakeaways (then vs now): sparsity helps ranking but LD induces instability; today we add PCs/kinship, LD‑aware priors, and stronger external validation."
  },
  {
    "objectID": "lectures/lecture-06.html#math-box-lasso-softthresholding-1d",
    "href": "lectures/lecture-06.html#math-box-lasso-softthresholding-1d",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Math Box: Lasso Soft‑Thresholding (1D)",
    "text": "Math Box: Lasso Soft‑Thresholding (1D)\nFor standardized x and response y, the lasso estimator solves \\hat\\beta = S(\\hat\\beta_{OLS}, \\lambda) where S is soft‑thresholding; in correlated blocks (LD), support can be unstable → prefer EN or group penalties."
  },
  {
    "objectID": "lectures/lecture-06.html#math-box-ridge-gaussian-prior-blup-dual",
    "href": "lectures/lecture-06.html#math-box-ridge-gaussian-prior-blup-dual",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Math Box: Ridge ≡ Gaussian Prior; BLUP Dual",
    "text": "Math Box: Ridge ≡ Gaussian Prior; BLUP Dual\n\nRidge ↔︎ \\beta_j ~ N(0, \\tau^2); BLUP emerges from the dual with K = XX^T/P.\nIntuition: ridge shrinks per‑SNP; BLUP shrinks in the individual space via K."
  },
  {
    "objectID": "lectures/lecture-06.html#demo-toy-genomic-prediction-with-glmnet-57-min",
    "href": "lectures/lecture-06.html#demo-toy-genomic-prediction-with-glmnet-57-min",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Demo: Toy Genomic Prediction With glmnet (5–7 min)",
    "text": "Demo: Toy Genomic Prediction With glmnet (5–7 min)\n\n# Simulate LD blocks and a polygenic architecture\nN &lt;- 800; P &lt;- 2000\nblocks &lt;- 40; bsize &lt;- P / blocks\nrho &lt;- 0.9\n\nSigma_block &lt;- rho ^ abs(row(diag(bsize)) - col(diag(bsize)))\nX &lt;- do.call(cbind, replicate(blocks, MASS::mvrnorm(N, mu = rep(0, bsize), Sigma = Sigma_block), simplify = FALSE))\nX &lt;- scale(X)\n\n# True effects: mix of sparse + dense\nbeta &lt;- rep(0, P)\nsignal_idx &lt;- sample(P, 150)\nbeta[signal_idx] &lt;- rnorm(150, 0, 0.15)\n\ny &lt;- as.numeric(X %*% beta + rnorm(N, 0, 1))\n\n# Train/validation/test split (ancestry/relatives would stratify here in real data)\nset.seed(2)\nidx &lt;- sample.int(N)\ntr &lt;- idx[1:500]; va &lt;- idx[501:650]; te &lt;- idx[651:N]\n\ncv_grid &lt;- 10^seq(-3, 1, length.out = 50)\n\nfit_ridge &lt;- glmnet(X[tr,], y[tr], alpha = 0)\npred_va &lt;- sapply(cv_grid, function(l) predict(fit_ridge, X[va,], s = l))\nmse_va &lt;- colMeans((pred_va - y[va])^2)\nlambda_best &lt;- cv_grid[which.min(mse_va)]\n\npred_te &lt;- as.numeric(predict(fit_ridge, X[te,], s = lambda_best))\nmse_te &lt;- mean((pred_te - y[te])^2)\nr2_te &lt;- cor(pred_te, y[te])^2\ndata.frame(metric = c(\"MSE\", \"R2\"), value = c(mse_te, r2_te))\n\n  metric     value\n1    MSE 2.2356675\n2     R2 0.4159352"
  },
  {
    "objectID": "lectures/lecture-06.html#demo-calibration-check-regression",
    "href": "lectures/lecture-06.html#demo-calibration-check-regression",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Demo: Calibration Check (Regression)",
    "text": "Demo: Calibration Check (Regression)\n\ndf_te &lt;- data.frame(y = y[te], yhat = pred_te)\nggplot(df_te, aes(yhat, y)) +\n  geom_point(alpha = .35) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Calibration: observed vs predicted\", x = \"Predicted\", y = \"Observed\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lectures/lecture-06.html#binary-trait-variant-optional",
    "href": "lectures/lecture-06.html#binary-trait-variant-optional",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Binary Trait Variant (Optional)",
    "text": "Binary Trait Variant (Optional)\n\n# Create a logistic trait with moderate heritability\neta &lt;- as.numeric(X %*% beta)\npr &lt;- plogis(scale(eta))\nybin &lt;- rbinom(N, 1, pr)\nfit_lasso &lt;- glmnet(X[tr,], ybin[tr], alpha = 1, family = \"binomial\")\ncv &lt;- cv.glmnet(X[tr,], ybin[tr], alpha = 1, family = \"binomial\", nfolds = 5)\nphat &lt;- as.numeric(predict(fit_lasso, X[te,], s = cv$lambda.min, type = \"response\"))\n# Compute AUC and reliability plot if pROC/yardstick available"
  },
  {
    "objectID": "lectures/lecture-06.html#diagnostics-to-show-on-slides",
    "href": "lectures/lecture-06.html#diagnostics-to-show-on-slides",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Diagnostics To Show On Slides",
    "text": "Diagnostics To Show On Slides\n\nRegularization paths (coefficients vs log‑lambda) from plot(fit_ridge).\nRisk distributions by case/control (binary) and reliability plots.\nCoefficient Manhattan vs predictive performance to contrast discovery vs prediction."
  },
  {
    "objectID": "lectures/lecture-06.html#common-pitfalls-and-fixes",
    "href": "lectures/lecture-06.html#common-pitfalls-and-fixes",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Common Pitfalls (And Fixes)",
    "text": "Common Pitfalls (And Fixes)\n\nData leakage: keep families/relateds within folds; compute PCs on train only; avoid sample overlap with discovery GWAS.\nAncestry mismatch: external validation and group‑wise calibration; consider multi‑ancestry training.\nHyperparameter overfit: nested CV; pre‑register analysis plan for high‑stakes use."
  },
  {
    "objectID": "lectures/lecture-06.html#discussion-prompts",
    "href": "lectures/lecture-06.html#discussion-prompts",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Discussion Prompts",
    "text": "Discussion Prompts\n\nWhen is sparsity a good inductive bias for complex traits?\nHow would you incorporate biological priors (annotations/pathways) in penalties?\nWhat metric matters for your application: ranking, calibration, or decision utility?"
  },
  {
    "objectID": "lectures/lecture-06.html#takeaways",
    "href": "lectures/lecture-06.html#takeaways",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "Takeaways",
    "text": "Takeaways\n\nChoose the inductive bias to match trait architecture: ridge/GBLUP (dense polygenic), lasso/EN (sparser), nonlinear only with strong justification and careful validation.\nReport discrimination, calibration, and portability; external validation is essential before deployment."
  },
  {
    "objectID": "lectures/lecture-06.html#references-for-this-lecture",
    "href": "lectures/lecture-06.html#references-for-this-lecture",
    "title": "Lecture 06: Prediction models in genetics",
    "section": "References for This Lecture",
    "text": "References for This Lecture\n\nWu, T. T., Chen, Y. F., Hastie, T., Sobel, E., & Lange, K. (2009). Genome‑wide association analysis by lasso‑penalized logistic regression. Bioinformatics, 25(6), 714–721. doi:10.1093/bioinformatics/btp041\nAdditional suggested readings on PRS evaluation, portability, and LD‑aware shrinkage (see course references list)."
  },
  {
    "objectID": "lectures/lecture-02.html#agenda",
    "href": "lectures/lecture-02.html#agenda",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Agenda",
    "text": "Agenda\n\nMLE Review\nNarrow-and broad-sense heritability: definitions and interpretation\nEstimating h^2 from pedigrees\nVariance components via pedigrees (LMM with A-matrix)\nBinary traits: liability-threshold, observed vs. liability scales\nFamilial aggregation for binary traits: \\lambda_R (concept \\rightarrow model)\nSegregation analysis: modeling inheritance without markers\nAscertainment as conditioning (truncation viewpoint)"
  },
  {
    "objectID": "lectures/lecture-02.html#mle-essentials",
    "href": "lectures/lecture-02.html#mle-essentials",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "MLE essentials",
    "text": "MLE essentials\n\n\n\nProbability vs Likelihood:\nL(\\theta\\mid y)\\propto p(y\\mid\\theta) (fix y, vary \\theta).\nLog-likelihood / score / information:\n\\ell(\\theta)=\\sum_i \\log p(y_i\\mid\\theta),\nS(\\theta)=\\partial\\ell/\\partial\\theta,\nI(\\theta)=-\\partial^2\\ell/\\partial\\theta\\partial\\theta^\\top.\nMLE: \\hat\\theta=\\arg\\max_\\theta \\ell(\\theta).\nLarge-sample:\n\\hat\\theta \\approx \\mathcal N\\!\\big(\\theta_0,\\; i(\\theta_0)^{-1}\\big),\ni(\\theta)=\\mathbb E[I(\\theta\\mid Y)].\nInvariance: MLE of g(\\theta) is g(\\hat\\theta).\n\n\n\n\n\n\n\n\nTip\n\n\nRegression as a likelihood\nIf e\\sim\\mathcal N(0,\\sigma^2 I), then OLS = MLE for \\beta;\n\\hat\\sigma^2=\\tfrac{1}{n}\\sum \\hat e_i^2.\n\n\n\n\n\n\n\n\n\nWarning\n\n\nPitfalls\nNon-unique maxima, flat ridges, boundary solutions, small-sample failures of asymptotics."
  },
  {
    "objectID": "lectures/lecture-02.html#lrt-unrestricted-vs-restricted-mle",
    "href": "lectures/lecture-02.html#lrt-unrestricted-vs-restricted-mle",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "LRT (unrestricted vs restricted MLE)",
    "text": "LRT (unrestricted vs restricted MLE)\n\nGoal: test H_0\\!:\\,\\theta=\\theta_0 in a model with log-likelihood \\ell(\\theta,\\eta) and nuisance \\eta.\nUnrestricted MLE: (\\hat\\theta,\\hat\\eta) = \\arg\\max_{\\theta,\\eta} \\ell(\\theta,\\eta).\nRestricted MLE under H_0: \\hat\\eta_0 = \\arg\\max_{\\eta} \\ell(\\theta_0,\\eta).\nTest statistic (fit improvement): \\Lambda = 2\\{\\ell(\\hat\\theta,\\hat\\eta) - \\ell(\\theta_0,\\hat\\eta_0)\\}.\nInterpretation: how much better the unrestricted fit is than the restricted fit; large values argue against H_0.\nAsymptotics: \\Lambda \\overset{d}{\\to} \\chi^2_q with q constraints (here q{=}1). p-value: 1-F_{\\chi^2_q}(\\Lambda).\n\n\n\n\n\n\n\nWarning\n\n\nVariance-component caveat\nWhen testing a variance component \\sigma^2=0 (boundary), the LRT null is a mixture (e.g., \\tfrac{1}{2}\\chi^2_0 + \\tfrac{1}{2}\\chi^2_1); standard \\chi^2 reference is invalid. Restricted LRTs or score tests are common alternatives."
  },
  {
    "objectID": "lectures/lecture-02.html#aggregation-heritability-and-segregation-analyses",
    "href": "lectures/lecture-02.html#aggregation-heritability-and-segregation-analyses",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Aggregation, heritability, and segregation analyses",
    "text": "Aggregation, heritability, and segregation analyses\n\n\n\nAggregation/heritability analyses: Investigating patterns of phenotypic correlation between relatives\nSegregation analysis: Finding support for a specific genetic model underlying inheritance patterns\n\n\n\n\n\n\n\n\nNote\n\n\nThese analyses do not always use molecular genetic data… so why should we care?"
  },
  {
    "objectID": "lectures/lecture-02.html#a-gap-in-estimation",
    "href": "lectures/lecture-02.html#a-gap-in-estimation",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "A gap in estimation",
    "text": "A gap in estimation\n\n\n\n\n\nYoung, A. I. (2019), “Solving the missing heritability problem,” PLOS Genetics, Public Library of Science, 15, e1008222. https://doi.org/10.1371/journal.pgen.1008222.\n\n\n\n\nAggregation and heritability analyses tend to have much higher heritability estimates of traits than genotyping methods\nUnderstanding these models may help explain this delta"
  },
  {
    "objectID": "lectures/lecture-02.html#discussion",
    "href": "lectures/lecture-02.html#discussion",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Discussion",
    "text": "Discussion\n\nThe author frames the “missing heritability” problem as the discrepancy between estimates from twin studies and those from early GWAS. Based on the text, what is the core assumption of the classical twin study design for estimating heritability? How might a violation of this assumption lead to an overestimation of heritability for certain traits?\nThe author argues that methods like RDR and Sib-Regression are more robust against certain biases. What specific confounding factor, prevalent in population-based genomic studies, are these family-based designs better at controlling for?\nIf, as the author suggests, the true narrow-sense heritability of a trait like height is closer to the 60-70% estimated by RDR and Sib-Regression than the 80% from twin studies, what are the primary sources of the remaining “gap”? Does this completely solve the missing heritability problem, or does it redefine it?"
  },
  {
    "objectID": "lectures/lecture-02.html#heritability-first-principles",
    "href": "lectures/lecture-02.html#heritability-first-principles",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Heritability, first principles",
    "text": "Heritability, first principles\nLet Y = G + E\n\nTrait Variance: \\operatorname{Var}(Y)\nVariance due to genes: \\operatorname{Var}(G)\nVariance due to environment: \\operatorname{Var}(E).\n\nIf we assume independence of G and E, then \\operatorname{Var}(Y) = \\operatorname{Var}(G) + \\operatorname{Var}(E)"
  },
  {
    "objectID": "lectures/lecture-02.html#heritability-first-principles-1",
    "href": "lectures/lecture-02.html#heritability-first-principles-1",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Heritability, first principles",
    "text": "Heritability, first principles\nWe can decompose \\operatorname{Var}(G) into different genetic effects: - Additive effects: A - Dominance effects: D - Epistatic effects: I\nSo we can write:\n\\operatorname{Var}(Y) = \\operatorname{Var}(A) + \\operatorname{Var}(D) + \\operatorname{Var}(I) + \\operatorname{Var}(E)\n\nBroad-sense: H^2 = \\operatorname{Var}(A) + \\operatorname{Var}(D) + \\operatorname{Var}(I) / \\operatorname{Var}(Y)\nNarrow-sense: h^2 =\\operatorname{Var}(A) / \\operatorname{Var}(Y)\nContext matters: h^2 depends on population, environment, and measurement; it is not a trait constant.\n\n\n\n\n\n\n\nWarning\n\n\nFamilial aggregation \\neq heritability. Shared environment and assortment can produce aggregation without genetic causation."
  },
  {
    "objectID": "lectures/lecture-02.html#estimating-h2-from-relatives-marker-free",
    "href": "lectures/lecture-02.html#estimating-h2-from-relatives-marker-free",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Estimating h^2 from relatives (marker-free)",
    "text": "Estimating h^2 from relatives (marker-free)\nParent-offspring regression\n\nY_{\\text{offspring}} = \\alpha + \\beta \\cdot Y_{\\text{mid-parent}} + \\varepsilon\n\n\nBy definition, \\beta = \\frac{\\operatorname{Cov}(Y_{\\text{offspring}}, Y_{\\text{mid-parent}})}{\\operatorname{Var}(Y_{\\text{mid-parent}})}.\nGiven that an offspring inherits half its genes from each parent, \\operatorname{Cov}(Y_{\\text{offspring}}, Y_{\\text{mid-parent}}) = \\frac{1}{2}\\operatorname{Var}(A)"
  },
  {
    "objectID": "lectures/lecture-02.html#estimating-h2-from-relatives-marker-free-1",
    "href": "lectures/lecture-02.html#estimating-h2-from-relatives-marker-free-1",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Estimating h^2 from relatives (marker-free)",
    "text": "Estimating h^2 from relatives (marker-free)\nParent-offspring regression\n\\begin{align*}\n\\operatorname{Var}(Y_{\\text{mid-parent}})\n&= \\operatorname{Var}\\left(\\frac{Y_{\\text{parent1}} + Y_{\\text{parent2}}}{2}\\right) \\\\\n&= \\frac{1}{4}\\left(\\operatorname{Var}(Y) + \\operatorname{Var}(Y)\\right) \\\\\n&= \\frac{1}{2}\\operatorname{Var}(Y)\n\\end{align*}\nPlugging this back into the expression for \\beta:\n\\beta = \\frac{\\frac{1}{2}\\operatorname{Var}(A)}{\\frac{1}{2}\\operatorname{Var}(Y)} = \\frac{\\operatorname{Var}(A)}{\\operatorname{Var}(Y)} = h^2"
  },
  {
    "objectID": "lectures/lecture-02.html#estimating-h2-from-relatives-marker-free-2",
    "href": "lectures/lecture-02.html#estimating-h2-from-relatives-marker-free-2",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Estimating h^2 from relatives (marker-free)",
    "text": "Estimating h^2 from relatives (marker-free)\nTwin Studies\n\nAssume equal environments for MZ and DZ twins, and a simplified model\n\\operatorname{Var}(Y) = \\operatorname{Var}(A) + \\operatorname{Var}(E)\nh^2 = 2(r_{\\text{MZ}} - r_{\\text{DZ}})\n\nwhere r_{\\text{MZ}} and r_{\\text{DZ}} are the correlations between monozygotic and dizygotic twins, respectively."
  },
  {
    "objectID": "lectures/lecture-02.html#pedigree-variance-components-continuous-traits",
    "href": "lectures/lecture-02.html#pedigree-variance-components-continuous-traits",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Pedigree variance components (continuous traits)",
    "text": "Pedigree variance components (continuous traits)\nModel: \n\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{u} + \\boldsymbol{\\varepsilon},\\quad\n\\mathbf{u} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{A}\\sigma_A^2),\\quad\n\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}\\sigma_E^2),\n\nwhere \\mathbf{A} is the pedigree additive relationship matrix with A_{ij}=2\\phi_{ij}.\n\nHeritability: h^2 = \\sigma_A^2 / (\\sigma_A^2 + \\sigma_E^2).\nREML estimates (\\sigma_A^2,\\sigma_E^2) efficiently; classical family estimators are special cases under balanced designs.\nACE (twin) view: \\operatorname{Cov}(\\text{MZ}) = A + C, \\operatorname{Cov}(\\text{DZ}) = \\tfrac{1}{2}A + C; h^2 = \\operatorname{Var}(A)/\\operatorname{Var}(Y), etc.\n\n\n\n\n\n\n\nWarning\n\n\nBoundary testing: testing \\sigma_A^2=0 is on the boundary. The LRT null is the mixture \\tfrac{1}{2}\\chi_0^2 + \\tfrac{1}{2}\\chi_1^2 (or use an RLRT)."
  },
  {
    "objectID": "lectures/lecture-02.html#binary-traits-via-the-liability-threshold-model",
    "href": "lectures/lecture-02.html#binary-traits-via-the-liability-threshold-model",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Binary traits via the liability-threshold model",
    "text": "Binary traits via the liability-threshold model\nIdea: a binary phenotype arises when a continuous liability \\ell crosses a threshold T.\n\nY = \\begin{cases}\n1 & \\text{if } \\ell &gt; T,\\\n0 & \\text{if } \\ell \\le T,\n\\end{cases}\n\n\n\\ell = G + E,\\quad G \\sim \\mathcal{N}(0,\\sigma_G^2),\\ E \\sim \\mathcal{N}(0,\\sigma_E^2)\nPopulation prevalence: K = \\Pr(\\ell &gt; T) = 1 - \\Phi(T) with T = \\Phi^{-1}(1-K).\nLiability-scale heritability: h_\\ell^2 = \\sigma_G^2 / (\\sigma_G^2 + \\sigma_E^2).\n\n\n\n\n\n\n\nWarning\n\n\nAssumptions: normal liability, single threshold, no G\\times E on the liability scale, correct K."
  },
  {
    "objectID": "lectures/lecture-02.html#probit-view-and-logistic-note",
    "href": "lectures/lecture-02.html#probit-view-and-logistic-note",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Probit view and logistic note",
    "text": "Probit view and logistic note\n\nProbit GLM/GLMM corresponds to a normal-liability threshold model; adding pedigree random effects on the probit scale estimates liability-scale variance components."
  },
  {
    "objectID": "lectures/lecture-02.html#observed-vs.-liability-scale-h2",
    "href": "lectures/lecture-02.html#observed-vs.-liability-scale-h2",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Observed vs. liability-scale h^2",
    "text": "Observed vs. liability-scale h^2\nFor an unascertained sample with sample prevalence P=K, \nh_{\\text{obs}}^2 ;\\approx; h_\\ell^2 \\cdot \\frac{\\phi(T)^2}{K(1-K)},\\qquad T=\\Phi^{-1}(1-K).\n\nFor case–control sampling with sample prevalence P \\ne K, \nh_\\ell^2 ;\\approx; h_{\\text{obs}}^2 \\cdot \\frac{K^2(1-K)^2}{\\phi(T)^2\\, P(1-P)}.\n\n\n\n\n\n\n\nTip\n\n\nLow-prevalence traits (K \\ll 0.5) often have h_{\\text{obs}}^2 \\ll h_\\ell^2. Always report the scale, K, and (if applicable) P."
  },
  {
    "objectID": "lectures/lecture-02.html#ascertainment-as-conditioning-truncation-view",
    "href": "lectures/lecture-02.html#ascertainment-as-conditioning-truncation-view",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Ascertainment as conditioning (truncation view)",
    "text": "Ascertainment as conditioning (truncation view)\nIf we only observe Y under an event A (e.g., Y recorded only when Y&gt;T or when a proband is affected), the correct likelihood uses the conditional density \np(y \\mid A, \\theta) ;=; \\frac{p(y \\mid \\theta),\\mathbf{1}{y \\in A}}{\\Pr(A \\mid \\theta)}.\n\n\nIgnoring ascertainment biases parameters (e.g., means, prevalences, and regression slopes).\nThe liability-threshold model and segregation analysis both require conditioning on how families were recruited."
  },
  {
    "objectID": "lectures/lecture-02.html#segregation-analysis-no-markers",
    "href": "lectures/lecture-02.html#segregation-analysis-no-markers",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Segregation analysis (no markers)",
    "text": "Segregation analysis (no markers)\nGoal: compare inheritance models (major gene vs polygenic vs mixed) using family phenotypes.\n\nLikelihood: specify penetrance by genotype \\Rightarrow build family likelihood; condition on ascertainment (e.g., proband affected).\nDominant example (Dd \\times dd): if p_D is the offspring affected probability, then with n children and n_A affected,\nN_A \\sim \\operatorname{Binom}(n, p_D)\n\\log L(p_D \\mid n_A,n) = n_A \\log p_D + (n-n_A)\\log(1-p_D)\nRecessive note (Dd \\times Dd): unaffected genotypes are ambiguous unless carriers are observed (harder identifiability).\nPitfalls: reduced penetrance, phenocopies, ascertainment, HWE assumptions."
  },
  {
    "objectID": "lectures/lecture-02.html#summary-key-takeaways",
    "href": "lectures/lecture-02.html#summary-key-takeaways",
    "title": "Lecture 02: Heritability, segregation, and the gene-mapping toolkit",
    "section": "Summary & key takeaways",
    "text": "Summary & key takeaways\n\nHeritability: H^2 vs h^2; interpretation is population- and environment-specific.\nVariance components (pedigrees): LMM with A-matrix unifies family estimators; handle boundary tests correctly.\nBinary traits: liability-threshold,\nSegregation: fit penetrance-based likelihoods; condition on ascertainment.\nMethod habit: when unsure, simulate to check intuition and bias under ascertainment."
  }
]